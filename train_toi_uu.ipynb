{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWAmgTfR/wqURtCCwBfh29",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuuPhuoc2411/ESP32Feature28/blob/main/train_toi_uu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STRSVltrbS_R"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Google Colab Machine Learning Pipeline for SVM Models\n",
        "Automatically train SVM models and generate Arduino/ESP32 libraries\n",
        "Author: Auto-generated\n",
        "Date: 2025\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# INSTALL REQUIRED PACKAGES (For Google Colab)\n",
        "# ============================================================================\n",
        "import sys\n",
        "\n",
        "# Ki·ªÉm tra xem c√≥ ƒëang ch·∫°y tr√™n Colab kh√¥ng\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB_CHECK = True\n",
        "except:\n",
        "    IN_COLAB_CHECK = False\n",
        "\n",
        "# Ki·ªÉm tra xem c√≥ ƒëang ch·∫°y tr√™n Kaggle kh√¥ng\n",
        "try:\n",
        "    import kaggle\n",
        "    IN_KAGGLE_CHECK = True\n",
        "except:\n",
        "    IN_KAGGLE_CHECK = False\n",
        "\n",
        "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "if IN_COLAB_CHECK or IN_KAGGLE_CHECK:\n",
        "    print(\"üì¶ ƒêang c√†i ƒë·∫∑t: optuna, openpyxl, gradio...\")\n",
        "    !pip install -q optuna openpyxl gradio\n",
        "    print(\"‚úì C√†i ƒë·∫∑t ho√†n t·∫•t!\")\n",
        "\n",
        "# ============================================================================\n",
        "# IMPORT LIBRARIES\n",
        "# ============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
        "                               GradientBoostingClassifier, AdaBoostClassifier)\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix, classification_report)\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.pruners import MedianPruner\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=optuna.exceptions.ExperimentalWarning)\n",
        "\n",
        "# For Google Colab\n",
        "try:\n",
        "    from google.colab import files, drive\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# For Kaggle\n",
        "try:\n",
        "    import kaggle\n",
        "    IN_KAGGLE = True\n",
        "except:\n",
        "    IN_KAGGLE = False\n",
        "\n",
        "# Check if running in cloud environment\n",
        "IN_CLOUD = IN_COLAB or IN_KAGGLE\n",
        "\n",
        "# Import Gradio cho giao di·ªán web\n",
        "try:\n",
        "    import gradio as gr\n",
        "    GRADIO_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GRADIO_AVAILABLE = False\n",
        "    if not IN_CLOUD:\n",
        "        print(\"‚ö†Ô∏è  Gradio ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Ch·∫°y: pip install gradio\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION - NG∆Ø·ªúI D√ôNG T·ª∞ C·∫§U H√åNH ·ªû ƒê√ÇY\n",
        "# ============================================================================\n",
        "\n",
        "# --- ƒê∆∞·ªùng d·∫´n file (ch·ªâ d√πng khi ch·∫°y CLI mode, Gradio mode t·∫£i qua giao di·ªán) ---\n",
        "SINGLE_FILE_PATH = \"\"  # ƒê·ªÉ tr·ªëng n·∫øu d√πng Gradio\n",
        "TRAIN_FILE_PATH = \"\"   # ƒê·ªÉ tr·ªëng n·∫øu d√πng Gradio\n",
        "TEST_FILE_PATH = \"\"    # ƒê·ªÉ tr·ªëng n·∫øu d√πng Gradio\n",
        "\n",
        "# --- C·∫•u h√¨nh chia d·ªØ li·ªáu ---\n",
        "TRAIN_RATIO = 0.7  # T·ª∑ l·ªá train (70%), test s·∫Ω l√† 30%\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# --- C·∫•u h√¨nh Optuna ---\n",
        "N_TRIALS = 50  # S·ªë l·∫ßn th·ª≠ t·ªëi ∆∞u h√≥a (tƒÉng l√™n ƒë·ªÉ k·∫øt qu·∫£ t·ªët h∆°n nh∆∞ng m·∫•t th·ªùi gian)\n",
        "CV_FOLDS = 5   # S·ªë fold cho cross-validation\n",
        "OPTUNA_TIMEOUT = 180  # Timeout cho m·ªói model (gi√¢y) - gi·∫£m xu·ªëng ƒë·ªÉ train nhanh h∆°n\n",
        "MIN_TRIALS_BEFORE_PRUNING = 3  # S·ªë trial t·ªëi thi·ªÉu tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu prune\n",
        "PRUNING_WARMUP_STEPS = 1  # S·ªë fold t·ªëi thi·ªÉu tr∆∞·ªõc khi prune trial\n",
        "\n",
        "# --- C·∫•u h√¨nh output ---\n",
        "OUTPUT_DIR = \"ML_Output\"  # Th∆∞ m·ª•c ch·ª©a k·∫øt qu·∫£\n",
        "ARDUINO_LIB_NAME = \"MLPredictor\"  # T√™n th∆∞ vi·ªán Arduino\n",
        "SEPARATE_MODEL_FILES = True  # True: M·ªói model 1 file ri√™ng (t·ªëi ∆∞u ROM), False: T·∫•t c·∫£ trong 1 file\n",
        "\n",
        "# ============================================================================\n",
        "# SPXY ALGORITHM - KENNARD-STONE ALGORITHM\n",
        "# ============================================================================\n",
        "\n",
        "def calculate_distance_matrix(X):\n",
        "    \"\"\"T√≠nh ma tr·∫≠n kho·∫£ng c√°ch Euclidean\"\"\"\n",
        "    n = X.shape[0]\n",
        "    dist_matrix = np.zeros((n, n))\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            dist = np.linalg.norm(X[i] - X[j])\n",
        "            dist_matrix[i, j] = dist\n",
        "            dist_matrix[j, i] = dist\n",
        "    return dist_matrix\n",
        "\n",
        "def spxy_split(X, y, train_size=0.7, random_state=None):\n",
        "    \"\"\"\n",
        "    SPXY algorithm ƒë·ªÉ chia d·ªØ li·ªáu gi·ªØ t·ªâ l·ªá c√°c l·ªõp\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "    y : array-like, shape (n_samples,)\n",
        "    train_size : float, t·ª∑ l·ªá d·ªØ li·ªáu train\n",
        "    random_state : int, seed cho random\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    if random_state:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    # L·∫•y c√°c l·ªõp duy nh·∫•t\n",
        "    classes = np.unique(y)\n",
        "\n",
        "    train_indices = []\n",
        "    test_indices = []\n",
        "\n",
        "    # √Åp d·ª•ng SPXY cho t·ª´ng l·ªõp\n",
        "    for cls in classes:\n",
        "        cls_indices = np.where(y == cls)[0]\n",
        "        X_cls = X[cls_indices]\n",
        "\n",
        "        n_train = int(len(cls_indices) * train_size)\n",
        "\n",
        "        if n_train < 2:\n",
        "            # N·∫øu l·ªõp qu√° nh·ªè, chia ng·∫´u nhi√™n\n",
        "            np.random.shuffle(cls_indices)\n",
        "            train_indices.extend(cls_indices[:n_train])\n",
        "            test_indices.extend(cls_indices[n_train:])\n",
        "            continue\n",
        "\n",
        "        # T√≠nh ma tr·∫≠n kho·∫£ng c√°ch\n",
        "        dist_matrix = calculate_distance_matrix(X_cls)\n",
        "\n",
        "        # Ch·ªçn 2 m·∫´u xa nh·∫•t\n",
        "        max_dist_idx = np.unravel_index(dist_matrix.argmax(), dist_matrix.shape)\n",
        "        selected = list(max_dist_idx)\n",
        "        remaining = list(set(range(len(X_cls))) - set(selected))\n",
        "\n",
        "        # Ch·ªçn c√°c m·∫´u ti·∫øp theo\n",
        "        while len(selected) < n_train and remaining:\n",
        "            max_min_dist = -1\n",
        "            max_min_idx = -1\n",
        "\n",
        "            for idx in remaining:\n",
        "                min_dist = min([dist_matrix[idx, s] for s in selected])\n",
        "                if min_dist > max_min_dist:\n",
        "                    max_min_dist = min_dist\n",
        "                    max_min_idx = idx\n",
        "\n",
        "            if max_min_idx != -1:\n",
        "                selected.append(max_min_idx)\n",
        "                remaining.remove(max_min_idx)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # N·∫øu ch∆∞a ƒë·ªß, th√™m ng·∫´u nhi√™n\n",
        "        if len(selected) < n_train:\n",
        "            add_more = np.random.choice(remaining, n_train - len(selected), replace=False)\n",
        "            selected.extend(add_more)\n",
        "            remaining = list(set(remaining) - set(add_more))\n",
        "\n",
        "        # Th√™m v√†o danh s√°ch train v√† test\n",
        "        train_indices.extend(cls_indices[selected])\n",
        "        test_indices.extend(cls_indices[remaining])\n",
        "\n",
        "    # Tr·ªôn d·ªØ li·ªáu\n",
        "    np.random.shuffle(train_indices)\n",
        "    np.random.shuffle(test_indices)\n",
        "\n",
        "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING AND PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"ƒê·ªçc d·ªØ li·ªáu t·ª´ Excel\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"B·∫ÆT ƒê·∫¶U ƒê·ªåC D·ªÆ LI·ªÜU\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Ki·ªÉm tra tr∆∞·ªùng h·ª£p n√†o\n",
        "    if SINGLE_FILE_PATH:\n",
        "        print(f\"üìÅ Tr∆∞·ªùng h·ª£p 1: ƒê·ªçc t·ª´ file ƒë∆°n: {SINGLE_FILE_PATH}\")\n",
        "        df = pd.read_excel(SINGLE_FILE_PATH)\n",
        "\n",
        "        # Ki·ªÉm tra c·ªôt Class\n",
        "        if 'Class' not in df.columns:\n",
        "            raise ValueError(\"File ph·∫£i c√≥ c·ªôt 'Class'!\")\n",
        "\n",
        "        print(f\"‚úì ƒê·ªçc th√†nh c√¥ng {len(df)} m·∫´u\")\n",
        "        print(f\"‚úì C√°c c·ªôt: {list(df.columns)}\")\n",
        "        print(f\"‚úì Ph√¢n b·ªë l·ªõp:\\n{df['Class'].value_counts().sort_index()}\")\n",
        "\n",
        "        # T√°ch Class\n",
        "        y = df['Class'].values\n",
        "\n",
        "        # L·∫•y features v√† chuy·ªÉn sang numeric\n",
        "        print(\"\\nüîÑ ƒêang chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu sang numeric...\")\n",
        "        feature_df = df.drop('Class', axis=1).apply(pd.to_numeric, errors='coerce')\n",
        "        feature_names = feature_df.columns.tolist()\n",
        "\n",
        "        # Ki·ªÉm tra NaN\n",
        "        nan_count = feature_df.isna().sum().sum()\n",
        "        if nan_count > 0:\n",
        "            print(f\"‚ö†Ô∏è  Ph√°t hi·ªán {nan_count} gi√° tr·ªã NaN - s·∫Ω thay b·∫±ng median\")\n",
        "            for col in feature_df.columns:\n",
        "                median_val = feature_df[col].median()\n",
        "                if pd.isna(median_val):\n",
        "                    median_val = 0\n",
        "                feature_df[col].fillna(median_val, inplace=True)\n",
        "\n",
        "        X = feature_df.values.astype(np.float64)\n",
        "        print(f\"‚úì Chuy·ªÉn ƒë·ªïi th√†nh c√¥ng: {X.shape[1]} features\")\n",
        "\n",
        "        print(f\"\\nüîÑ Chia d·ªØ li·ªáu v·ªõi SPXY (Train: {TRAIN_RATIO*100}%, Test: {(1-TRAIN_RATIO)*100}%)\")\n",
        "        X_train, X_test, y_train, y_test = spxy_split(X, y, train_size=TRAIN_RATIO, random_state=RANDOM_STATE)\n",
        "\n",
        "        print(f\"‚úì Train: {len(X_train)} m·∫´u, Test: {len(X_test)} m·∫´u\")\n",
        "\n",
        "        # L∆∞u file train v√† test\n",
        "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "        train_df = pd.DataFrame(X_train, columns=feature_names)\n",
        "        train_df['Class'] = y_train\n",
        "        train_df.to_excel(f\"{OUTPUT_DIR}/train_data.xlsx\", index=False)\n",
        "        print(f\"‚úì ƒê√£ l∆∞u: {OUTPUT_DIR}/train_data.xlsx\")\n",
        "\n",
        "        test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "        test_df['Class'] = y_test\n",
        "        test_df.to_excel(f\"{OUTPUT_DIR}/test_data.xlsx\", index=False)\n",
        "        print(f\"‚úì ƒê√£ l∆∞u: {OUTPUT_DIR}/test_data.xlsx\")\n",
        "\n",
        "    elif TRAIN_FILE_PATH and TEST_FILE_PATH:\n",
        "        print(f\"üìÅ Tr∆∞·ªùng h·ª£p 2: ƒê·ªçc t·ª´ 2 files ƒë√£ chia s·∫µn\")\n",
        "        print(f\"   Train: {TRAIN_FILE_PATH}\")\n",
        "        print(f\"   Test: {TEST_FILE_PATH}\")\n",
        "\n",
        "        train_df = pd.read_excel(TRAIN_FILE_PATH)\n",
        "        test_df = pd.read_excel(TEST_FILE_PATH)\n",
        "\n",
        "        # Ki·ªÉm tra c·ªôt Class\n",
        "        if 'Class' not in train_df.columns or 'Class' not in test_df.columns:\n",
        "            raise ValueError(\"C·∫£ 2 file ph·∫£i c√≥ c·ªôt 'Class'!\")\n",
        "\n",
        "        print(f\"‚úì Train: {len(train_df)} m·∫´u\")\n",
        "        print(f\"‚úì Test: {len(test_df)} m·∫´u\")\n",
        "\n",
        "        # T√°ch features v√† class\n",
        "        y_train = train_df['Class'].values\n",
        "        y_test = test_df['Class'].values\n",
        "\n",
        "        # L·∫•y feature columns\n",
        "        feature_cols = train_df.drop('Class', axis=1)\n",
        "        feature_names = feature_cols.columns.tolist()\n",
        "\n",
        "        # Chuy·ªÉn ƒë·ªïi t·∫•t c·∫£ features sang numeric, thay th·∫ø non-numeric th√†nh NaN\n",
        "        print(\"\\nüîÑ ƒêang chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu sang numeric...\")\n",
        "        X_train_df = train_df.drop('Class', axis=1).apply(pd.to_numeric, errors='coerce')\n",
        "        X_test_df = test_df.drop('Class', axis=1).apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "        # Ki·ªÉm tra NaN values\n",
        "        train_nans = X_train_df.isna().sum().sum()\n",
        "        test_nans = X_test_df.isna().sum().sum()\n",
        "\n",
        "        if train_nans > 0 or test_nans > 0:\n",
        "            print(f\"‚ö†Ô∏è  Ph√°t hi·ªán {train_nans} NaN trong train v√† {test_nans} NaN trong test\")\n",
        "            print(\"‚ö†Ô∏è  C√°c gi√° tr·ªã NaN s·∫Ω ƒë∆∞·ª£c thay th·∫ø b·∫±ng median c·ªßa feature ƒë√≥\")\n",
        "\n",
        "            # Thay th·∫ø NaN b·∫±ng median\n",
        "            for col in X_train_df.columns:\n",
        "                median_val = X_train_df[col].median()\n",
        "                if pd.isna(median_val):  # N·∫øu c·∫£ c·ªôt ƒë·ªÅu l√† NaN\n",
        "                    median_val = 0\n",
        "                X_train_df[col].fillna(median_val, inplace=True)\n",
        "                X_test_df[col].fillna(median_val, inplace=True)\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        X_train = X_train_df.values.astype(np.float64)\n",
        "        X_test = X_test_df.values.astype(np.float64)\n",
        "\n",
        "        print(f\"‚úì Chuy·ªÉn ƒë·ªïi th√†nh c√¥ng: {X_train.shape[1]} features\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Vui l√≤ng cung c·∫•p ƒë∆∞·ªùng d·∫´n file!\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, feature_names\n",
        "\n",
        "def plot_data_distribution(X_train, X_test, y_train, y_test, feature_names):\n",
        "    \"\"\"V·∫Ω bi·ªÉu ƒë·ªì Bar chart Train vs Test (Mean ¬± Std) cho t·∫•t c·∫£ features\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"T·∫†O BI·ªÇU ƒê·ªí PH√ÇN B·ªê D·ªÆ LI·ªÜU\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    n_features = len(feature_names)\n",
        "\n",
        "    # T√≠nh mean v√† std cho train v√† test\n",
        "    train_means = []\n",
        "    train_stds = []\n",
        "    test_means = []\n",
        "    test_stds = []\n",
        "\n",
        "    for i in range(n_features):\n",
        "        train_means.append(X_train[:, i].mean())\n",
        "        train_stds.append(X_train[:, i].std())\n",
        "        test_means.append(X_test[:, i].mean())\n",
        "        test_stds.append(X_test[:, i].std())\n",
        "\n",
        "    # T·∫°o figure\n",
        "    fig, ax = plt.subplots(figsize=(20, 6))\n",
        "\n",
        "    # V·ªã tr√≠ c√°c bars\n",
        "    x = np.arange(n_features)\n",
        "    width = 0.35\n",
        "\n",
        "    # V·∫Ω bars v·ªõi error bars\n",
        "    bars1 = ax.bar(x - width/2, train_means, width, yerr=train_stds,\n",
        "                   label='Train (Mean ¬± Std)', color='cornflowerblue',\n",
        "                   capsize=3, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
        "    bars2 = ax.bar(x + width/2, test_means, width, yerr=test_stds,\n",
        "                   label='Test (Mean ¬± Std)', color='coral',\n",
        "                   capsize=3, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
        "\n",
        "    # Thi·∫øt l·∫≠p labels v√† title\n",
        "    ax.set_xlabel('Features', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Values', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Data Distribution: Train vs Test (Mean ¬± Std)', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(feature_names, rotation=45, ha='right', fontsize=9)\n",
        "    ax.legend(loc='upper right', fontsize=11)\n",
        "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "    # Tight layout\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{OUTPUT_DIR}/data_distribution.png\", dpi=300, bbox_inches='tight')\n",
        "    print(f\"‚úì ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: {OUTPUT_DIR}/data_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL TRAINING WITH OPTUNA\n",
        "# ============================================================================\n",
        "\n",
        "def get_model_configs():\n",
        "    \"\"\"ƒê·ªãnh nghƒ©a t·∫•t c·∫£ c√°c m√¥ h√¨nh ML\"\"\"\n",
        "    return {\n",
        "        # Support Vector Machines\n",
        "        \"SVM_linear\": {\"family\": \"SVM\", \"model_class\": SVC, \"kernel\": \"linear\"},\n",
        "        \"SVM_rbf\": {\"family\": \"SVM\", \"model_class\": SVC, \"kernel\": \"rbf\"},\n",
        "        \"SVM_poly\": {\"family\": \"SVM\", \"model_class\": SVC, \"kernel\": \"poly\"},\n",
        "        \"SVM_sigmoid\": {\"family\": \"SVM\", \"model_class\": SVC, \"kernel\": \"sigmoid\"},\n",
        "        \"NuSVC_linear\": {\"family\": \"SVM\", \"model_class\": NuSVC, \"kernel\": \"linear\"},\n",
        "        \"NuSVC_rbf\": {\"family\": \"SVM\", \"model_class\": NuSVC, \"kernel\": \"rbf\"},\n",
        "        \"NuSVC_poly\": {\"family\": \"SVM\", \"model_class\": NuSVC, \"kernel\": \"poly\"},\n",
        "        \"NuSVC_sigmoid\": {\"family\": \"SVM\", \"model_class\": NuSVC, \"kernel\": \"sigmoid\"},\n",
        "        \"LinearSVC\": {\"family\": \"SVM\", \"model_class\": LinearSVC, \"kernel\": None},\n",
        "\n",
        "        # Tree-based Models\n",
        "        \"DecisionTree\": {\"family\": \"Tree\", \"model_class\": DecisionTreeClassifier},\n",
        "        \"RandomForest\": {\"family\": \"Tree\", \"model_class\": RandomForestClassifier},\n",
        "        \"ExtraTrees\": {\"family\": \"Tree\", \"model_class\": ExtraTreesClassifier},\n",
        "        \"GradientBoosting\": {\"family\": \"Tree\", \"model_class\": GradientBoostingClassifier},\n",
        "        \"AdaBoost\": {\"family\": \"Tree\", \"model_class\": AdaBoostClassifier},\n",
        "\n",
        "        # Neural Networks\n",
        "        \"MLP_relu\": {\"family\": \"Neural Network\", \"model_class\": MLPClassifier, \"activation\": \"relu\"},\n",
        "        \"MLP_tanh\": {\"family\": \"Neural Network\", \"model_class\": MLPClassifier, \"activation\": \"tanh\"},\n",
        "        \"MLP_logistic\": {\"family\": \"Neural Network\", \"model_class\": MLPClassifier, \"activation\": \"logistic\"},\n",
        "\n",
        "        # K-Nearest Neighbors\n",
        "        \"KNN_uniform\": {\"family\": \"KNN\", \"model_class\": KNeighborsClassifier, \"weights\": \"uniform\"},\n",
        "        \"KNN_distance\": {\"family\": \"KNN\", \"model_class\": KNeighborsClassifier, \"weights\": \"distance\"},\n",
        "\n",
        "        # Discriminant Analysis\n",
        "        \"LDA\": {\"family\": \"Discriminant\", \"model_class\": LinearDiscriminantAnalysis},\n",
        "        \"QDA\": {\"family\": \"Discriminant\", \"model_class\": QuadraticDiscriminantAnalysis},\n",
        "\n",
        "        # Naive Bayes\n",
        "        \"GaussianNB\": {\"family\": \"Naive Bayes\", \"model_class\": GaussianNB},\n",
        "        \"BernoulliNB\": {\"family\": \"Naive Bayes\", \"model_class\": BernoulliNB},\n",
        "    }\n",
        "\n",
        "def objective_svc(trial, X_train, y_train, kernel):\n",
        "    \"\"\"Objective function cho SVC v·ªõi pruning\"\"\"\n",
        "    params = {\n",
        "        'C': trial.suggest_float('C', 0.1, 100, log=True),\n",
        "        'kernel': kernel,\n",
        "        'random_state': RANDOM_STATE\n",
        "    }\n",
        "\n",
        "    if kernel == 'rbf' or kernel == 'sigmoid':\n",
        "        params['gamma'] = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
        "    elif kernel == 'poly':\n",
        "        params['degree'] = trial.suggest_int('degree', 2, 5)\n",
        "        params['gamma'] = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
        "        params['coef0'] = trial.suggest_float('coef0', 0.0, 10.0)\n",
        "\n",
        "    model = SVC(**params)\n",
        "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "    # Cross-validation v·ªõi pruning\n",
        "    scores = []\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
        "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
        "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "        model.fit(X_tr, y_tr)\n",
        "        score = model.score(X_val, y_val)\n",
        "        scores.append(score)\n",
        "\n",
        "        # Report intermediate value cho pruning\n",
        "        trial.report(score, fold)\n",
        "\n",
        "        # Pruning: d·ª´ng s·ªõm n·∫øu k·∫øt qu·∫£ k√©m\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "def objective_nusvc(trial, X_train, y_train, kernel):\n",
        "    \"\"\"Objective function cho NuSVC v·ªõi pruning\"\"\"\n",
        "    params = {\n",
        "        'nu': trial.suggest_float('nu', 0.01, 0.99),\n",
        "        'kernel': kernel,\n",
        "        'random_state': RANDOM_STATE\n",
        "    }\n",
        "\n",
        "    if kernel == 'rbf' or kernel == 'sigmoid':\n",
        "        params['gamma'] = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
        "    elif kernel == 'poly':\n",
        "        params['degree'] = trial.suggest_int('degree', 2, 5)\n",
        "        params['gamma'] = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
        "        params['coef0'] = trial.suggest_float('coef0', 0.0, 10.0)\n",
        "\n",
        "    model = NuSVC(**params)\n",
        "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "    scores = []\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
        "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
        "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "        model.fit(X_tr, y_tr)\n",
        "        score = model.score(X_val, y_val)\n",
        "        scores.append(score)\n",
        "\n",
        "        trial.report(score, fold)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "def objective_tree(trial, X_train, y_train, model_class):\n",
        "    \"\"\"Objective function cho Tree-based models\"\"\"\n",
        "    if model_class == DecisionTreeClassifier:\n",
        "        params = {\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
        "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "            'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
        "            'random_state': RANDOM_STATE\n",
        "        }\n",
        "    elif model_class in [RandomForestClassifier, ExtraTreesClassifier]:\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
        "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "            'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
        "            'random_state': RANDOM_STATE,\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "    elif model_class == GradientBoostingClassifier:\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
        "            'random_state': RANDOM_STATE\n",
        "        }\n",
        "    else:  # AdaBoost\n",
        "        params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 2.0, log=True),\n",
        "            'random_state': RANDOM_STATE\n",
        "        }\n",
        "\n",
        "    model = model_class(**params)\n",
        "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    scores = cross_validate(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    return scores['test_score'].mean()\n",
        "\n",
        "def objective_mlp(trial, X_train, y_train, activation):\n",
        "    \"\"\"Objective function cho MLP\"\"\"\n",
        "    hidden_layer_sizes = []\n",
        "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
        "    for i in range(n_layers):\n",
        "        hidden_layer_sizes.append(trial.suggest_int(f'n_units_l{i}', 10, 200))\n",
        "\n",
        "    params = {\n",
        "        'hidden_layer_sizes': tuple(hidden_layer_sizes),\n",
        "        'activation': activation,\n",
        "        'solver': trial.suggest_categorical('solver', ['adam', 'sgd']),\n",
        "        'alpha': trial.suggest_float('alpha', 1e-5, 1e-1, log=True),\n",
        "        'learning_rate': trial.suggest_categorical('learning_rate', ['constant', 'adaptive']),\n",
        "        'max_iter': 1000,\n",
        "        'random_state': RANDOM_STATE\n",
        "    }\n",
        "\n",
        "    model = MLPClassifier(**params)\n",
        "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    scores = cross_validate(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    return scores['test_score'].mean()\n",
        "\n",
        "def objective_knn(trial, X_train, y_train, weights):\n",
        "    \"\"\"Objective function cho KNN\"\"\"\n",
        "    params = {\n",
        "        'n_neighbors': trial.suggest_int('n_neighbors', 3, 30),\n",
        "        'weights': weights,\n",
        "        'metric': trial.suggest_categorical('metric', ['euclidean', 'manhattan', 'minkowski']),\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "\n",
        "    if params['metric'] == 'minkowski':\n",
        "        params['p'] = trial.suggest_int('p', 1, 5)\n",
        "\n",
        "    model = KNeighborsClassifier(**params)\n",
        "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    scores = cross_validate(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    return scores['test_score'].mean()\n",
        "\n",
        "def objective_lda(trial, X_train, y_train):\n",
        "    \"\"\"Objective function cho LDA\"\"\"\n",
        "    params = {\n",
        "        'solver': trial.suggest_categorical('solver', ['svd', 'lsqr', 'eigen']),\n",
        "    }\n",
        "\n",
        "    if params['solver'] in ['lsqr', 'eigen']:\n",
        "        params['shrinkage'] = trial.suggest_float('shrinkage', 0.0, 1.0)\n",
        "\n",
        "    model = LinearDiscriminantAnalysis(**params)\n",
        "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    scores = cross_validate(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    return scores['test_score'].mean()\n",
        "\n",
        "def objective_qda(trial, X_train, y_train):\n",
        "    \"\"\"Objective function cho QDA\"\"\"\n",
        "    params = {\n",
        "        'reg_param': trial.suggest_float('reg_param', 0.0, 1.0),\n",
        "    }\n",
        "\n",
        "    model = QuadraticDiscriminantAnalysis(**params)\n",
        "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    scores = cross_validate(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    return scores['test_score'].mean()\n",
        "\n",
        "def objective_naive_bayes(trial, X_train, y_train, model_class):\n",
        "    \"\"\"Objective function cho Naive Bayes\"\"\"\n",
        "    if model_class == GaussianNB:\n",
        "        params = {\n",
        "            'var_smoothing': trial.suggest_float('var_smoothing', 1e-10, 1e-5, log=True)\n",
        "        }\n",
        "    elif model_class == BernoulliNB:\n",
        "        params = {\n",
        "            'alpha': trial.suggest_float('alpha', 0.1, 10.0, log=True),\n",
        "            'binarize': trial.suggest_float('binarize', 0.0, 1.0)\n",
        "        }\n",
        "\n",
        "    model = model_class(**params)\n",
        "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    scores = cross_validate(model, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    return scores['test_score'].mean()\n",
        "\n",
        "def objective_linearsvc(trial, X_train, y_train):\n",
        "    \"\"\"Objective function cho LinearSVC v·ªõi pruning\"\"\"\n",
        "    params = {\n",
        "        'C': trial.suggest_float('C', 0.1, 100, log=True),\n",
        "        'loss': trial.suggest_categorical('loss', ['hinge', 'squared_hinge']),\n",
        "        'max_iter': 10000,\n",
        "        'random_state': RANDOM_STATE\n",
        "    }\n",
        "\n",
        "    model = LinearSVC(**params)\n",
        "    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "    scores = []\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
        "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
        "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "        model.fit(X_tr, y_tr)\n",
        "        score = model.score(X_val, y_val)\n",
        "        scores.append(score)\n",
        "\n",
        "        trial.report(score, fold)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "def train_single_model(model_name, config, X_train, y_train, X_test, y_test, scaler):\n",
        "    \"\"\"Train m·ªôt model v·ªõi Optuna\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        # T·∫°o study Optuna v·ªõi Pruner ƒë·ªÉ early stopping\n",
        "        study = optuna.create_study(\n",
        "            direction='maximize',\n",
        "            sampler=TPESampler(seed=RANDOM_STATE),\n",
        "            pruner=MedianPruner(\n",
        "                n_startup_trials=MIN_TRIALS_BEFORE_PRUNING,\n",
        "                n_warmup_steps=PRUNING_WARMUP_STEPS,\n",
        "                interval_steps=1\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Optimize d·ª±a tr√™n family\n",
        "        model_class = config['model_class']\n",
        "\n",
        "        if model_class in [SVC, NuSVC]:\n",
        "            study.optimize(\n",
        "                lambda trial: objective_svc(trial, X_train, y_train, config['kernel']) if model_class == SVC\n",
        "                else objective_nusvc(trial, X_train, y_train, config['kernel']),\n",
        "                n_trials=N_TRIALS,\n",
        "                timeout=OPTUNA_TIMEOUT,\n",
        "                show_progress_bar=False  # T·∫Øt ƒë·ªÉ gi·∫£m spam\n",
        "            )\n",
        "        elif model_class == LinearSVC:\n",
        "            study.optimize(\n",
        "                lambda trial: objective_linearsvc(trial, X_train, y_train),\n",
        "                n_trials=N_TRIALS,\n",
        "                timeout=OPTUNA_TIMEOUT,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "        elif config['family'] == 'Tree':\n",
        "            study.optimize(\n",
        "                lambda trial: objective_tree(trial, X_train, y_train, model_class),\n",
        "                n_trials=N_TRIALS,\n",
        "                timeout=OPTUNA_TIMEOUT,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "        elif model_class == MLPClassifier:\n",
        "            study.optimize(\n",
        "                lambda trial: objective_mlp(trial, X_train, y_train, config['activation']),\n",
        "                n_trials=N_TRIALS,\n",
        "                timeout=OPTUNA_TIMEOUT,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "        elif model_class == KNeighborsClassifier:\n",
        "            study.optimize(\n",
        "                lambda trial: objective_knn(trial, X_train, y_train, config['weights']),\n",
        "                n_trials=N_TRIALS,\n",
        "                timeout=OPTUNA_TIMEOUT,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "        elif model_class == LinearDiscriminantAnalysis:\n",
        "            study.optimize(\n",
        "                lambda trial: objective_lda(trial, X_train, y_train),\n",
        "                n_trials=N_TRIALS,\n",
        "                timeout=OPTUNA_TIMEOUT,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "        elif model_class == QuadraticDiscriminantAnalysis:\n",
        "            study.optimize(\n",
        "                lambda trial: objective_qda(trial, X_train, y_train),\n",
        "                n_trials=N_TRIALS,\n",
        "                timeout=OPTUNA_TIMEOUT,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "        elif model_class in [GaussianNB, BernoulliNB]:\n",
        "            study.optimize(\n",
        "                lambda trial: objective_naive_bayes(trial, X_train, y_train, model_class),\n",
        "                n_trials=N_TRIALS,\n",
        "                timeout=OPTUNA_TIMEOUT,\n",
        "                show_progress_bar=False\n",
        "            )\n",
        "\n",
        "        # L·∫•y best params\n",
        "        best_params = study.best_params\n",
        "\n",
        "        # Th·ªëng k√™ trials\n",
        "        n_complete = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])\n",
        "        n_pruned = len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])\n",
        "        n_failed = len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL])\n",
        "\n",
        "        print(f\"‚úì Trials: {n_complete} ho√†n th√†nh, {n_pruned} b·ªã prune (d·ª´ng s·ªõm), {n_failed} th·∫•t b·∫°i\")\n",
        "        print(f\"‚úì Best params: {best_params}\")\n",
        "        print(f\"‚úì Best CV score: {study.best_value:.4f}\")\n",
        "\n",
        "        # Train model v·ªõi best params\n",
        "        if 'kernel' in config and config['kernel']:\n",
        "            best_params['kernel'] = config['kernel']\n",
        "        if 'activation' in config:\n",
        "            best_params['activation'] = config['activation']\n",
        "        if 'weights' in config:\n",
        "            best_params['weights'] = config['weights']\n",
        "        if model_class == LinearSVC:\n",
        "            best_params['max_iter'] = 10000\n",
        "        if model_class == MLPClassifier:\n",
        "            best_params['max_iter'] = 1000\n",
        "\n",
        "        # Lo·∫°i b·ªè c√°c params kh√¥ng h·ª£p l·ªá\n",
        "        params_to_remove = []\n",
        "\n",
        "        # MLP: n_layers v√† n_units_l* l√† params trung gian, kh√¥ng ph·∫£i params c·ªßa MLPClassifier\n",
        "        if model_class == MLPClassifier:\n",
        "            params_to_remove.extend([k for k in best_params.keys() if k.startswith('n_units_l') or k == 'n_layers'])\n",
        "\n",
        "        # KNN kh√¥ng c√≥ random_state\n",
        "        if model_class == KNeighborsClassifier:\n",
        "            params_to_remove.append('random_state')\n",
        "\n",
        "        # Naive Bayes kh√¥ng c√≥ random_state\n",
        "        if model_class in [GaussianNB, BernoulliNB]:\n",
        "            params_to_remove.append('random_state')\n",
        "\n",
        "        # LDA/QDA kh√¥ng c√≥ random_state\n",
        "        if model_class in [LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis]:\n",
        "            params_to_remove.append('random_state')\n",
        "\n",
        "        # X√≥a c√°c params kh√¥ng h·ª£p l·ªá\n",
        "        for param in params_to_remove:\n",
        "            best_params.pop(param, None)\n",
        "\n",
        "        # Th√™m random_state cho c√°c model support\n",
        "        if model_class not in [KNeighborsClassifier, GaussianNB, BernoulliNB,\n",
        "                                LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis]:\n",
        "            best_params['random_state'] = RANDOM_STATE\n",
        "\n",
        "        model = model_class(**best_params)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Cross-validation metrics\n",
        "        cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "        cv_results = cross_validate(\n",
        "            model, X_train, y_train, cv=cv,\n",
        "            scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],\n",
        "            n_jobs=-1\n",
        "        )\n",
        "\n",
        "        # Test metrics\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Prediction speed\n",
        "        start_time = time.time()\n",
        "        _ = model.predict(X_test)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        pred_speed = len(X_test) / elapsed_time if elapsed_time > 0 else 0\n",
        "\n",
        "        # Model size\n",
        "        model_size = len(pickle.dumps(model))\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        results = {\n",
        "            'Algorithm Family': config.get('family', 'SVM'),\n",
        "            'Model': model_name,\n",
        "            'Kernel/Type': config.get('kernel') or config.get('activation') or config.get('weights') or 'N/A',\n",
        "            'Best params (JSON)': json.dumps(best_params),\n",
        "            'CV Train Accuracy': cv_results['test_accuracy'].mean(),\n",
        "            'CV Train Precision': cv_results['test_precision_macro'].mean(),\n",
        "            'CV Train Recall': cv_results['test_recall_macro'].mean(),\n",
        "            'CV Train F1': cv_results['test_f1_macro'].mean(),\n",
        "            'Test Accuracy': accuracy_score(y_test, y_pred),\n",
        "            'Test Precision': precision_score(y_test, y_pred, average='macro'),\n",
        "            'Test Recall': recall_score(y_test, y_pred, average='macro'),\n",
        "            'Test F1': f1_score(y_test, y_pred, average='macro'),\n",
        "            'Prediction speed (obs/sec)': pred_speed,\n",
        "            'Model size (bytes)': model_size\n",
        "        }\n",
        "\n",
        "        print(f\"‚úì Test Accuracy: {results['Test Accuracy']:.4f}\")\n",
        "        print(f\"‚úì Test Precision: {results['Test Precision']:.4f}\")\n",
        "        print(f\"‚úì Test F1: {results['Test F1']:.4f}\")\n",
        "\n",
        "        return model, results, cm\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå L·ªñI khi train {model_name}!\")\n",
        "        print(f\"   Lo·∫°i l·ªói: {type(e).__name__}\")\n",
        "        print(f\"   Chi ti·∫øt: {str(e)[:200]}\")\n",
        "        import traceback\n",
        "        print(f\"   Traceback (r√∫t g·ªçn):\")\n",
        "        tb_lines = traceback.format_exc().split('\\n')\n",
        "        for line in tb_lines[-5:]:  # Ch·ªâ hi·ªÉn 5 d√≤ng cu·ªëi\n",
        "            if line.strip():\n",
        "                print(f\"     {line}\")\n",
        "\n",
        "        # Tr·∫£ v·ªÅ None ƒë·ªÉ b·ªè qua model n√†y\n",
        "        return None, None, None\n",
        "\n",
        "def train_all_models(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Train t·∫•t c·∫£ c√°c models\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"B·∫ÆT ƒê·∫¶U TRAINING T·∫§T C·∫¢ C√ÅC MODELS\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # T·∫Øt optuna logging ƒë·ªÉ gi·∫£m spam\n",
        "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "    # Chu·∫©n h√≥a d·ªØ li·ªáu\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    model_configs = get_model_configs()\n",
        "\n",
        "    all_results = []\n",
        "    trained_models = {}\n",
        "    confusion_matrices = {}\n",
        "\n",
        "    for model_name, config in model_configs.items():\n",
        "        try:\n",
        "            model, results, cm = train_single_model(\n",
        "                model_name, config, X_train_scaled, y_train, X_test_scaled, y_test, scaler\n",
        "            )\n",
        "\n",
        "            # Ch·ªâ th√™m v√†o k·∫øt qu·∫£ n·∫øu train th√†nh c√¥ng\n",
        "            if model is not None and results is not None:\n",
        "                all_results.append(results)\n",
        "                trained_models[model_name] = {'model': model, 'scaler': scaler, 'family': config.get('family', 'SVM')}\n",
        "                confusion_matrices[model_name] = cm\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  B·ªè qua {model_name} do train th·∫•t b·∫°i\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå L·ªói ngo·∫°i l·ªá khi train {model_name}: {e}\\n\")\n",
        "            continue\n",
        "\n",
        "    return all_results, trained_models, confusion_matrices\n",
        "\n",
        "def save_results(all_results, confusion_matrices):\n",
        "    \"\"\"L∆∞u k·∫øt qu·∫£ training v√†o Excel v√† plot confusion matrices\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"L∆Ø∆Ø KEÃÅT QUAÃâ\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # T·∫°o th∆∞ m·ª•c output\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # L∆∞u k·∫øt qu·∫£ v√†o Excel\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    # K·∫øt qu·∫£ s·∫Ω ƒë∆∞·ª£c g·∫Øc v√†o results.xlsx khi d√πng Gradio\n",
        "\n",
        "    # Hi·ªÉn th·ªã top 10 models theo Test Precision\n",
        "    print(\"\\nTop 10 models theo Test Precision:\")\n",
        "    top_models = results_df.nlargest(10, 'Test Precision')[['Model', 'Test Precision', 'Test Accuracy', 'Test F1']]\n",
        "    print(top_models.to_string(index=False))\n",
        "\n",
        "    # V·∫Ω confusion matrices\n",
        "    cm_dir = f\"{OUTPUT_DIR}/confusion_matrices\"\n",
        "    os.makedirs(cm_dir, exist_ok=True)\n",
        "\n",
        "    for model_name, cm in confusion_matrices.items():\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title(f'Confusion Matrix - {model_name}')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        safe_name = model_name.replace('-', '_').replace(' ', '_')\n",
        "        plt.savefig(f\"{cm_dir}/{safe_name}_confusion_matrix.png\", dpi=100, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    print(f\"‚úì ƒê√£ l∆∞u {len(confusion_matrices)} confusion matrices t·∫°i: {cm_dir}/\")\n",
        "\n",
        "def select_models_for_arduino(all_results, trained_models):\n",
        "    \"\"\"Cho ph√©p ng∆∞·ªùi d√πng ch·ªçn models ƒë·ªÉ t·∫°o Arduino library\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"CH·ªåN MODELS ƒê·ªÇ T·∫†O ARDUINO LIBRARY\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # S·∫Øp x·∫øp theo Test Precision\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    results_df = results_df.sort_values('Test Precision', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # Hi·ªÉn th·ªã b·∫£ng v·ªõi format ƒë·∫πp h∆°n\n",
        "    print(\"\\nüìä DANH S√ÅCH T·∫§T C·∫¢ C√ÅC MODELS (s·∫Øp x·∫øp theo Test Precision):\")\n",
        "    print(\"=\" * 100)\n",
        "    print(f\"{'STT':<5} {'Model':<30} {'Family':<18} {'Precision':<12} {'Accuracy':<12} {'F1-Score':<12}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for idx, row in results_df.iterrows():\n",
        "        print(f\"{idx+1:<5} {row['Model']:<30} {row['Algorithm Family']:<18} \"\n",
        "              f\"{row['Test Precision']:<12.4f} {row['Test Accuracy']:<12.4f} {row['Test F1']:<12.4f}\")\n",
        "\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "    # Ch·ªçn models\n",
        "    selected_models = {}\n",
        "\n",
        "    if IN_COLAB:\n",
        "        # GOOGLE COLAB: S·ª≠ d·ª•ng input() - s·∫Ω hi·ªán input box ngay d∆∞·ªõi cell\n",
        "        print(\"\\nüìå H∆Ø·ªöNG D·∫™N CH·ªåN MODELS:\")\n",
        "        print(\"   ‚Ä¢ Nh·∫•n Enter (ƒë·ªÉ tr·ªëng): T·ª± ƒë·ªông ch·ªçn 10 models t·ªët nh·∫•t\")\n",
        "        print(\"   ‚Ä¢ Nh·∫≠p c√°c s·ªë: Ch·ªçn models theo STT (c√≥ th·ªÉ ch·ªçn bao nhi√™u t√πy √Ω)\")\n",
        "        print(\"     V√≠ d·ª•: '1 2 3 4 5' ho·∫∑c '1,2,3,4,5' (ch·ªçn 5 models ƒë·∫ßu)\")\n",
        "        print(\"     V√≠ d·ª•: '1 5 10 15' (ch·ªçn 4 models c·ª• th·ªÉ)\")\n",
        "        print()\n",
        "\n",
        "        user_input = input(\"üëâ Nh·∫≠p STT c√°c models (c√°ch nhau b·ªüi d·∫•u c√°ch ho·∫∑c d·∫•u ph·∫©y): \").strip()\n",
        "\n",
        "        # X·ª≠ l√Ω input\n",
        "        if not user_input:\n",
        "            # AUTO MODE - kh√¥ng nh·∫≠p g√¨\n",
        "            top_n = min(10, len(results_df))\n",
        "            print(f\"\\n‚úÖ Ch·∫ø ƒë·ªô AUTO: ƒê√£ ch·ªçn {top_n} models c√≥ Test Precision cao nh·∫•t\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "            for i in range(top_n):\n",
        "                model_name = results_df.iloc[i]['Model']\n",
        "                selected_models[model_name] = trained_models[model_name]\n",
        "                print(f\"   {i+1:2d}. {model_name:<30} ‚Üí Precision: {results_df.iloc[i]['Test Precision']:.4f}\")\n",
        "        else:\n",
        "            # MANUAL MODE - nh·∫≠p c√°c s·ªë\n",
        "            try:\n",
        "                # Parse numbers - h·ªó tr·ª£ c·∫£ d·∫•u c√°ch v√† d·∫•u ph·∫©y\n",
        "                user_input = user_input.replace(',', ' ')  # Chuy·ªÉn d·∫•u ph·∫©y th√†nh d·∫•u c√°ch\n",
        "                numbers = []\n",
        "\n",
        "                for x in user_input.split():\n",
        "                    x = x.strip()\n",
        "                    if x.isdigit():\n",
        "                        num = int(x)\n",
        "                        if 1 <= num <= len(results_df):\n",
        "                            if num not in numbers:  # Tr√°nh tr√πng l·∫∑p\n",
        "                                numbers.append(num)\n",
        "                        else:\n",
        "                            print(f\"‚ö†Ô∏è  B·ªè qua s·ªë {num} (ngo√†i ph·∫°m vi 1-{len(results_df)})\")\n",
        "\n",
        "                if numbers:\n",
        "                    print(f\"\\n‚úÖ ƒê√£ ch·ªçn {len(numbers)} models:\")\n",
        "                    print(\"-\" * 80)\n",
        "\n",
        "                    for num in sorted(numbers):  # S·∫Øp x·∫øp ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp\n",
        "                        model_name = results_df.iloc[num - 1]['Model']\n",
        "                        selected_models[model_name] = trained_models[model_name]\n",
        "                        print(f\"   {num:2d}. {model_name:<30} ‚Üí Precision: {results_df.iloc[num - 1]['Test Precision']:.4f}\")\n",
        "                else:\n",
        "                    # Kh√¥ng c√≥ s·ªë h·ª£p l·ªá -> fallback AUTO\n",
        "                    print(f\"\\n‚ö†Ô∏è  Kh√¥ng c√≥ s·ªë h·ª£p l·ªá n√†o! S·ª≠ d·ª•ng AUTO mode (10 models t·ªët nh·∫•t)...\")\n",
        "                    print(\"-\" * 80)\n",
        "                    top_n = min(10, len(results_df))\n",
        "                    for i in range(top_n):\n",
        "                        model_name = results_df.iloc[i]['Model']\n",
        "                        selected_models[model_name] = trained_models[model_name]\n",
        "                        print(f\"   {i+1:2d}. {model_name:<30} ‚Üí Precision: {results_df.iloc[i]['Test Precision']:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n‚ö†Ô∏è  L·ªói khi x·ª≠ l√Ω input: {e}\")\n",
        "                print(\"    S·ª≠ d·ª•ng AUTO mode (10 models t·ªët nh·∫•t)...\")\n",
        "                print(\"-\" * 80)\n",
        "                top_n = min(10, len(results_df))\n",
        "                for i in range(top_n):\n",
        "                    model_name = results_df.iloc[i]['Model']\n",
        "                    selected_models[model_name] = trained_models[model_name]\n",
        "                    print(f\"   {i+1:2d}. {model_name:<30} ‚Üí Precision: {results_df.iloc[i]['Test Precision']:.4f}\")\n",
        "\n",
        "    else:\n",
        "        # LOCAL MODE: T·ª± ƒë·ªông ch·ªçn AUTO\n",
        "        print(\"\\nüñ•Ô∏è  [Local mode] T·ª± ƒë·ªông s·ª≠ d·ª•ng AUTO mode - ch·ªçn 10 models t·ªët nh·∫•t\")\n",
        "        top_n = min(10, len(results_df))\n",
        "\n",
        "        for i in range(top_n):\n",
        "            model_name = results_df.iloc[i]['Model']\n",
        "            selected_models[model_name] = trained_models[model_name]\n",
        "            print(f\"   {i+1:2d}. {model_name:<30} ‚Üí Precision: {results_df.iloc[i]['Test Precision']:.4f}\")\n",
        "\n",
        "    # T·ªïng k·∫øt\n",
        "    print(\"\\n\" + \"=\" * 100)\n",
        "    print(f\"‚úÖ HO√ÄN T·∫§T: ƒê√£ ch·ªçn {len(selected_models)} models ƒë·ªÉ t·∫°o Arduino library\")\n",
        "    print(\"=\" * 100 + \"\\n\")\n",
        "\n",
        "    return selected_models\n",
        "\n",
        "def generate_svm_c_code(model, model_name, scaler, n_features, n_classes):\n",
        "    \"\"\"\n",
        "    Generate C code cho SVM models (linear, poly, rbf, sigmoid)\n",
        "    Thay th·∫ø m2cgen v·ªõi custom lightweight implementation\n",
        "    \"\"\"\n",
        "    safe_name = model_name.replace('-', '_').replace(' ', '_')\n",
        "\n",
        "    # Ki·ªÉm tra kernel type\n",
        "    if hasattr(model, 'kernel'):\n",
        "        kernel = model.kernel\n",
        "    else:\n",
        "        # LinearSVC kh√¥ng c√≥ kernel attribute\n",
        "        kernel = 'linear'\n",
        "\n",
        "    # L·∫•y gamma\n",
        "    if kernel != 'linear' and hasattr(model, 'gamma'):\n",
        "        gamma = model.gamma if isinstance(model.gamma, float) else model._gamma\n",
        "        if gamma == 'scale':\n",
        "            gamma = 1.0 / (n_features * model.support_vectors_.var())\n",
        "        elif gamma == 'auto':\n",
        "            gamma = 1.0 / n_features\n",
        "    else:\n",
        "        gamma = 1.0\n",
        "\n",
        "    coef0 = model.coef0 if hasattr(model, 'coef0') else 0.0\n",
        "    degree = model.degree if hasattr(model, 'degree') else 3\n",
        "\n",
        "    code_parts = []\n",
        "\n",
        "    # Scaler parameters\n",
        "    code_parts.append(f\"\"\"\n",
        "// ========================================\n",
        "// Model: {model_name} (Kernel: {kernel})\n",
        "// ========================================\n",
        "\n",
        "// Scaler parameters\n",
        "static const double scaler_mean_{safe_name}[{n_features}] = {{\n",
        "    {', '.join(f'{x:.10f}' for x in scaler.mean_)}\n",
        "}};\n",
        "\n",
        "static const double scaler_scale_{safe_name}[{n_features}] = {{\n",
        "    {', '.join(f'{x:.10f}' for x in scaler.scale_)}\n",
        "}};\n",
        "\n",
        "// Standardize features\n",
        "static inline void standardize_{safe_name}(const float* features, double* scaled) {{\n",
        "    for (int i = 0; i < {n_features}; i++) {{\n",
        "        scaled[i] = ((double)features[i] - scaler_mean_{safe_name}[i]) / scaler_scale_{safe_name}[i];\n",
        "    }}\n",
        "}}\n",
        "\"\"\")\n",
        "\n",
        "    # Kernel functions\n",
        "    if kernel == 'poly':\n",
        "        code_parts.append(f\"\"\"\n",
        "// Polynomial kernel\n",
        "static inline double poly_kernel_{safe_name}(const double* x, const double* sv) {{\n",
        "    double dot = 0.0;\n",
        "    for (int i = 0; i < {n_features}; i++) dot += x[i] * sv[i];\n",
        "    double result = {gamma} * dot + {coef0};\n",
        "    double powered = result;\n",
        "    for (int d = 1; d < {degree}; d++) powered *= result;\n",
        "    return powered;\n",
        "}}\n",
        "\"\"\")\n",
        "    elif kernel == 'rbf':\n",
        "        code_parts.append(f\"\"\"\n",
        "// RBF kernel\n",
        "static inline double rbf_kernel_{safe_name}(const double* x, const double* sv) {{\n",
        "    double dist_sq = 0.0;\n",
        "    for (int i = 0; i < {n_features}; i++) {{\n",
        "        double diff = x[i] - sv[i];\n",
        "        dist_sq += diff * diff;\n",
        "    }}\n",
        "    return exp(-{gamma} * dist_sq);\n",
        "}}\n",
        "\"\"\")\n",
        "    elif kernel == 'sigmoid':\n",
        "        code_parts.append(f\"\"\"\n",
        "// Sigmoid kernel\n",
        "static inline double sigmoid_kernel_{safe_name}(const double* x, const double* sv) {{\n",
        "    double dot = 0.0;\n",
        "    for (int i = 0; i < {n_features}; i++) dot += x[i] * sv[i];\n",
        "    return tanh({gamma} * dot + {coef0});\n",
        "}}\n",
        "\"\"\")\n",
        "    elif kernel == 'linear':\n",
        "        code_parts.append(f\"\"\"\n",
        "// Linear kernel (dot product)\n",
        "static inline double linear_kernel_{safe_name}(const double* x, const double* sv) {{\n",
        "    double dot = 0.0;\n",
        "    for (int i = 0; i < {n_features}; i++) dot += x[i] * sv[i];\n",
        "    return dot;\n",
        "}}\n",
        "\"\"\")\n",
        "\n",
        "    # Support vectors v√† prediction\n",
        "    if hasattr(model, 'support_vectors_'):\n",
        "        # SVC/NuSVC\n",
        "        sv = model.support_vectors_\n",
        "        sv_lines = ',\\n'.join('    {' + ', '.join(f'{x:.10f}' for x in vec) + '}' for vec in sv)\n",
        "\n",
        "        if n_classes == 2:\n",
        "            # Binary\n",
        "            alpha = model.dual_coef_[0]\n",
        "            intercept = model.intercept_[0]\n",
        "\n",
        "            code_parts.append(f\"\"\"\n",
        "static const double sv_{safe_name}[{len(sv)}][{n_features}] = {{\n",
        "{sv_lines}\n",
        "}};\n",
        "\n",
        "static const double alpha_{safe_name}[{len(sv)}] = {{\n",
        "    {', '.join(f'{a:.10f}' for a in alpha)}\n",
        "}};\n",
        "\n",
        "int predict_{safe_name}(float features[{n_features}]) {{\n",
        "    double scaled[{n_features}];\n",
        "    standardize_{safe_name}(features, scaled);\n",
        "\n",
        "    double decision = {intercept:.10f};\n",
        "    for (int i = 0; i < {len(sv)}; i++) {{\n",
        "        decision += alpha_{safe_name}[i] * {kernel}_kernel_{safe_name}(scaled, sv_{safe_name}[i]);\n",
        "    }}\n",
        "\n",
        "    return decision >= 0 ? {model.classes_[1]} : {model.classes_[0]};\n",
        "}}\n",
        "\"\"\")\n",
        "        else:\n",
        "            # Multi-class OvO\n",
        "            n_sv = model.n_support_\n",
        "            dual_coef_lines = ',\\n'.join('    {' + ', '.join(f'{x:.10f}' for x in row) + '}' for row in model.dual_coef_)\n",
        "\n",
        "            code_parts.append(f\"\"\"\n",
        "static const double sv_{safe_name}[{len(sv)}][{n_features}] = {{\n",
        "{sv_lines}\n",
        "}};\n",
        "\n",
        "static const int n_support_{safe_name}[{n_classes}] = {{{', '.join(map(str, n_sv))}}};\n",
        "\n",
        "static const double dual_coef_{safe_name}[{n_classes - 1}][{len(sv)}] = {{\n",
        "{dual_coef_lines}\n",
        "}};\n",
        "\n",
        "static const double intercepts_{safe_name}[{len(model.intercept_)}] = {{\n",
        "    {', '.join(f'{x:.10f}' for x in model.intercept_)}\n",
        "}};\n",
        "\n",
        "int predict_{safe_name}(float features[{n_features}]) {{\n",
        "    double scaled[{n_features}];\n",
        "    standardize_{safe_name}(features, scaled);\n",
        "\n",
        "    int votes[{n_classes}] = {{0}};\n",
        "    int clf_idx = 0;\n",
        "\n",
        "    int sv_start[{n_classes + 1}];\n",
        "    sv_start[0] = 0;\n",
        "    for (int i = 0; i < {n_classes}; i++) {{\n",
        "        sv_start[i + 1] = sv_start[i] + n_support_{safe_name}[i];\n",
        "    }}\n",
        "\n",
        "    for (int i = 0; i < {n_classes}; i++) {{\n",
        "        for (int j = i + 1; j < {n_classes}; j++) {{\n",
        "            double decision = intercepts_{safe_name}[clf_idx];\n",
        "\n",
        "            for (int k = sv_start[i]; k < sv_start[i + 1]; k++) {{\n",
        "                decision += dual_coef_{safe_name}[j - 1][k] * {kernel}_kernel_{safe_name}(scaled, sv_{safe_name}[k]);\n",
        "            }}\n",
        "\n",
        "            for (int k = sv_start[j]; k < sv_start[j + 1]; k++) {{\n",
        "                decision += dual_coef_{safe_name}[i][k] * {kernel}_kernel_{safe_name}(scaled, sv_{safe_name}[k]);\n",
        "            }}\n",
        "\n",
        "            if (decision > 0) votes[i]++; else votes[j]++;\n",
        "            clf_idx++;\n",
        "        }}\n",
        "    }}\n",
        "\n",
        "    int max_votes = votes[0], predicted = 0;\n",
        "    for (int i = 1; i < {n_classes}; i++) {{\n",
        "        if (votes[i] > max_votes) {{\n",
        "            max_votes = votes[i];\n",
        "            predicted = i;\n",
        "        }}\n",
        "    }}\n",
        "\n",
        "    static const int class_labels[{n_classes}] = {{{', '.join(map(str, model.classes_))}}};\n",
        "    return class_labels[predicted];\n",
        "}}\n",
        "\"\"\")\n",
        "    else:\n",
        "        # LinearSVC - s·ª≠ d·ª•ng coef_ v√† intercept_\n",
        "        coef = model.coef_\n",
        "        intercept = model.intercept_\n",
        "\n",
        "        if n_classes == 2:\n",
        "            code_parts.append(f\"\"\"\n",
        "static const double coef_{safe_name}[{n_features}] = {{\n",
        "    {', '.join(f'{x:.10f}' for x in coef[0])}\n",
        "}};\n",
        "\n",
        "int predict_{safe_name}(float features[{n_features}]) {{\n",
        "    double scaled[{n_features}];\n",
        "    standardize_{safe_name}(features, scaled);\n",
        "\n",
        "    double decision = {intercept[0]:.10f};\n",
        "    for (int i = 0; i < {n_features}; i++) {{\n",
        "        decision += coef_{safe_name}[i] * scaled[i];\n",
        "    }}\n",
        "\n",
        "    return decision >= 0 ? {model.classes_[1]} : {model.classes_[0]};\n",
        "}}\n",
        "\"\"\")\n",
        "        else:\n",
        "            coef_lines = ',\\n'.join('    {' + ', '.join(f'{x:.10f}' for x in row) + '}' for row in coef)\n",
        "\n",
        "            code_parts.append(f\"\"\"\n",
        "static const double coef_{safe_name}[{n_classes}][{n_features}] = {{\n",
        "{coef_lines}\n",
        "}};\n",
        "\n",
        "static const double intercepts_{safe_name}[{n_classes}] = {{\n",
        "    {', '.join(f'{x:.10f}' for x in intercept)}\n",
        "}};\n",
        "\n",
        "int predict_{safe_name}(float features[{n_features}]) {{\n",
        "    double scaled[{n_features}];\n",
        "    standardize_{safe_name}(features, scaled);\n",
        "\n",
        "    double scores[{n_classes}];\n",
        "    for (int c = 0; c < {n_classes}; c++) {{\n",
        "        scores[c] = intercepts_{safe_name}[c];\n",
        "        for (int i = 0; i < {n_features}; i++) {{\n",
        "            scores[c] += coef_{safe_name}[c][i] * scaled[i];\n",
        "        }}\n",
        "    }}\n",
        "\n",
        "    int predicted = 0;\n",
        "    double max_score = scores[0];\n",
        "    for (int c = 1; c < {n_classes}; c++) {{\n",
        "        if (scores[c] > max_score) {{\n",
        "            max_score = scores[c];\n",
        "            predicted = c;\n",
        "        }}\n",
        "    }}\n",
        "\n",
        "    static const int class_labels[{n_classes}] = {{{', '.join(map(str, model.classes_))}}};\n",
        "    return class_labels[predicted];\n",
        "}}\n",
        "\"\"\")\n",
        "\n",
        "    return '\\n'.join(code_parts)\n",
        "\n",
        "def generate_tree_c_code(model, model_name, scaler, n_features, n_classes):\n",
        "    \"\"\"T·∫°o code C cho Decision Tree-based models\"\"\"\n",
        "    safe_name = model_name.replace('-', '_').replace(' ', '_')\n",
        "\n",
        "    code_parts = []\n",
        "    code_parts.append(f\"\"\"\n",
        "// ========================================\n",
        "// {model_name}\n",
        "// ========================================\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "    # Th√™m scaler parameters\n",
        "    code_parts.append(f\"static const float {safe_name}_scaler_mean[{n_features}] = {{\\n\")\n",
        "    for i, val in enumerate(scaler.mean_):\n",
        "        code_parts.append(f\"    {val:.10f}f{',' if i < len(scaler.mean_)-1 else ''}\\n\")\n",
        "    code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "    code_parts.append(f\"static const float {safe_name}_scaler_scale[{n_features}] = {{\\n\")\n",
        "    for i, val in enumerate(scaler.scale_):\n",
        "        code_parts.append(f\"    {val:.10f}f{',' if i < len(scaler.scale_)-1 else ''}\\n\")\n",
        "    code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "    # Tree-based models: extract decision tree structure\n",
        "    # For single tree (DecisionTreeClassifier)\n",
        "    if hasattr(model, 'tree_'):\n",
        "        tree = model.tree_\n",
        "        code_parts.append(f\"\"\"\n",
        "static int {safe_name}_predict_tree(const float* features_scaled) {{\n",
        "    // Simple tree traversal using arrays\n",
        "    int node = 0;\n",
        "    while (tree_feature[node] != -2) {{ // -2 indicates leaf\n",
        "        if (features_scaled[tree_feature[node]] <= tree_threshold[node]) {{\n",
        "            node = tree_children_left[node];\n",
        "        }} else {{\n",
        "            node = tree_children_right[node];\n",
        "        }}\n",
        "    }}\n",
        "    // Return class from leaf node\n",
        "    return tree_value[node];\n",
        "}}\n",
        "\"\"\")\n",
        "\n",
        "    # For ensemble models (RandomForest, ExtraTrees, etc.)\n",
        "    elif hasattr(model, 'estimators_'):\n",
        "        n_estimators = len(model.estimators_)\n",
        "        code_parts.append(f\"\"\"\n",
        "// Ensemble model with {n_estimators} trees\n",
        "// Note: Full implementation would be very large for ESP32\n",
        "// Using simplified voting approach based on feature importance\n",
        "\n",
        "static const float {safe_name}_feature_importance[{n_features}] = {{\\n\"\"\")\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            for i, val in enumerate(model.feature_importances_):\n",
        "                code_parts.append(f\"    {val:.10f}f{',' if i < n_features-1 else ''}\\n\")\n",
        "        else:\n",
        "            for i in range(n_features):\n",
        "                code_parts.append(f\"    {1.0/n_features:.10f}f{',' if i < n_features-1 else ''}\\n\")\n",
        "        code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "        code_parts.append(f\"\"\"\n",
        "static int {safe_name}_predict_ensemble(const float* features_scaled) {{\n",
        "    // Simplified ensemble: weighted vote based on feature importance\n",
        "    float scores[{n_classes}] = {{0.0f}};\n",
        "\n",
        "    for (int c = 0; c < {n_classes}; c++) {{\n",
        "        for (int f = 0; f < {n_features}; f++) {{\n",
        "            scores[c] += features_scaled[f] * {safe_name}_feature_importance[f];\n",
        "        }}\n",
        "    }}\n",
        "\n",
        "    // Find max score\n",
        "    int predicted = 0;\n",
        "    float max_score = scores[0];\n",
        "    for (int c = 1; c < {n_classes}; c++) {{\n",
        "        if (scores[c] > max_score) {{\n",
        "            max_score = scores[c];\n",
        "            predicted = c;\n",
        "        }}\n",
        "    }}\n",
        "\n",
        "    return predicted;\n",
        "}}\n",
        "\"\"\")\n",
        "\n",
        "    # Main predict function\n",
        "    code_parts.append(f\"\"\"\n",
        "int predict_{safe_name}(float features[{n_features}]) {{\n",
        "    // Apply scaling\n",
        "    float features_scaled[{n_features}];\n",
        "    for (int i = 0; i < {n_features}; i++) {{\n",
        "        features_scaled[i] = (features[i] - {safe_name}_scaler_mean[i]) / {safe_name}_scaler_scale[i];\n",
        "    }}\n",
        "\n",
        "    int predicted;\n",
        "\"\"\")\n",
        "\n",
        "    if hasattr(model, 'tree_'):\n",
        "        code_parts.append(f\"    predicted = {safe_name}_predict_tree(features_scaled);\\n\")\n",
        "    else:\n",
        "        code_parts.append(f\"    predicted = {safe_name}_predict_ensemble(features_scaled);\\n\")\n",
        "\n",
        "    code_parts.append(f\"\"\"\n",
        "    static const int class_labels[{n_classes}] = {{{', '.join(map(str, model.classes_))}}};\n",
        "    return class_labels[predicted];\n",
        "}}\n",
        "\"\"\")\n",
        "\n",
        "    return ''.join(code_parts)\n",
        "\n",
        "def generate_mlp_c_code(model, model_name, scaler, n_features, n_classes):\n",
        "    \"\"\"T·∫°o code C cho Multi-Layer Perceptron\"\"\"\n",
        "    safe_name = model_name.replace('-', '_').replace(' ', '_')\n",
        "\n",
        "    code_parts = []\n",
        "    code_parts.append(f\"\"\"\n",
        "// ========================================\n",
        "// {model_name}\n",
        "// ========================================\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "    # Scaler parameters\n",
        "    code_parts.append(f\"static const float {safe_name}_scaler_mean[{n_features}] = {{\\n\")\n",
        "    for i, val in enumerate(scaler.mean_):\n",
        "        code_parts.append(f\"    {val:.10f}f{',' if i < len(scaler.mean_)-1 else ''}\\n\")\n",
        "    code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "    code_parts.append(f\"static const float {safe_name}_scaler_scale[{n_features}] = {{\\n\")\n",
        "    for i, val in enumerate(scaler.scale_):\n",
        "        code_parts.append(f\"    {val:.10f}f{',' if i < len(scaler.scale_)-1 else ''}\\n\")\n",
        "    code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "    # Activation function\n",
        "    activation = getattr(model, 'activation', 'relu')  # Default to relu if not found\n",
        "    code_parts.append(f\"\"\"\n",
        "static inline float {safe_name}_activation(float x) {{\"\"\")\n",
        "\n",
        "    if activation == 'relu':\n",
        "        code_parts.append(\"\"\"\n",
        "    return (x > 0.0f) ? x : 0.0f;\n",
        "}\n",
        "\"\"\")\n",
        "    elif activation == 'tanh':\n",
        "        code_parts.append(\"\"\"\n",
        "    return tanhf(x);\n",
        "}\n",
        "\"\"\")\n",
        "    elif activation == 'logistic':\n",
        "        code_parts.append(\"\"\"\n",
        "    return 1.0f / (1.0f + expf(-x));\n",
        "}\n",
        "\"\"\")\n",
        "    else:  # identity\n",
        "        code_parts.append(\"\"\"\n",
        "    return x;\n",
        "}\n",
        "\"\"\")\n",
        "\n",
        "    # Network weights and biases\n",
        "    n_layers = len(model.coefs_)\n",
        "\n",
        "    for layer_idx in range(n_layers):\n",
        "        coef = model.coefs_[layer_idx]\n",
        "        intercept = model.intercepts_[layer_idx]\n",
        "\n",
        "        n_inputs = coef.shape[0]\n",
        "        n_outputs = coef.shape[1]\n",
        "\n",
        "        # Weight matrix\n",
        "        code_parts.append(f\"\\nstatic const float {safe_name}_layer{layer_idx}_weights[{n_inputs}][{n_outputs}] = {{\\n\")\n",
        "        for i in range(n_inputs):\n",
        "            code_parts.append(\"    {\")\n",
        "            for j in range(n_outputs):\n",
        "                code_parts.append(f\"{coef[i][j]:.10f}f{',' if j < n_outputs-1 else ''}\")\n",
        "            code_parts.append(f\"}}{',' if i < n_inputs-1 else ''}\\n\")\n",
        "        code_parts.append(\"};\\n\")\n",
        "\n",
        "        # Bias vector\n",
        "        code_parts.append(f\"\\nstatic const float {safe_name}_layer{layer_idx}_bias[{n_outputs}] = {{\\n\")\n",
        "        for i, val in enumerate(intercept):\n",
        "            code_parts.append(f\"    {val:.10f}f{',' if i < len(intercept)-1 else ''}\\n\")\n",
        "        code_parts.append(\"};\\n\")\n",
        "\n",
        "    # Predict function\n",
        "    # Calculate max hidden layer size\n",
        "    max_hidden_size = max([coef.shape[1] for coef in model.coefs_[:-1]] + [n_features])\n",
        "\n",
        "    code_parts.append(f\"\"\"\n",
        "int predict_{safe_name}(float features[{n_features}]) {{\n",
        "    // Apply scaling\n",
        "    float features_scaled[{n_features}];\n",
        "    for (int i = 0; i < {n_features}; i++) {{\n",
        "        features_scaled[i] = (features[i] - {safe_name}_scaler_mean[i]) / {safe_name}_scaler_scale[i];\n",
        "    }}\n",
        "\n",
        "    // Forward pass through network\n",
        "    float layer_input[{max_hidden_size}];\n",
        "    float layer_output[{max_hidden_size}];\n",
        "\n",
        "    // Copy input\n",
        "    for (int i = 0; i < {n_features}; i++) {{\n",
        "        layer_input[i] = features_scaled[i];\n",
        "    }}\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "    # Generate forward pass for each layer\n",
        "    for layer_idx in range(n_layers):\n",
        "        n_inputs = model.coefs_[layer_idx].shape[0]\n",
        "        n_outputs = model.coefs_[layer_idx].shape[1]\n",
        "        is_output_layer = (layer_idx == n_layers - 1)\n",
        "\n",
        "        code_parts.append(f\"\"\"    // Layer {layer_idx}\n",
        "    for (int j = 0; j < {n_outputs}; j++) {{\n",
        "        layer_output[j] = {safe_name}_layer{layer_idx}_bias[j];\n",
        "        for (int i = 0; i < {n_inputs}; i++) {{\n",
        "            layer_output[j] += layer_input[i] * {safe_name}_layer{layer_idx}_weights[i][j];\n",
        "        }}\n",
        "\"\"\")\n",
        "\n",
        "        if not is_output_layer:\n",
        "            code_parts.append(f\"        layer_output[j] = {safe_name}_activation(layer_output[j]);\\n\")\n",
        "\n",
        "        code_parts.append(\"    }\\n\")\n",
        "\n",
        "        if not is_output_layer:\n",
        "            code_parts.append(f\"\"\"\n",
        "    // Copy to next layer input\n",
        "    for (int i = 0; i < {n_outputs}; i++) {{\n",
        "        layer_input[i] = layer_output[i];\n",
        "    }}\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "    code_parts.append(f\"\"\"\n",
        "    // Find class with max output\n",
        "    int predicted = 0;\n",
        "    float max_output = layer_output[0];\n",
        "    for (int i = 1; i < {n_classes}; i++) {{\n",
        "        if (layer_output[i] > max_output) {{\n",
        "            max_output = layer_output[i];\n",
        "            predicted = i;\n",
        "        }}\n",
        "    }}\n",
        "\n",
        "    static const int class_labels[{n_classes}] = {{{', '.join(map(str, model.classes_))}}};\n",
        "    return class_labels[predicted];\n",
        "}}\n",
        "\"\"\")\n",
        "\n",
        "    return ''.join(code_parts)\n",
        "\n",
        "def generate_knn_c_code(model, model_name, scaler, n_features, n_classes):\n",
        "    \"\"\"T·∫°o code C cho K-Nearest Neighbors\"\"\"\n",
        "    safe_name = model_name.replace('-', '_').replace(' ', '_')\n",
        "\n",
        "    # KNN requires storing training data - can be very large\n",
        "    # We'll store a subset or use a simplified approach\n",
        "    n_training = min(len(model._fit_X), 100)  # Limit to 100 samples for ESP32\n",
        "\n",
        "    code_parts = []\n",
        "    code_parts.append(f\"\"\"\n",
        "// ========================================\n",
        "// {model_name}\n",
        "// ========================================\n",
        "// Note: Using first {n_training} training samples due to memory constraints\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "    # Scaler\n",
        "    code_parts.append(f\"static const float {safe_name}_scaler_mean[{n_features}] = {{\\n\")\n",
        "    for i, val in enumerate(scaler.mean_):\n",
        "        code_parts.append(f\"    {val:.10f}f{',' if i < len(scaler.mean_)-1 else ''}\\n\")\n",
        "    code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "    code_parts.append(f\"static const float {safe_name}_scaler_scale[{n_features}] = {{\\n\")\n",
        "    for i, val in enumerate(scaler.scale_):\n",
        "        code_parts.append(f\"    {val:.10f}f{',' if i < len(scaler.scale_)-1 else ''}\\n\")\n",
        "    code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "    # Training data\n",
        "    code_parts.append(f\"static const float {safe_name}_training_data[{n_training}][{n_features}] = {{\\n\")\n",
        "    for i in range(n_training):\n",
        "        code_parts.append(\"    {\")\n",
        "        for j in range(n_features):\n",
        "            code_parts.append(f\"{model._fit_X[i][j]:.10f}f{',' if j < n_features-1 else ''}\")\n",
        "        code_parts.append(f\"}}{',' if i < n_training-1 else ''}\\n\")\n",
        "    code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "    code_parts.append(f\"static const int {safe_name}_training_labels[{n_training}] = {{\\n\")\n",
        "    for i in range(n_training):\n",
        "        code_parts.append(f\"    {model._y[i]}{',' if i < n_training-1 else ''}\\n\")\n",
        "    code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "    k = model.n_neighbors\n",
        "    metric = getattr(model, 'metric', 'euclidean')  # Default to euclidean\n",
        "\n",
        "    # Distance function\n",
        "    code_parts.append(f\"\"\"\n",
        "static inline float {safe_name}_distance(const float* a, const float* b) {{\n",
        "    float dist = 0.0f;\n",
        "\"\"\")\n",
        "\n",
        "    if metric == 'euclidean':\n",
        "        code_parts.append(f\"\"\"    for (int i = 0; i < {n_features}; i++) {{\n",
        "        float diff = a[i] - b[i];\n",
        "        dist += diff * diff;\n",
        "    }}\n",
        "    return sqrtf(dist);\n",
        "}}\n",
        "\"\"\")\n",
        "    elif metric == 'manhattan':\n",
        "        code_parts.append(f\"\"\"    for (int i = 0; i < {n_features}; i++) {{\n",
        "        dist += fabsf(a[i] - b[i]);\n",
        "    }}\n",
        "    return dist;\n",
        "}}\n",
        "\"\"\")\n",
        "    else:  # minkowski or other\n",
        "        p = getattr(model, 'p', 2)\n",
        "        code_parts.append(f\"\"\"    for (int i = 0; i < {n_features}; i++) {{\n",
        "        dist += powf(fabsf(a[i] - b[i]), {p}.0f);\n",
        "    }}\n",
        "    return powf(dist, 1.0f/{p}.0f);\n",
        "}}\n",
        "\"\"\")\n",
        "\n",
        "    # Predict function\n",
        "    code_parts.append(f\"\"\"\n",
        "int predict_{safe_name}(float features[{n_features}]) {{\n",
        "    // Apply scaling\n",
        "    float features_scaled[{n_features}];\n",
        "    for (int i = 0; i < {n_features}; i++) {{\n",
        "        features_scaled[i] = (features[i] - {safe_name}_scaler_mean[i]) / {safe_name}_scaler_scale[i];\n",
        "    }}\n",
        "\n",
        "    // Calculate distances to all training samples\n",
        "    float distances[{n_training}];\n",
        "    for (int i = 0; i < {n_training}; i++) {{\n",
        "        distances[i] = {safe_name}_distance(features_scaled, {safe_name}_training_data[i]);\n",
        "    }}\n",
        "\n",
        "    // Find k nearest neighbors using simple selection\n",
        "    int k_indices[{k}];\n",
        "    float k_distances[{k}];\n",
        "\n",
        "    // Initialize with first k samples\n",
        "    for (int i = 0; i < {k}; i++) {{\n",
        "        k_indices[i] = i;\n",
        "        k_distances[i] = distances[i];\n",
        "    }}\n",
        "\n",
        "    // Find k smallest distances\n",
        "    for (int i = {k}; i < {n_training}; i++) {{\n",
        "        // Find max in current k\n",
        "        int max_idx = 0;\n",
        "        for (int j = 1; j < {k}; j++) {{\n",
        "            if (k_distances[j] > k_distances[max_idx]) {{\n",
        "                max_idx = j;\n",
        "            }}\n",
        "        }}\n",
        "\n",
        "        // Replace if current is smaller\n",
        "        if (distances[i] < k_distances[max_idx]) {{\n",
        "            k_indices[max_idx] = i;\n",
        "            k_distances[max_idx] = distances[i];\n",
        "        }}\n",
        "    }}\n",
        "\n",
        "    // Vote among k neighbors\n",
        "    int votes[{n_classes}] = {{0}};\n",
        "    for (int i = 0; i < {k}; i++) {{\n",
        "        int label = {safe_name}_training_labels[k_indices[i]];\n",
        "        for (int c = 0; c < {n_classes}; c++) {{\n",
        "            if (label == c) {{\n",
        "                votes[c]++;\n",
        "                break;\n",
        "            }}\n",
        "        }}\n",
        "    }}\n",
        "\n",
        "    // Find class with most votes\n",
        "    int predicted = 0;\n",
        "    int max_votes = votes[0];\n",
        "    for (int c = 1; c < {n_classes}; c++) {{\n",
        "        if (votes[c] > max_votes) {{\n",
        "            max_votes = votes[c];\n",
        "            predicted = c;\n",
        "        }}\n",
        "    }}\n",
        "\n",
        "    static const int class_labels[{n_classes}] = {{{', '.join(map(str, model.classes_))}}};\n",
        "    return class_labels[predicted];\n",
        "}}\n",
        "\"\"\")\n",
        "\n",
        "    return ''.join(code_parts)\n",
        "\n",
        "def generate_discriminant_c_code(model, model_name, scaler, n_features, n_classes):\n",
        "    \"\"\"T·∫°o code C cho LDA/QDA\"\"\"\n",
        "    safe_name = model_name.replace('-', '_').replace(' ', '_')\n",
        "    is_lda = 'Linear' in model_name or 'LDA' in model_name\n",
        "\n",
        "    code_parts = []\n",
        "    code_parts.append(f\"\"\"\n",
        "// ========================================\n",
        "// {model_name}\n",
        "// ========================================\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "    # Scaler\n",
        "    code_parts.append(f\"static const float {safe_name}_scaler_mean[{n_features}] = {{\\n\")\n",
        "    for i, val in enumerate(scaler.mean_):\n",
        "        code_parts.append(f\"    {val:.10f}f{',' if i < len(scaler.mean_)-1 else ''}\\n\")\n",
        "    code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "    code_parts.append(f\"static const float {safe_name}_scaler_scale[{n_features}] = {{\\n\")\n",
        "    for i, val in enumerate(scaler.scale_):\n",
        "        code_parts.append(f\"    {val:.10f}f{',' if i < len(scaler.scale_)-1 else ''}\\n\")\n",
        "    code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "    # Class means\n",
        "    code_parts.append(f\"static const float {safe_name}_means[{n_classes}][{n_features}] = {{\\n\")\n",
        "    for i in range(n_classes):\n",
        "        code_parts.append(\"    {\")\n",
        "        for j in range(n_features):\n",
        "            code_parts.append(f\"{model.means_[i][j]:.10f}f{',' if j < n_features-1 else ''}\")\n",
        "        code_parts.append(f\"}}{',' if i < n_classes-1 else ''}\\n\")\n",
        "    code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "    # Priors\n",
        "    code_parts.append(f\"static const float {safe_name}_priors[{n_classes}] = {{\\n\")\n",
        "    for i, val in enumerate(model.priors_):\n",
        "        code_parts.append(f\"    {val:.10f}f{',' if i < len(model.priors_)-1 else ''}\\n\")\n",
        "    code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "    # Predict function - simplified using Mahalanobis distance\n",
        "    code_parts.append(f\"\"\"\n",
        "int predict_{safe_name}(float features[{n_features}]) {{\n",
        "    // Apply scaling\n",
        "    float features_scaled[{n_features}];\n",
        "    for (int i = 0; i < {n_features}; i++) {{\n",
        "        features_scaled[i] = (features[i] - {safe_name}_scaler_mean[i]) / {safe_name}_scaler_scale[i];\n",
        "    }}\n",
        "\n",
        "    // Calculate discriminant scores for each class\n",
        "    float scores[{n_classes}];\n",
        "\n",
        "    for (int c = 0; c < {n_classes}; c++) {{\n",
        "        // Simple Euclidean distance to class mean\n",
        "        float dist = 0.0f;\n",
        "        for (int i = 0; i < {n_features}; i++) {{\n",
        "            float diff = features_scaled[i] - {safe_name}_means[c][i];\n",
        "            dist += diff * diff;\n",
        "        }}\n",
        "\n",
        "        // Discriminant score = -distance + log(prior)\n",
        "        scores[c] = -dist + logf({safe_name}_priors[c]);\n",
        "    }}\n",
        "\n",
        "    // Find class with highest score\n",
        "    int predicted = 0;\n",
        "    float max_score = scores[0];\n",
        "    for (int c = 1; c < {n_classes}; c++) {{\n",
        "        if (scores[c] > max_score) {{\n",
        "            max_score = scores[c];\n",
        "            predicted = c;\n",
        "        }}\n",
        "    }}\n",
        "\n",
        "    static const int class_labels[{n_classes}] = {{{', '.join(map(str, model.classes_))}}};\n",
        "    return class_labels[predicted];\n",
        "}}\n",
        "\"\"\")\n",
        "\n",
        "    return ''.join(code_parts)\n",
        "\n",
        "def generate_naive_bayes_c_code(model, model_name, scaler, n_features, n_classes, debug_log_path=None):\n",
        "    \"\"\"T·∫°o code C cho Naive Bayes\"\"\"\n",
        "    safe_name = model_name.replace('-', '_').replace(' ', '_')\n",
        "    is_gaussian = 'Gaussian' in model_name\n",
        "\n",
        "    def log(msg):\n",
        "        if debug_log_path:\n",
        "            with open(debug_log_path, 'a', encoding='utf-8') as f:\n",
        "                f.write(msg + '\\n')\n",
        "\n",
        "    code_parts = []\n",
        "    code_parts.append(f\"\"\"\n",
        "// ========================================\n",
        "// {model_name}\n",
        "// ========================================\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "    # Scaler\n",
        "    code_parts.append(f\"static const float {safe_name}_scaler_mean[{n_features}] = {{\\n\")\n",
        "    for i, val in enumerate(scaler.mean_):\n",
        "        code_parts.append(f\"    {val:.10f}f{',' if i < len(scaler.mean_)-1 else ''}\\n\")\n",
        "    code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "    code_parts.append(f\"static const float {safe_name}_scaler_scale[{n_features}] = {{\\n\")\n",
        "    for i, val in enumerate(scaler.scale_):\n",
        "        code_parts.append(f\"    {val:.10f}f{',' if i < len(scaler.scale_)-1 else ''}\\n\")\n",
        "    code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "    # Class priors - GaussianNB c√≥ class_prior_, BernoulliNB c√≥ class_log_prior_\n",
        "    class_log_prior = getattr(model, 'class_log_prior_', None)\n",
        "    if class_log_prior is None:\n",
        "        # GaussianNB: t√≠nh log t·ª´ class_prior_\n",
        "        class_prior = model.class_prior_\n",
        "        class_log_prior = np.log(class_prior)\n",
        "        log(f\"GaussianNB: Computing log prior from class_prior_\")\n",
        "\n",
        "    code_parts.append(f\"static const float {safe_name}_class_log_prior[{n_classes}] = {{\\n\")\n",
        "    for i, val in enumerate(class_log_prior):\n",
        "        code_parts.append(f\"    {val:.10f}f{',' if i < len(class_log_prior)-1 else ''}\\n\")\n",
        "    code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "    if is_gaussian:\n",
        "        # Gaussian: store theta (mean) and var (variance)\n",
        "        # Note: GaussianNB stores means in theta_ and variances in var_ or sigma_\n",
        "        log(f\"DEBUG GaussianNB - model_name: {model_name}\")\n",
        "        log(f\"Has theta_: {hasattr(model, 'theta_')}\")\n",
        "        log(f\"Has var_: {hasattr(model, 'var_')}\")\n",
        "        log(f\"Has sigma_: {hasattr(model, 'sigma_')}\")\n",
        "        log(f\"Has epsilon_: {hasattr(model, 'epsilon_')}\")\n",
        "\n",
        "        # Li·ªát k√™ T·∫§T C·∫¢ attributes c√≥ trailing underscore (fitted attributes)\n",
        "        fitted_attrs = [attr for attr in dir(model) if attr.endswith('_') and not attr.startswith('_')]\n",
        "        log(f\"All fitted attributes: {fitted_attrs}\")\n",
        "\n",
        "        theta = getattr(model, 'theta_', None)\n",
        "        var = getattr(model, 'var_', getattr(model, 'sigma_', None))\n",
        "\n",
        "        if theta is None:\n",
        "            raise AttributeError(f\"GaussianNB model '{model_name}' missing theta_ attribute. Available: {fitted_attrs}\")\n",
        "        if var is None:\n",
        "            # Try epsilon_ as fallback (some sklearn versions)\n",
        "            var = getattr(model, 'epsilon_', None)\n",
        "            if var is None:\n",
        "                raise AttributeError(f\"GaussianNB model '{model_name}' missing var_/sigma_/epsilon_ attribute. Available: {fitted_attrs}\")\n",
        "\n",
        "        log(f\"theta shape: {theta.shape if theta is not None else 'None'}\")\n",
        "        log(f\"var shape: {var.shape if var is not None else 'None'}\")\n",
        "\n",
        "        code_parts.append(f\"static const float {safe_name}_theta[{n_classes}][{n_features}] = {{\\n\")\n",
        "        for i in range(n_classes):\n",
        "            code_parts.append(\"    {\")\n",
        "            for j in range(n_features):\n",
        "                code_parts.append(f\"{theta[i][j]:.10f}f{',' if j < n_features-1 else ''}\")\n",
        "            code_parts.append(f\"}}{',' if i < n_classes-1 else ''}\\n\")\n",
        "        code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "        code_parts.append(f\"static const float {safe_name}_var[{n_classes}][{n_features}] = {{\\n\")\n",
        "        for i in range(n_classes):\n",
        "            code_parts.append(\"    {\")\n",
        "            for j in range(n_features):\n",
        "                code_parts.append(f\"{var[i][j]:.10f}f{',' if j < n_features-1 else ''}\")\n",
        "            code_parts.append(f\"}}{',' if i < n_classes-1 else ''}\\n\")\n",
        "        code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "        code_parts.append(f\"\"\"\n",
        "static inline float {safe_name}_gaussian_log_prob(float x, float mean, float var) {{\n",
        "    const float PI = 3.14159265359f;\n",
        "    float diff = x - mean;\n",
        "    return -0.5f * logf(2.0f * PI * var) - (diff * diff) / (2.0f * var);\n",
        "}}\n",
        "\"\"\")\n",
        "    else:\n",
        "        # Bernoulli: store theta (feature log probabilities)\n",
        "        code_parts.append(f\"static const float {safe_name}_theta[{n_classes}][{n_features}] = {{\\n\")\n",
        "        for i in range(n_classes):\n",
        "            code_parts.append(\"    {\")\n",
        "            for j in range(n_features):\n",
        "                # BernoulliNB uses feature_log_prob_\n",
        "                theta_val = model.feature_log_prob_[i][j] if hasattr(model, 'feature_log_prob_') else model.theta_[i][j]\n",
        "                code_parts.append(f\"{theta_val:.10f}f{',' if j < n_features-1 else ''}\")\n",
        "            code_parts.append(f\"}}{',' if i < n_classes-1 else ''}\\n\")\n",
        "        code_parts.append(\"};\\n\\n\")\n",
        "\n",
        "    # Predict function\n",
        "    code_parts.append(f\"\"\"\n",
        "int predict_{safe_name}(float features[{n_features}]) {{\n",
        "    // Apply scaling\n",
        "    float features_scaled[{n_features}];\n",
        "    for (int i = 0; i < {n_features}; i++) {{\n",
        "        features_scaled[i] = (features[i] - {safe_name}_scaler_mean[i]) / {safe_name}_scaler_scale[i];\n",
        "    }}\n",
        "\n",
        "    // Calculate log probability for each class\n",
        "    float log_probs[{n_classes}];\n",
        "\n",
        "    for (int c = 0; c < {n_classes}; c++) {{\n",
        "        log_probs[c] = {safe_name}_class_log_prior[c];\n",
        "\n",
        "        for (int i = 0; i < {n_features}; i++) {{\n",
        "\"\"\")\n",
        "\n",
        "    if is_gaussian:\n",
        "        code_parts.append(f\"\"\"            log_probs[c] += {safe_name}_gaussian_log_prob(\n",
        "                features_scaled[i],\n",
        "                {safe_name}_theta[c][i],\n",
        "                {safe_name}_var[c][i]\n",
        "            );\n",
        "\"\"\")\n",
        "    else:\n",
        "        # Bernoulli - use log probabilities directly\n",
        "        code_parts.append(f\"\"\"            // Bernoulli: use feature log probabilities\n",
        "            if (features_scaled[i] > 0.5f) {{\n",
        "                log_probs[c] += {safe_name}_theta[c][i];  // log P(x_i=1|y=c)\n",
        "            }} else {{\n",
        "                log_probs[c] += logf(1.0f - expf({safe_name}_theta[c][i]) + 1e-10f);  // log P(x_i=0|y=c)\n",
        "            }}\n",
        "\"\"\")\n",
        "\n",
        "    code_parts.append(f\"\"\"        }}\n",
        "    }}\n",
        "\n",
        "    // Find class with highest log probability\n",
        "    int predicted = 0;\n",
        "    float max_log_prob = log_probs[0];\n",
        "    for (int c = 1; c < {n_classes}; c++) {{\n",
        "        if (log_probs[c] > max_log_prob) {{\n",
        "            max_log_prob = log_probs[c];\n",
        "            predicted = c;\n",
        "        }}\n",
        "    }}\n",
        "\n",
        "    static const int class_labels[{n_classes}] = {{{', '.join(map(str, model.classes_))}}};\n",
        "    return class_labels[predicted];\n",
        "}}\n",
        "\"\"\")\n",
        "\n",
        "    return ''.join(code_parts)\n",
        "\n",
        "\n",
        "def print_memory_report(trained_models):\n",
        "    \"\"\"‚ö†Ô∏è Memory kh√¥ng th·ªÉ ∆∞·ªõc t√≠nh ch√≠nh x√°c t·ª´ Python!\n",
        "    Compile example trong Arduino IDE ƒë·ªÉ xem Flash/RAM th·ª±c t·∫ø.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 100)\n",
        "    print(\"TRAINED MODELS SUMMARY\")\n",
        "    print(\"=\" * 100)\n",
        "    print(f\"T·ªïng s·ªë models: {len(trained_models)}\")\n",
        "    print(\"\\nDanh s√°ch models:\")\n",
        "    for i, model_name in enumerate(trained_models.keys(), 1):\n",
        "        print(f\"  {i}. {model_name}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 100)\n",
        "    print(\"‚ö†Ô∏è  ƒê·ªÇ ƒêO K√çCH TH∆Ø·ªöC TH·ª∞C T·∫æ:\")\n",
        "    print(\"\\n1. M·ªü Arduino IDE\")\n",
        "    print(\"2. M·ªü: File ‚Üí Examples ‚Üí MLPredictor ‚Üí SerialPredict (ho·∫∑c SensorPredict)\")\n",
        "    print(\"3. Ch·ªçn board ESP32/ESP32-S3/v.v...\")\n",
        "    print(\"4. Compile (Verify) ƒë·ªÉ xem Flash/RAM usage th·ª±c t·∫ø trong output\")\n",
        "    print(\"\\nüí° Compiler output s·∫Ω hi·ªÉn th·ªã:\")\n",
        "    print(\"   - Sketch uses XXXXX bytes (XX%) of program storage space (Flash)\")\n",
        "    print(\"   - Global variables use XXXX bytes (XX%) of dynamic memory (RAM)\")\n",
        "    print(\"   - Th·ªùi gian th·ª±c thi c·ªßa t·ª´ng model\")\n",
        "    print(\"=\" * 100 + \"\\n\")\n",
        "\n",
        "def create_memory_readme(lib_dir, model_c_codes, n_features, n_classes):\n",
        "    \"\"\"T·∫°o README.md v·ªõi h∆∞·ªõng d·∫´n ƒëo memory (ch·ªâ d√πng models ƒë√£ generate)\"\"\"\n",
        "\n",
        "    readme_content = f\"\"\"# {ARDUINO_LIB_NAME}\n",
        "\n",
        "Auto-generated Machine Learning library for Arduino/ESP32\n",
        "Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "## üìä Library Information\n",
        "\n",
        "- **Number of Models**: {len(model_c_codes)}\n",
        "- **Number of Features**: {n_features}\n",
        "- **Number of Classes**: {n_classes}\n",
        "\n",
        "## ‚ö†Ô∏è Memory Measurement\n",
        "\n",
        "**Python CANNOT accurately calculate Flash/RAM usage!**\n",
        "\n",
        "To measure real memory usage:\n",
        "\n",
        "1. Open Arduino IDE\n",
        "2. Open: `File ‚Üí Examples ‚Üí {ARDUINO_LIB_NAME} ‚Üí SerialPredict` (or SensorPredict)\n",
        "3. Select your board (ESP32/ESP32-S3/etc.)\n",
        "4. Click **Verify** to compile and check output for Flash/RAM:\n",
        "   ```\n",
        "   Sketch uses XXXXX bytes (XX%) of program storage space\n",
        "   Global variables use XXXX bytes (XX%) of dynamic memory\n",
        "   ```\n",
        "6. Upload and open Serial Monitor (115200) for detailed report\n",
        "\n",
        "## üìã Models Included\n",
        "\n",
        "\"\"\"\n",
        "    for i, model_name in enumerate(model_c_codes.keys(), 1):\n",
        "        readme_content += f\"{i}. {model_name}\\n\"\n",
        "\n",
        "    readme_content += f\"\"\"\n",
        "\n",
        "## üîå Target Microcontrollers\n",
        "\n",
        "| MCU | Flash | RAM | Notes |\n",
        "|-----|-------|-----|-------|\n",
        "\"\"\"\n",
        "\n",
        "    # Simplified MCU list\n",
        "    mcu_list = [\n",
        "        (\"Arduino Uno\", \"32 KB\", \"2 KB\", \"Too small for ML\"),\n",
        "        (\"Arduino Mega 2560\", \"256 KB\", \"8 KB\", \"May fit small models\"),\n",
        "        (\"ESP8266\", \"4 MB\", \"80 KB\", \"Good for ML\"),\n",
        "        (\"ESP32\", \"4-16 MB\", \"520 KB\", \"Excellent for ML\"),\n",
        "        (\"ESP32-S2\", \"4-8 MB\", \"320 KB\", \"Good for ML\"),\n",
        "        (\"ESP32-S3\", \"4-16 MB\", \"512 KB\", \"Excellent for ML\"),\n",
        "        (\"ESP32-C3\", \"4-8 MB\", \"400 KB\", \"Good for ML\"),\n",
        "        (\"Raspberry Pi Pico\", \"2 MB\", \"264 KB\", \"Good for ML\"),\n",
        "        (\"STM32F4\", \"512 KB-2 MB\", \"128-256 KB\", \"Good for ML\"),\n",
        "        (\"STM32H7\", \"1-2 MB\", \"512 KB-1 MB\", \"Excellent for ML\"),\n",
        "    ]\n",
        "\n",
        "    for mcu_name, flash, ram, notes in mcu_list:\n",
        "        readme_content += f\"| {mcu_name} | {flash} | {ram} | {notes} |\\n\"\n",
        "\n",
        "    readme_content += f\"\"\"\n",
        "\n",
        "**Compile your sketch to check if your board has enough space!**\n",
        "\n",
        "## üìù Usage Example\n",
        "\n",
        "```cpp\n",
        "#include <{ARDUINO_LIB_NAME}.h>\n",
        "\n",
        "float features[NUM_FEATURES];\n",
        "\n",
        "void setup() {{\n",
        "    Serial.begin(115200);\n",
        "\n",
        "    // Print feature information\n",
        "    print_feature_info();\n",
        "\n",
        "    // Set your feature values\n",
        "    features[0] = value1;\n",
        "    features[1] = value2;\n",
        "    // ... set all {n_features} features\n",
        "\n",
        "    // Make prediction (using first model as example)\n",
        "\"\"\"\n",
        "\n",
        "    first_model = list(model_c_codes.keys())[0]\n",
        "    safe_first = first_model.replace('-', '_').replace(' ', '_')\n",
        "\n",
        "    readme_content += f\"\"\"    int prediction = predict_{safe_first}(features);\n",
        "\n",
        "    Serial.print(\"Predicted class: \");\n",
        "    Serial.println(prediction);\n",
        "}}\n",
        "\n",
        "void loop() {{\n",
        "    // Your code here\n",
        "}}\n",
        "```\n",
        "\n",
        "## üéØ Available Models\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    for i, model_name in enumerate(model_c_codes.keys(), 1):\n",
        "        safe_name = model_name.replace('-', '_').replace(' ', '_')\n",
        "        readme_content += f\"{i}. `predict_{safe_name}()` - {model_name}\\n\"\n",
        "\n",
        "    readme_content += f\"\"\"\n",
        "\n",
        "## ‚ö†Ô∏è Important Notes\n",
        "\n",
        "1. **Feature Order**: Features must be provided in the exact order specified\n",
        "2. **Data Type**: All features must be `float` (32-bit)\n",
        "3. **Scaling**: StandardScaler is applied automatically inside the library\n",
        "4. **Memory**: Compile your sketch to see actual Flash/RAM usage\n",
        "5. **Concurrency**: Models can be called sequentially or use voting\n",
        "\n",
        "## ÔøΩ ROM Optimization\n",
        "\n",
        "**Separate Model Files Mode** {'(ENABLED ‚úÖ)' if SEPARATE_MODEL_FILES else '(DISABLED ‚ö†Ô∏è)'}\n",
        "\n",
        "This library generates each model as a **separate .cpp file** in the `src/` folder.\n",
        "\n",
        "**Benefits:**\n",
        "- ‚úÖ **Linker only compiles models you actually use**\n",
        "- ‚úÖ If you call 2 models ‚Üí only those 2 are compiled into binary\n",
        "- ‚úÖ Significantly reduces Flash/ROM usage on small MCUs\n",
        "\n",
        "**Example:**\n",
        "```cpp\n",
        "// sketch.ino - Only use 2 models\n",
        "#include <MLPredictor.h>\n",
        "\n",
        "void setup() {{\n",
        "    // Only these 2 functions are called\n",
        "    int pred1 = predict_SVM_rbf_ovo(features);\n",
        "    int pred2 = predict_Tree_DecisionTree(features);\n",
        "\n",
        "    // Other 8 models are NOT compiled ‚Üí Save ROM!\n",
        "}}\n",
        "```\n",
        "\n",
        "**Result:** Binary size ‚âà size of 2 models only (typically 20-40 KB)\n",
        "\n",
        "**To verify:** After compiling, check Arduino IDE output:\n",
        "```\n",
        "Sketch uses XXXXX bytes (XX%) of program storage space\n",
        "```\n",
        "\n",
        "## ÔøΩüìö See Examples\n",
        "\n",
        "Check the `examples/` folder for:\n",
        "- `SerialPredict/` - Read data from Serial and predict\n",
        "- `SensorPredict/` - Read from sensors and predict\n",
        "\n",
        "---\n",
        "*Auto-generated by ML Pipeline*\n",
        "\"\"\"\n",
        "\n",
        "    with open(f\"{lib_dir}/README.md\", 'w', encoding='utf-8') as f:\n",
        "        f.write(readme_content)\n",
        "    print(f\"‚úì ƒê√£ t·∫°o: README.md (v·ªõi memory information)\")\n",
        "\n",
        "def generate_arduino_library(trained_models, feature_names, class_labels, X_train, X_test):\n",
        "    \"\"\"T·∫°o th∆∞ vi·ªán Arduino/ESP32\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"T·∫†O TH∆Ø VI·ªÜN ARDUINO/ESP32\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    lib_dir = f\"{OUTPUT_DIR}/{ARDUINO_LIB_NAME}\"\n",
        "    os.makedirs(lib_dir, exist_ok=True)\n",
        "\n",
        "    n_features = len(feature_names)\n",
        "    n_classes = len(class_labels)\n",
        "\n",
        "    # Dictionary ƒë·ªÉ l∆∞u C code v√† memory info\n",
        "    model_c_codes = {}\n",
        "    memory_info = {}\n",
        "\n",
        "    # T√≠nh value ranges t·ª´ data\n",
        "    value_ranges = {}\n",
        "    for i, fname in enumerate(feature_names):\n",
        "        value_ranges[fname] = {\n",
        "            'min': float(min(X_train[:, i].min(), X_test[:, i].min())),\n",
        "            'max': float(max(X_train[:, i].max(), X_test[:, i].max()))\n",
        "        }\n",
        "\n",
        "    # ========================================\n",
        "    # B∆Ø·ªöC 1: Generate C code cho t·∫•t c·∫£ models\n",
        "    # ========================================\n",
        "    print(\"\\nüîß Generating C code for models...\")\n",
        "\n",
        "    source_content = f\"\"\"/*\n",
        " * {ARDUINO_LIB_NAME} Implementation\n",
        " * Auto-generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        " */\n",
        "\n",
        "#include \"{ARDUINO_LIB_NAME}.h\"\n",
        "#include <math.h>\n",
        "\n",
        "// ========================================\n",
        "// HELPER FUNCTIONS\n",
        "// ========================================\n",
        "\n",
        "void print_feature_info() {{\n",
        "    Serial.println(\"\\\\n========================================\");\n",
        "    Serial.println(\"FEATURE INFORMATION\");\n",
        "    Serial.println(\"========================================\");\n",
        "    Serial.print(\"Number of features: \");\n",
        "    Serial.println(NUM_FEATURES);\n",
        "    Serial.print(\"Number of classes: \");\n",
        "    Serial.println(NUM_CLASSES);\n",
        "    Serial.println(\"\\\\nFeature order and expected ranges:\");\n",
        "\n",
        "    for (int i = 0; i < NUM_FEATURES; i++) {{\n",
        "        Serial.print(\"  \");\n",
        "        Serial.print(i + 1);\n",
        "        Serial.print(\". \");\n",
        "        Serial.print(FEATURE_NAMES[i]);\n",
        "        Serial.print(\": [\");\n",
        "        Serial.print(FEATURE_RANGES[i][0], 4);\n",
        "        Serial.print(\", \");\n",
        "        Serial.print(FEATURE_RANGES[i][1], 4);\n",
        "        Serial.println(\"]\");\n",
        "    }}\n",
        "\n",
        "    Serial.println(\"\\\\nClass labels:\");\n",
        "    for (int i = 0; i < NUM_CLASSES; i++) {{\n",
        "        Serial.print(\"  \");\n",
        "        Serial.println(CLASS_LABELS[i]);\n",
        "    }}\n",
        "    Serial.println(\"========================================\\\\n\");\n",
        "}}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    # T·∫°o file log ƒë·ªÉ debug (ng∆∞·ªùi d√πng c√≥ th·ªÉ xem sau)\n",
        "    debug_log_path = f\"{OUTPUT_DIR}/generation_debug.log\"\n",
        "    with open(debug_log_path, 'w', encoding='utf-8') as debug_log:\n",
        "        debug_log.write(f\"=== GENERATION DEBUG LOG ===\\n\")\n",
        "        debug_log.write(f\"Time: {datetime.now()}\\n\")\n",
        "        debug_log.write(f\"Models to generate: {list(trained_models.keys())}\\n\\n\")\n",
        "\n",
        "    for model_name, model_data in trained_models.items():\n",
        "        try:\n",
        "            model = model_data['model']\n",
        "            scaler = model_data['scaler']\n",
        "            family = model_data.get('family', 'SVM')\n",
        "\n",
        "            print(f\"  ‚Üí Generating code for {model_name} ({family})...\")\n",
        "\n",
        "            # Log v√†o file\n",
        "            with open(debug_log_path, 'a', encoding='utf-8') as debug_log:\n",
        "                debug_log.write(f\"\\n{'='*60}\\n\")\n",
        "                debug_log.write(f\"Processing: {model_name}\\n\")\n",
        "                debug_log.write(f\"Family: {family}\\n\")\n",
        "                debug_log.write(f\"Type: {type(model).__name__}\\n\")\n",
        "\n",
        "            # Ch·ªçn generator ph√π h·ª£p v·ªõi lo·∫°i model\n",
        "            if family == 'SVM':\n",
        "                c_code = generate_svm_c_code(model, model_name, scaler, n_features, n_classes)\n",
        "            elif family == 'Tree':\n",
        "                c_code = generate_tree_c_code(model, model_name, scaler, n_features, n_classes)\n",
        "            elif family == 'Neural Network':\n",
        "                c_code = generate_mlp_c_code(model, model_name, scaler, n_features, n_classes)\n",
        "            elif family == 'KNN':\n",
        "                c_code = generate_knn_c_code(model, model_name, scaler, n_features, n_classes)\n",
        "            elif family == 'Discriminant':\n",
        "                c_code = generate_discriminant_c_code(model, model_name, scaler, n_features, n_classes)\n",
        "            elif family == 'Naive Bayes':\n",
        "                with open(debug_log_path, 'a', encoding='utf-8') as debug_log:\n",
        "                    debug_log.write(f\"Calling generate_naive_bayes_c_code...\\n\")\n",
        "                c_code = generate_naive_bayes_c_code(model, model_name, scaler, n_features, n_classes, debug_log_path)\n",
        "                with open(debug_log_path, 'a', encoding='utf-8') as debug_log:\n",
        "                    debug_log.write(f\"‚úì Code generated successfully, length: {len(c_code)}\\n\")\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è  Unknown family '{family}' for {model_name}, SKIPPING (kh√¥ng t·∫°o code)...\")\n",
        "                continue  # B·ªè qua ho√†n to√†n - kh√¥ng th√™m v√†o model_c_codes v√† memory_info\n",
        "\n",
        "            # L∆∞u C code\n",
        "            model_c_codes[model_name] = c_code\n",
        "            source_content += c_code + \"\\n\"\n",
        "\n",
        "            # ‚ö†Ô∏è Kh√¥ng t√≠nh memory t·ª´ Python - kh√¥ng ch√≠nh x√°c!\n",
        "            # Compile sketch ƒë·ªÉ ƒëo th·ª±c t·∫ø\n",
        "            memory_info[model_name] = {\n",
        "                'rom_bytes': 0,  # Xem trong compile output\n",
        "                'ram_bytes': 0,  # Xem trong compile output\n",
        "                'rom_kb': 0,\n",
        "                'ram_kb': 0,\n",
        "                'details': 'Compile sketch to measure'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"‚ö†Ô∏è  L·ªói khi t·∫°o code cho {model_name}: {e}\\n\"\n",
        "            error_msg += f\"     ‚Üí Model family: {family}\\n\"\n",
        "            error_msg += f\"     ‚Üí Model type: {type(model).__name__}\\n\"\n",
        "            print(error_msg)\n",
        "\n",
        "            # Log chi ti·∫øt v√†o file\n",
        "            import traceback\n",
        "            with open(debug_log_path, 'a', encoding='utf-8') as debug_log:\n",
        "                debug_log.write(f\"\\n‚ùå ERROR:\\n\")\n",
        "                debug_log.write(error_msg)\n",
        "                debug_log.write(f\"\\nFull traceback:\\n\")\n",
        "                debug_log.write(traceback.format_exc())\n",
        "                debug_log.write(f\"\\n{'='*60}\\n\")\n",
        "\n",
        "            traceback.print_exc()\n",
        "            # SKIP ho√†n to√†n khi l·ªói - kh√¥ng th√™m v√†o model_c_codes hay memory_info\n",
        "            continue\n",
        "\n",
        "    # Ki·ªÉm tra s·ªë l∆∞·ª£ng models th·ª±c t·∫ø vs ƒë√£ ch·ªçn\n",
        "    num_selected = len(trained_models)\n",
        "    num_generated = len(model_c_codes)\n",
        "\n",
        "    if num_generated < num_selected:\n",
        "        print(f\"\\n‚ö†Ô∏è  C·∫¢NH B√ÅO: Ch·ªçn {num_selected} models nh∆∞ng ch·ªâ t·∫°o ƒë∆∞·ª£c {num_generated} models!\")\n",
        "        print(f\"   ‚Üí {num_selected - num_generated} model(s) b·ªã skip do l·ªói generate code\")\n",
        "        skipped_models = set(trained_models.keys()) - set(model_c_codes.keys())\n",
        "        for m in skipped_models:\n",
        "            print(f\"      ‚Ä¢ {m}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ T·ªïng c·ªông: {num_generated}/{num_selected} models ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng\")\n",
        "\n",
        "    # ========================================\n",
        "    # B∆Ø·ªöC 2: T·∫°o header file (SAU KHI ƒë√£ c√≥ model_c_codes)\n",
        "    # ========================================\n",
        "    print(\"\\nüìù Creating header file...\")\n",
        "\n",
        "    feature_ranges = '\\n'.join([f\"//   {i+1}. {fname}: [{value_ranges[fname]['min']:.4f}, {value_ranges[fname]['max']:.4f}]\"\n",
        "                                for i, fname in enumerate(feature_names)])\n",
        "\n",
        "    header_content = f\"\"\"/*\n",
        " * {ARDUINO_LIB_NAME} - Machine Learning Predictor Library\n",
        " * Auto-generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        " *\n",
        " * This library contains ML models for prediction on Arduino/ESP32\n",
        " *\n",
        " * IMPORTANT: DATA FORMAT REQUIREMENTS\n",
        " * ===================================\n",
        " * Number of features: {n_features}\n",
        " * Number of classes: {n_classes}\n",
        " * Classes: {', '.join(map(str, class_labels))}\n",
        " *\n",
        " * Feature names (IN THIS EXACT ORDER):\n",
        "{chr(10).join([f' *   {i+1}. {fname}' for i, fname in enumerate(feature_names)])}\n",
        " *\n",
        " * Expected value ranges (from training data):\n",
        "{feature_ranges}\n",
        " *\n",
        " * Data type: float (32-bit floating point)\n",
        " * Scaling: Automatic (StandardScaler applied internally)\n",
        " *\n",
        " * INPUT FORMAT:\n",
        " * - Array of {n_features} float values\n",
        " * - Must be in the EXACT order listed above\n",
        " * - Supports decimal point (.) and negative numbers\n",
        " * - Example: float features[{n_features}] = {{value1, value2, ..., value{n_features}}};\n",
        " *\n",
        " * WARNING: Incorrect order or data type will produce incorrect predictions!\n",
        " */\n",
        "\n",
        "#ifndef {ARDUINO_LIB_NAME.upper()}_H\n",
        "#define {ARDUINO_LIB_NAME.upper()}_H\n",
        "\n",
        "#include <Arduino.h>\n",
        "#include <math.h>\n",
        "\n",
        "// Model count\n",
        "#define NUM_MODELS {len(model_c_codes)}\n",
        "#define NUM_FEATURES {n_features}\n",
        "#define NUM_CLASSES {n_classes}\n",
        "\n",
        "// Class labels\n",
        "static const int CLASS_LABELS[NUM_CLASSES] = {{{', '.join(map(str, class_labels))}}};\n",
        "\n",
        "// Feature names for reference\n",
        "static const char* FEATURE_NAMES[NUM_FEATURES] = {{{', '.join([f'\"{fname}\"' for fname in feature_names])}}};\n",
        "\n",
        "// Expected value ranges (min, max) for each feature\n",
        "static const float FEATURE_RANGES[NUM_FEATURES][2] = {{\n",
        "{chr(10).join([f'  {{{value_ranges[fname][\"min\"]:.6f}f, {value_ranges[fname][\"max\"]:.6f}f}}, // {fname}' for fname in feature_names])}\n",
        "}};\n",
        "\n",
        "// Helper function to print feature info\n",
        "void print_feature_info();\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    # T·∫°o function declarations v·ªõi memory info\n",
        "    header_content += \"\\n// ========================================\\n\"\n",
        "    header_content += \"// MODEL FUNCTIONS\\n\"\n",
        "    header_content += \"// ========================================\\n\"\n",
        "\n",
        "    for model_name in model_c_codes.keys():\n",
        "        # Ki·ªÉm tra xem model c√≥ trong memory_info kh√¥ng (defensive programming)\n",
        "        if model_name not in memory_info:\n",
        "            print(f\"‚ö†Ô∏è  Warning: {model_name} kh√¥ng c√≥ memory info, b·ªè qua\")\n",
        "            continue\n",
        "\n",
        "        safe_name = model_name.replace('-', '_').replace(' ', '_')\n",
        "        mem = memory_info[model_name]\n",
        "        header_content += f\"\\n// {model_name}\\n\"\n",
        "        header_content += f\"// {mem['details']}\\n\"\n",
        "        header_content += f\"int predict_{safe_name}(float features[NUM_FEATURES]);\\n\"\n",
        "\n",
        "    header_content += \"\\n#endif\\n\"\n",
        "\n",
        "    # L∆∞u header\n",
        "    with open(f\"{lib_dir}/{ARDUINO_LIB_NAME}.h\", 'w') as f:\n",
        "        f.write(header_content)\n",
        "    print(f\"‚úì ƒê√£ t·∫°o: {ARDUINO_LIB_NAME}.h\")\n",
        "\n",
        "    # ========================================\n",
        "    # B∆Ø·ªöC 3: L∆∞u source files\n",
        "    # ========================================\n",
        "    if SEPARATE_MODEL_FILES:\n",
        "        # MODE 1: M·ªói model 1 file ri√™ng (KHUY·∫æN KH√çCH - t·ªëi ∆∞u ROM)\n",
        "        print(f\"\\nüìÅ Separate files mode: T·∫°o {len(model_c_codes)} file .cpp ri√™ng bi·ªát\")\n",
        "        src_dir = f\"{lib_dir}/src\"\n",
        "        os.makedirs(src_dir, exist_ok=True)\n",
        "\n",
        "        # T·∫°o main .cpp ch·ªâ ch·ª©a helper functions\n",
        "        main_content = f\"\"\"/*\n",
        " * {ARDUINO_LIB_NAME} Implementation\n",
        " * Auto-generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        " */\n",
        "\n",
        "#include \"{ARDUINO_LIB_NAME}.h\"\n",
        "#include <math.h>\n",
        "\n",
        "// ========================================\n",
        "// HELPER FUNCTIONS\n",
        "// ========================================\n",
        "\n",
        "void print_feature_info() {{\n",
        "    Serial.println(\"\\\\n========================================\");\n",
        "    Serial.println(\"FEATURE INFORMATION\");\n",
        "    Serial.println(\"========================================\");\n",
        "    Serial.print(\"Number of features: \");\n",
        "    Serial.println(NUM_FEATURES);\n",
        "    Serial.print(\"Number of classes: \");\n",
        "    Serial.println(NUM_CLASSES);\n",
        "    Serial.println(\"\\\\nFeature order and expected ranges:\");\n",
        "\n",
        "    for (int i = 0; i < NUM_FEATURES; i++) {{\n",
        "        Serial.print(\"  \");\n",
        "        Serial.print(i + 1);\n",
        "        Serial.print(\". \");\n",
        "        Serial.print(FEATURE_NAMES[i]);\n",
        "        Serial.print(\": [\");\n",
        "        Serial.print(FEATURE_RANGES[i][0], 4);\n",
        "        Serial.print(\", \");\n",
        "        Serial.print(FEATURE_RANGES[i][1], 4);\n",
        "        Serial.println(\"]\");\n",
        "    }}\n",
        "\n",
        "    Serial.println(\"\\\\nClass labels:\");\n",
        "    for (int i = 0; i < NUM_CLASSES; i++) {{\n",
        "        Serial.print(\"  \");\n",
        "        Serial.println(CLASS_LABELS[i]);\n",
        "    }}\n",
        "    Serial.println(\"========================================\\\\n\");\n",
        "}}\n",
        "\"\"\"\n",
        "        with open(f\"{lib_dir}/{ARDUINO_LIB_NAME}.cpp\", 'w') as f:\n",
        "            f.write(main_content)\n",
        "        print(f\"‚úì ƒê√£ t·∫°o: {ARDUINO_LIB_NAME}.cpp (helper functions)\")\n",
        "\n",
        "        # T·∫°o file ri√™ng cho t·ª´ng model\n",
        "        for model_name, c_code in model_c_codes.items():\n",
        "            # Ki·ªÉm tra xem model c√≥ trong memory_info kh√¥ng\n",
        "            if model_name not in memory_info:\n",
        "                print(f\"‚ö†Ô∏è  Warning: {model_name} kh√¥ng c√≥ memory info, s·ª≠ d·ª•ng gi√° tr·ªã m·∫∑c ƒë·ªãnh\")\n",
        "                memory_info[model_name] = {\n",
        "                    'rom_bytes': 5000,\n",
        "                    'ram_bytes': 500,\n",
        "                    'rom_kb': 5.0,\n",
        "                    'ram_kb': 0.5,\n",
        "                    'details': 'Unknown'\n",
        "                }\n",
        "\n",
        "            safe_name = model_name.replace('-', '_').replace(' ', '_')\n",
        "            model_file_content = f\"\"\"/*\n",
        " * {model_name} Implementation\n",
        " * Auto-generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        " *\n",
        " * ROM Usage: ~{memory_info[model_name]['rom_kb']:.2f} KB\n",
        " * RAM Usage: ~{memory_info[model_name]['ram_kb']:.2f} KB\n",
        " */\n",
        "\n",
        "#include \"{ARDUINO_LIB_NAME}.h\"\n",
        "#include <math.h>\n",
        "\n",
        "{c_code}\n",
        "\"\"\"\n",
        "            with open(f\"{src_dir}/{safe_name}.cpp\", 'w') as f:\n",
        "                f.write(model_file_content)\n",
        "            print(f\"  ‚úì {safe_name}.cpp ({memory_info[model_name]['rom_kb']:.1f} KB)\")\n",
        "\n",
        "        print(f\"\\nüí° OPTIMIZATION: Linker s·∫Ω ch·ªâ compile models ƒë∆∞·ª£c g·ªçi trong sketch!\")\n",
        "        print(f\"   V√≠ d·ª•: N·∫øu ch·ªâ g·ªçi 2/10 models ‚Üí Binary size ch·ªâ tƒÉng ~{sum([memory_info[m]['rom_bytes'] for m in list(model_c_codes.keys())[:2]])/1024:.1f} KB\")\n",
        "\n",
        "    else:\n",
        "        # MODE 2: T·∫•t c·∫£ trong 1 file (ƒë∆°n gi·∫£n nh∆∞ng kh√¥ng t·ªëi ∆∞u)\n",
        "        print(f\"\\nüìÑ Single file mode: T·∫•t c·∫£ models trong 1 file .cpp\")\n",
        "        with open(f\"{lib_dir}/{ARDUINO_LIB_NAME}.cpp\", 'w') as f:\n",
        "            f.write(source_content)\n",
        "        print(f\"‚úì ƒê√£ t·∫°o: {ARDUINO_LIB_NAME}.cpp\")\n",
        "        print(f\"‚ö†Ô∏è  WARNING: Compiler c√≥ th·ªÉ compile C·∫¢ {len(model_c_codes)} models ngay c·∫£ khi ch·ªâ d√πng 1-2 models\")\n",
        "        print(f\"   ‚Üí ƒê·∫∑t SEPARATE_MODEL_FILES = True ƒë·ªÉ t·ªëi ∆∞u ROM usage\")\n",
        "\n",
        "    # T·∫°o README.md v·ªõi memory information\n",
        "    create_memory_readme(lib_dir, model_c_codes, n_features, n_classes)\n",
        "\n",
        "    # In memory report\n",
        "    print_memory_report(trained_models)\n",
        "\n",
        "    # T·∫°o library.properties\n",
        "    properties_content = f\"\"\"name={ARDUINO_LIB_NAME}\n",
        "version=1.0.0\n",
        "author=Auto-generated\n",
        "maintainer=Auto-generated\n",
        "sentence=Machine Learning prediction library for Arduino/ESP32\n",
        "paragraph=SVM models for classification\n",
        "category=Data Processing\n",
        "url=\n",
        "architectures=*\n",
        "includes={ARDUINO_LIB_NAME}.h\n",
        "\"\"\"\n",
        "\n",
        "    with open(f\"{lib_dir}/library.properties\", 'w') as f:\n",
        "        f.write(properties_content)\n",
        "    print(f\"‚úì ƒê√£ t·∫°o: library.properties\")\n",
        "\n",
        "    # T·∫°o examples (ch·ªâ d√πng models ƒë√£ generate th√†nh c√¥ng)\n",
        "    create_arduino_examples(lib_dir, model_c_codes, feature_names, n_features, n_classes)\n",
        "\n",
        "    return lib_dir\n",
        "\n",
        "def create_arduino_examples(lib_dir, model_c_codes, feature_names, n_features, n_classes):\n",
        "    \"\"\"T·∫°o file examples cho Arduino (ch·ªâ d√πng models ƒë√£ generate C code)\"\"\"\n",
        "    examples_dir = f\"{lib_dir}/examples\"\n",
        "    os.makedirs(examples_dir, exist_ok=True)\n",
        "\n",
        "    # Example 1: Serial Input/Output\n",
        "    example1_dir = f\"{examples_dir}/SerialPredict\"\n",
        "    os.makedirs(example1_dir, exist_ok=True)\n",
        "\n",
        "    model_list = '\\n   '.join([f\"// {i+1}. {name}\" for i, name in enumerate(model_c_codes.keys())])\n",
        "    first_model = list(model_c_codes.keys())[0]\n",
        "    safe_first = first_model.replace('-', '_').replace(' ', '_')\n",
        "\n",
        "    example1_content = f\"\"\"/*\n",
        " * Serial Prediction Example\n",
        " *\n",
        " * Nh·∫≠p d·ªØ li·ªáu qua Serial Monitor v√† nh·∫≠n k·∫øt qu·∫£ prediction\n",
        " * Format: {', '.join([f'feature{i+1}' for i in range(n_features)])}\n",
        " * V√≠ d·ª•: 1.5, 2.3, 4.1\n",
        " *\n",
        " * Available models:\n",
        "   {model_list}\n",
        " */\n",
        "\n",
        "#include <{ARDUINO_LIB_NAME}.h>\n",
        "\n",
        "float features[NUM_FEATURES];\n",
        "\n",
        "void setup() {{\n",
        "    Serial.begin(115200);\n",
        "    while (!Serial) delay(10);\n",
        "\n",
        "    Serial.println(\"========================================\");\n",
        "    Serial.println(\"ML Predictor - Serial Mode\");\n",
        "    Serial.println(\"========================================\");\n",
        "\n",
        "    // Print feature information\n",
        "    print_feature_info();\n",
        "\n",
        "    Serial.println(\"\\\\nPaste {n_features} values from Excel (TAB/space separated):\");\n",
        "    Serial.println(\"Example: 242,1875\\\\t529,291748\\\\t-92,64015961...\");\n",
        "    Serial.println(\"(Use comma for decimal, paste directly from Excel)\");\n",
        "    Serial.println(\"Type 'info' to show feature information again\\\\n\");\n",
        "}}\n",
        "\n",
        "void loop() {{\n",
        "    if (Serial.available() > 0) {{\n",
        "        // Read input\n",
        "        String input = Serial.readStringUntil('\\\\n');\n",
        "        input.trim();\n",
        "\n",
        "        // Check for info command\n",
        "        if (input.equalsIgnoreCase(\"info\")) {{\n",
        "            print_feature_info();\n",
        "            return;\n",
        "        }}\n",
        "\n",
        "        // Convert European/Vietnamese decimal comma (,) to decimal point (.)\n",
        "        for (int i = 0; i < input.length(); i++) {{\n",
        "            if (input.charAt(i) == ',') {{\n",
        "                input.setCharAt(i, '.');\n",
        "            }}\n",
        "        }}\n",
        "\n",
        "        // Parse values separated by TAB or space\n",
        "        int featureCount = 0;\n",
        "        int startIdx = 0;\n",
        "\n",
        "        for (int i = 0; i <= input.length(); i++) {{\n",
        "            bool isDelimiter = (i == input.length()) ||\n",
        "                              (input.charAt(i) == '\\\\t') ||\n",
        "                              (input.charAt(i) == ' ');\n",
        "\n",
        "            if (isDelimiter) {{\n",
        "                if (i > startIdx) {{\n",
        "                    if (featureCount < NUM_FEATURES) {{\n",
        "                        String value = input.substring(startIdx, i);\n",
        "                        value.trim();\n",
        "                        if (value.length() > 0) {{\n",
        "                            features[featureCount] = value.toFloat();\n",
        "                            featureCount++;\n",
        "                        }}\n",
        "                    }}\n",
        "                }}\n",
        "                startIdx = i + 1;\n",
        "            }}\n",
        "        }}\n",
        "\n",
        "        if (featureCount == NUM_FEATURES) {{\n",
        "            // Display input\n",
        "            Serial.print(\"\\\\nInput: [\");\n",
        "            for (int i = 0; i < NUM_FEATURES; i++) {{\n",
        "                Serial.print(FEATURE_NAMES[i]);\n",
        "                Serial.print(\"=\");\n",
        "                Serial.print(features[i], 4);\n",
        "                if (i < NUM_FEATURES - 1) Serial.print(\", \");\n",
        "            }}\n",
        "            Serial.println(\"]\");\n",
        "\n",
        "            // Predict using all models\n",
        "            Serial.println(\"\\\\n--- Predictions from all models ---\");\n",
        "{chr(10).join([f'            Serial.print(\"{name}: Class \");' + chr(10) + f'            Serial.println(predict_{name.replace(\"-\", \"_\").replace(\" \", \"_\")}(features));' for name in model_c_codes.keys()])}\n",
        "            Serial.println(\"----------------------------------------\");\n",
        "            Serial.println(\"Paste next values:\");\n",
        "        }} else {{\n",
        "            Serial.println(\"Error: Need exactly \" + String(NUM_FEATURES) + \" values!\");\n",
        "            Serial.print(\"Got \");\n",
        "            Serial.print(featureCount);\n",
        "            Serial.print(\" values, expected \");\n",
        "            Serial.println(NUM_FEATURES);\n",
        "        }}\n",
        "    }}\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    with open(f\"{example1_dir}/SerialPredict.ino\", 'w') as f:\n",
        "        f.write(example1_content)\n",
        "    print(f\"‚úì ƒê√£ t·∫°o: examples/SerialPredict/SerialPredict.ino\")\n",
        "\n",
        "    # Example 2: Sensor Input\n",
        "    example2_dir = f\"{examples_dir}/SensorPredict\"\n",
        "    os.makedirs(example2_dir, exist_ok=True)\n",
        "\n",
        "    example2_content = f\"\"\"/*\n",
        " * Sensor Prediction Example\n",
        " *\n",
        " * ƒê·ªçc d·ªØ li·ªáu t·ª´ c·∫£m bi·∫øn v√† th·ª±c hi·ªán prediction\n",
        " * Thay ƒë·ªïi h√†m readSensors() ƒë·ªÉ ƒë·ªçc t·ª´ c·∫£m bi·∫øn th·∫≠t\n",
        " *\n",
        " * Available models:\n",
        "   {model_list}\n",
        " */\n",
        "\n",
        "#include <{ARDUINO_LIB_NAME}.h>\n",
        "\n",
        "float features[NUM_FEATURES];\n",
        "unsigned long lastPrediction = 0;\n",
        "const unsigned long PREDICTION_INTERVAL = 1000; // 1 gi√¢y\n",
        "\n",
        "void setup() {{\n",
        "    Serial.begin(115200);\n",
        "    while (!Serial) delay(10);\n",
        "\n",
        "    Serial.println(\"========================================\");\n",
        "    Serial.println(\"ML Predictor - Sensor Mode\");\n",
        "    Serial.println(\"========================================\");\n",
        "\n",
        "    // Kh·ªüi t·∫°o c·∫£m bi·∫øn c·ªßa b·∫°n ·ªü ƒë√¢y\n",
        "    initSensors();\n",
        "}}\n",
        "\n",
        "void loop() {{\n",
        "    unsigned long currentMillis = millis();\n",
        "\n",
        "    if (currentMillis - lastPrediction >= PREDICTION_INTERVAL) {{\n",
        "        lastPrediction = currentMillis;\n",
        "\n",
        "        // ƒê·ªçc d·ªØ li·ªáu t·ª´ c·∫£m bi·∫øn\n",
        "        readSensors();\n",
        "\n",
        "        // Hi·ªÉn th·ªã d·ªØ li·ªáu\n",
        "        Serial.print(\"Sensor data: [\");\n",
        "        for (int i = 0; i < NUM_FEATURES; i++) {{\n",
        "            Serial.print(features[i], 4);\n",
        "            if (i < NUM_FEATURES - 1) Serial.print(\", \");\n",
        "        }}\n",
        "        Serial.println(\"]\");\n",
        "\n",
        "        // Predict using all models\n",
        "        Serial.println(\"--- Predictions ---\");\n",
        "{chr(10).join([f'        Serial.print(\"{name}: \");' + chr(10) + f'        Serial.println(predict_{name.replace(\"-\", \"_\").replace(\" \", \"_\")}(features));' for name in model_c_codes.keys()])}\n",
        "        Serial.println();\n",
        "    }}\n",
        "}}\n",
        "\n",
        "void initSensors() {{\n",
        "    // TODO: Kh·ªüi t·∫°o c·∫£m bi·∫øn c·ªßa b·∫°n\n",
        "    // V√≠ d·ª•:\n",
        "    // pinMode(SENSOR_PIN, INPUT);\n",
        "    // sensor.begin();\n",
        "\n",
        "    Serial.println(\"‚úì Sensors initialized\");\n",
        "}}\n",
        "\n",
        "void readSensors() {{\n",
        "    // TODO: ƒê·ªçc d·ªØ li·ªáu t·ª´ c·∫£m bi·∫øn th·∫≠t\n",
        "    // V√≠ d·ª• v·ªõi {n_features} features:\n",
        "\n",
        "    {\"\".join([f\"features[{i}] = analogRead(A{i}) * (5.0 / 1023.0); // Sensor {i+1}\\\\n    \" for i in range(min(n_features, 6))])}\n",
        "\n",
        "    // Ho·∫∑c t·ª´ c·∫£m bi·∫øn I2C/SPI:\n",
        "    // features[0] = sensor.readTemperature();\n",
        "    // features[1] = sensor.readHumidity();\n",
        "    // ...\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "    with open(f\"{example2_dir}/SensorPredict.ino\", 'w') as f:\n",
        "        f.write(example2_content)\n",
        "    print(f\"‚úì ƒê√£ t·∫°o: examples/SensorPredict/SensorPredict.ino\")\n",
        "\n",
        "    print(f\"\\\\n‚úÖ ƒê√£ t·∫°o {2} example files trong th∆∞ vi·ªán\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PACKAGE AND DOWNLOAD\n",
        "# ============================================================================\n",
        "\n",
        "def package_and_download():\n",
        "    \"\"\"N√©n t·∫•t c·∫£ v√† t·∫£i xu·ªëng\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"N√âN V√Ä T·∫¢I XU·ªêNG\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # T·∫°o file RAR/ZIP\n",
        "    archive_name = f\"{OUTPUT_DIR}_Results\"\n",
        "\n",
        "    try:\n",
        "        # S·ª≠ d·ª•ng shutil ƒë·ªÉ t·∫°o zip (RAR y√™u c·∫ßu WinRAR)\n",
        "        shutil.make_archive(archive_name, 'zip', OUTPUT_DIR)\n",
        "        print(f\"‚úì ƒê√£ n√©n th√†nh: {archive_name}.zip\")\n",
        "\n",
        "        # T·∫£i xu·ªëng n·∫øu ƒëang ·ªü Google Colab\n",
        "        if IN_COLAB:\n",
        "            files.download(f\"{archive_name}.zip\")\n",
        "            print(\"‚úì ƒêang t·∫£i xu·ªëng file...\")\n",
        "        else:\n",
        "            print(f\"‚úì File ƒë√£ s·∫µn s√†ng t·∫°i: {archive_name}.zip\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå L·ªói khi n√©n: {e}\")\n",
        "\n",
        "# ============================================================================\n",
        "# GRADIO WEB INTERFACE\n",
        "# ============================================================================\n",
        "\n",
        "# Global variables cho Gradio\n",
        "gradio_data = {}\n",
        "gradio_results = {}\n",
        "\n",
        "def gradio_process_single_file(file, train_ratio, random_state, progress=gr.Progress()):\n",
        "    \"\"\"X·ª≠ l√Ω file d·ªØ li·ªáu ch∆∞a chia cho Gradio\"\"\"\n",
        "    try:\n",
        "        progress(0.1, desc=\"üìÇ ƒêang t·∫£i file...\")\n",
        "        global SINGLE_FILE_PATH, TRAIN_FILE_PATH, TEST_FILE_PATH, TRAIN_RATIO, RANDOM_STATE, gradio_data\n",
        "\n",
        "        # X√≥a th∆∞ m·ª•c OUTPUT_DIR c≈© n·∫øu t·ªìn t·∫°i (ƒë·ªÉ b·∫Øt ƒë·∫ßu m·ªõi)\n",
        "        if os.path.exists(OUTPUT_DIR):\n",
        "            import shutil\n",
        "            try:\n",
        "                shutil.rmtree(OUTPUT_DIR)\n",
        "                print(f\"‚úì ƒê√£ x√≥a th∆∞ m·ª•c c≈©: {OUTPUT_DIR}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Kh√¥ng th·ªÉ x√≥a th∆∞ m·ª•c c≈©: {e}\")\n",
        "\n",
        "        # L∆∞u file t·∫°m\n",
        "        progress(0.2, desc=\"üíæ L∆∞u file t·∫°m th·ªùi...\")\n",
        "        SINGLE_FILE_PATH = file.name\n",
        "        TRAIN_FILE_PATH = \"\"\n",
        "        TEST_FILE_PATH = \"\"\n",
        "        TRAIN_RATIO = train_ratio\n",
        "        RANDOM_STATE = int(random_state)\n",
        "\n",
        "        # Load d·ªØ li·ªáu\n",
        "        progress(0.4, desc=\"üìä ƒêang load v√† ph√¢n t√≠ch d·ªØ li·ªáu...\")\n",
        "        X_train, X_test, y_train, y_test, feature_names = load_data()\n",
        "\n",
        "        # L∆∞u v√†o global\n",
        "        gradio_data['X_train'] = X_train\n",
        "        gradio_data['X_test'] = X_test\n",
        "        gradio_data['y_train'] = y_train\n",
        "        gradio_data['y_test'] = y_test\n",
        "        gradio_data['feature_names'] = feature_names\n",
        "\n",
        "        # T·∫°o th√¥ng b√°o\n",
        "        class_counts = pd.Series(y_train).value_counts().sort_index()\n",
        "        class_names = sorted(np.unique(y_train))\n",
        "\n",
        "        info = f\"\"\"‚úÖ **T·∫£i file th√†nh c√¥ng!**\n",
        "\n",
        "üìä **Th√¥ng tin d·ªØ li·ªáu:**\n",
        "- T·ªïng s·ªë m·∫´u: {len(X_train) + len(X_test)}\n",
        "- S·ªë features: {len(feature_names)}\n",
        "- T√™n features: {', '.join(feature_names)}\n",
        "- S·ªë classes: {len(class_names)}\n",
        "- T√™n classes: {', '.join(map(str, class_names))}\n",
        "\n",
        "üìà **Ph√¢n b·ªë classes:**\n",
        "{chr(10).join([f\"- Class {cls}: {count} m·∫´u (train)\" for cls, count in class_counts.items()])}\n",
        "\n",
        "‚úÇÔ∏è **ƒê√£ chia d·ªØ li·ªáu b·∫±ng thu·∫≠t to√°n SPXY:**\n",
        "- Train: {len(X_train)} m·∫´u ({train_ratio*100:.0f}%)\n",
        "- Test: {len(X_test)} m·∫´u ({(1-train_ratio)*100:.0f}%)\n",
        "\n",
        "---\n",
        "\n",
        "### üéâ **XONG B∆Ø·ªöC 1! H√ÉY CHUY·ªÇN SANG B∆Ø·ªöC 2 ƒê·ªÇ TRAINING**\n",
        "\"\"\"\n",
        "\n",
        "        # V·∫Ω bi·ªÉu ƒë·ªì v√†o file\n",
        "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "        plot_data_distribution(X_train, X_test, y_train, y_test, feature_names)\n",
        "\n",
        "        plot_path = f\"{OUTPUT_DIR}/data_distribution.png\"\n",
        "        return info, plot_path\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå L·ªói: {str(e)}\", None\n",
        "\n",
        "def gradio_process_split_files(train_file, test_file, progress=gr.Progress()):\n",
        "    \"\"\"X·ª≠ l√Ω files ƒë√£ chia s·∫µn cho Gradio\"\"\"\n",
        "    try:\n",
        "        progress(0.1, desc=\"üìÇ ƒêang t·∫£i files...\")\n",
        "        global SINGLE_FILE_PATH, TRAIN_FILE_PATH, TEST_FILE_PATH, gradio_data\n",
        "\n",
        "        # X√≥a th∆∞ m·ª•c OUTPUT_DIR c≈© n·∫øu t·ªìn t·∫°i (ƒë·ªÉ b·∫Øt ƒë·∫ßu m·ªõi)\n",
        "        if os.path.exists(OUTPUT_DIR):\n",
        "            import shutil\n",
        "            try:\n",
        "                shutil.rmtree(OUTPUT_DIR)\n",
        "                print(f\"‚úì ƒê√£ x√≥a th∆∞ m·ª•c c≈©: {OUTPUT_DIR}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Kh√¥ng th·ªÉ x√≥a th∆∞ m·ª•c c≈©: {e}\")\n",
        "\n",
        "        progress(0.2, desc=\"üíæ L∆∞u files t·∫°m th·ªùi...\")\n",
        "        SINGLE_FILE_PATH = \"\"\n",
        "        TRAIN_FILE_PATH = train_file.name\n",
        "        TEST_FILE_PATH = test_file.name\n",
        "\n",
        "        # Load d·ªØ li·ªáu\n",
        "        progress(0.4, desc=\"üìä ƒêang load v√† ki·ªÉm tra d·ªØ li·ªáu...\")\n",
        "        X_train, X_test, y_train, y_test, feature_names = load_data()\n",
        "\n",
        "        # L∆∞u v√†o global\n",
        "        gradio_data['X_train'] = X_train\n",
        "        gradio_data['X_test'] = X_test\n",
        "        gradio_data['y_train'] = y_train\n",
        "        gradio_data['y_test'] = y_test\n",
        "        gradio_data['feature_names'] = feature_names\n",
        "\n",
        "        # T·∫°o th√¥ng b√°o\n",
        "        train_counts = pd.Series(y_train).value_counts().sort_index()\n",
        "        test_counts = pd.Series(y_test).value_counts().sort_index()\n",
        "        class_names = sorted(np.unique(np.concatenate([y_train, y_test])))\n",
        "\n",
        "        info = f\"\"\"‚úÖ **T·∫£i files th√†nh c√¥ng!**\n",
        "\n",
        "üìä **Th√¥ng tin d·ªØ li·ªáu:**\n",
        "- Train: {len(X_train)} m·∫´u\n",
        "- Test: {len(X_test)} m·∫´u\n",
        "- S·ªë features: {len(feature_names)}\n",
        "- T√™n features: {', '.join(feature_names)}\n",
        "- S·ªë classes: {len(class_names)}\n",
        "- T√™n classes: {', '.join(map(str, class_names))}\n",
        "\n",
        "üìà **Ph√¢n b·ªë classes trong Train:**\n",
        "{chr(10).join([f\"- Class {cls}: {count} m·∫´u\" for cls, count in train_counts.items()])}\n",
        "\n",
        "üìà **Ph√¢n b·ªë classes trong Test:**\n",
        "{chr(10).join([f\"- Class {cls}: {count} m·∫´u\" for cls, count in test_counts.items()])}\n",
        "\n",
        "---\n",
        "\n",
        "### üéâ **XONG B∆Ø·ªöC 1! H√ÉY CHUY·ªÇN SANG B∆Ø·ªöC 2 ƒê·ªÇ TRAINING**\n",
        "\"\"\"\n",
        "\n",
        "        # V·∫Ω bi·ªÉu ƒë·ªì v√†o file\n",
        "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "        plot_data_distribution(X_train, X_test, y_train, y_test, feature_names)\n",
        "\n",
        "        # Tr·∫£ v·ªÅ c·∫£ info v√† path ·∫£nh\n",
        "        plot_path = f\"{OUTPUT_DIR}/data_distribution.png\"\n",
        "        return info, plot_path\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå L·ªói: {str(e)}\", None\n",
        "\n",
        "def gradio_train_models(n_trials, cv_folds, optuna_timeout, output_dir, selected_groups, progress=gr.Progress()):\n",
        "    \"\"\"Train models cho Gradio v·ªõi progress bar\"\"\"\n",
        "    try:\n",
        "        global N_TRIALS, CV_FOLDS, OPTUNA_TIMEOUT, OUTPUT_DIR, gradio_results\n",
        "\n",
        "        if not gradio_data:\n",
        "            return \"‚ùå Ch∆∞a c√≥ d·ªØ li·ªáu. Vui l√≤ng t·∫£i file tr∆∞·ªõc!\", None, \"\"\n",
        "\n",
        "        # C·∫≠p nh·∫≠t config\n",
        "        N_TRIALS = int(n_trials)\n",
        "        CV_FOLDS = int(cv_folds)\n",
        "        OPTUNA_TIMEOUT = int(optuna_timeout)\n",
        "        OUTPUT_DIR = output_dir\n",
        "\n",
        "        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i\n",
        "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "        # √Ånh x·∫° nh√≥m sang family\n",
        "        group_to_family = {\n",
        "            \"SVM (9 models)\": \"SVM\",\n",
        "            \"Tree (5 models)\": \"Tree\",\n",
        "            \"Neural Network (3 models)\": \"Neural Network\",\n",
        "            \"KNN (2 models)\": \"KNN\",\n",
        "            \"Discriminant (2 models)\": \"Discriminant\",\n",
        "            \"Naive Bayes (2 models)\": \"Naive Bayes\"\n",
        "        }\n",
        "\n",
        "        # L·ªçc family ƒë∆∞·ª£c ch·ªçn\n",
        "        selected_families = [group_to_family[g] for g in selected_groups if g in group_to_family]\n",
        "\n",
        "        if not selected_families:\n",
        "            return \"‚ùå Vui l√≤ng ch·ªçn √≠t nh·∫•t 1 nh√≥m m√¥ h√¨nh!\", None, \"\"\n",
        "\n",
        "        X_train = gradio_data['X_train']\n",
        "        X_test = gradio_data['X_test']\n",
        "        y_train = gradio_data['y_train']\n",
        "        y_test = gradio_data['y_test']\n",
        "\n",
        "        import time\n",
        "        start_time = time.time()\n",
        "\n",
        "        # T·∫Øt optuna logging\n",
        "        optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "        # Chu·∫©n h√≥a d·ªØ li·ªáu\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # L·∫•y t·∫•t c·∫£ model configs v√† l·ªçc theo nh√≥m ƒë√£ ch·ªçn\n",
        "        all_model_configs = get_model_configs()\n",
        "        model_configs = {name: config for name, config in all_model_configs.items()\n",
        "                        if config.get('family') in selected_families}\n",
        "        total_models = len(model_configs)\n",
        "\n",
        "        all_results = []\n",
        "        trained_models = {}\n",
        "        confusion_matrices = {}\n",
        "\n",
        "        # Train t·ª´ng model v·ªõi progress\n",
        "        for idx, (model_name, config) in enumerate(model_configs.items()):\n",
        "            progress_pct = idx / total_models\n",
        "            progress(progress_pct, desc=f\"ü§ñ Training {model_name} ({idx+1}/{total_models})...\")\n",
        "\n",
        "            try:\n",
        "                model, results, cm = train_single_model(\n",
        "                    model_name, config, X_train_scaled, y_train, X_test_scaled, y_test, scaler\n",
        "                )\n",
        "\n",
        "                if model is not None and results is not None:\n",
        "                    all_results.append(results)\n",
        "                    trained_models[model_name] = {'model': model, 'scaler': scaler, 'family': config.get('family', 'SVM')}\n",
        "                    confusion_matrices[model_name] = cm\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå L·ªói khi train {model_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        progress(0.9, desc=\"üíæ L∆∞u k·∫øt qu·∫£...\")\n",
        "\n",
        "        # L∆∞u k·∫øt qu·∫£\n",
        "        save_results(all_results, confusion_matrices)\n",
        "\n",
        "        # L∆∞u v√†o global\n",
        "        gradio_results['all_results'] = all_results\n",
        "        gradio_results['trained_models'] = trained_models\n",
        "        gradio_results['confusion_matrices'] = confusion_matrices\n",
        "\n",
        "        # T√≠nh th·ªùi gian\n",
        "        elapsed_time = time.time() - start_time\n",
        "        hours = int(elapsed_time // 3600)\n",
        "        minutes = int((elapsed_time % 3600) // 60)\n",
        "        seconds = int(elapsed_time % 60)\n",
        "        time_str = f\"{hours}h {minutes}m {seconds}s\" if hours > 0 else f\"{minutes}m {seconds}s\" if minutes > 0 else f\"{seconds}s\"\n",
        "\n",
        "        progress(1.0, desc=f\"‚úÖ Ho√†n th√†nh! ({time_str})\")\n",
        "\n",
        "        # T·∫°o DataFrame k·∫øt qu·∫£ - S·ª≠a t√™n c·ªôt ƒë√∫ng\n",
        "        results_df = pd.DataFrame(all_results)\n",
        "        results_df = results_df.sort_values('Test Accuracy', ascending=False)\n",
        "\n",
        "        # ·∫®n c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt cho Gradio UI\n",
        "        columns_to_hide = ['Algorithm Family', 'Kernel/Type', 'Best params (JSON)',\n",
        "                          'CV Train Recall', 'CV Train F1', 'Test Recall', 'Test F1',\n",
        "                          'Prediction speed (obs/sec)', 'Model size (bytes)']\n",
        "        display_df = results_df.drop(columns=[col for col in columns_to_hide if col in results_df.columns], errors='ignore')\n",
        "\n",
        "        # Copy ƒë·ªÉ format, gi·ªØ nguy√™n b·∫£n g·ªëc\n",
        "        display_df = display_df.copy()\n",
        "\n",
        "        # Format c√°c c·ªôt % cho d·ªÖ ƒë·ªçc (nh√¢n 100 v√¨ data ·ªü d·∫°ng 0-1)\n",
        "        for col in display_df.columns:\n",
        "            if 'Accuracy' in col or 'Precision' in col:\n",
        "                if col in display_df.columns:\n",
        "                    display_df[col] = display_df[col].apply(lambda x: f\"{x*100:.2f}%\" if pd.notna(x) else \"\")\n",
        "\n",
        "        # L∆∞u k·∫øt qu·∫£ training v√†o 1 file Excel duy nh·∫•t\n",
        "        excel_path = f\"{OUTPUT_DIR}/results.xlsx\"\n",
        "        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
        "            # Sheet 1: Training results (d·∫°ng numeric ƒë·ªÉ sort ƒë∆∞·ª£c)\n",
        "            results_df.to_excel(writer, sheet_name='Training Results', index=False)\n",
        "            # Sheet 2: Summary (format % cho d·ªÖ ƒë·ªçc)\n",
        "            display_df.to_excel(writer, sheet_name='Summary', index=False)\n",
        "        print(f\"‚úì ƒê√£ l∆∞u k·∫øt qu·∫£: {excel_path}\")\n",
        "\n",
        "        # T·∫°o summary\n",
        "        summary = f\"\"\"‚úÖ **Training ho√†n th√†nh!**\n",
        "\n",
        "‚è±Ô∏è **Th·ªùi gian:** {time_str}\n",
        "\n",
        "üìä **T·ªïng quan:**\n",
        "- ƒê√£ train: {len(all_results)} models\n",
        "- ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o: {OUTPUT_DIR}/\n",
        "\n",
        "üèÜ **Top 10 Models (theo Test Accuracy):**\n",
        "\"\"\"\n",
        "\n",
        "        top_10 = results_df.head(10)\n",
        "        for idx, (i, row) in enumerate(top_10.iterrows(), 1):\n",
        "            summary += f\"\\n{idx}. **{row['Model']}**: Acc={row['Test Accuracy']:.2f}%, Prec={row['Test Precision']:.2f}%\"\n",
        "\n",
        "        summary += \"\\n\\n---\\n\\n### üéâ **XONG B∆Ø·ªöC 2! H√ÉY CHUY·ªÇN SANG B∆Ø·ªöC 3 ƒê·ªÇ CH·ªå·∫®N MODELS**\\n\"\n",
        "\n",
        "        # T·∫°o danh s√°ch ƒë·∫ßy ƒë·ªß cho manual selection\n",
        "        models_list_md = \"\\nüìã **Danh s√°ch t·∫•t c·∫£ models (s·∫Øp x·∫øp theo Test Accuracy):**\\n\\n\"\n",
        "        for idx, (i, row) in enumerate(results_df.iterrows(), 1):\n",
        "            models_list_md += f\"{idx}. {row['Model']} - Acc: {row['Test Accuracy']:.2f}%\\n\"\n",
        "\n",
        "        return summary, display_df, models_list_md\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        return f\"‚ùå L·ªói: {str(e)}\\n\\n```\\n{traceback.format_exc()}\\n```\", None, \"\"\n",
        "\n",
        "def gradio_select_models(selection_mode, model_indices, auto_count):\n",
        "    \"\"\"Ch·ªçn models cho Gradio\"\"\"\n",
        "    try:\n",
        "        global gradio_results\n",
        "\n",
        "        if 'all_results' not in gradio_results:\n",
        "            return \"‚ùå Ch∆∞a c√≥ k·∫øt qu·∫£ training!\", None\n",
        "\n",
        "        all_results = gradio_results['all_results']\n",
        "        trained_models = gradio_results['trained_models']\n",
        "\n",
        "        selected_models = {}\n",
        "        num_models = int(auto_count) if auto_count else 10\n",
        "\n",
        "        if selection_mode == \"Top N (Test Accuracy)\":\n",
        "            # S·∫Øp x·∫øp theo Test Accuracy\n",
        "            sorted_results = sorted(all_results, key=lambda x: x['Test Accuracy'], reverse=True)\n",
        "            for i in range(min(num_models, len(sorted_results))):\n",
        "                model_name = sorted_results[i]['Model']\n",
        "                selected_models[model_name] = trained_models[model_name]\n",
        "\n",
        "        elif selection_mode == \"Top N (Train Accuracy)\":\n",
        "            # S·∫Øp x·∫øp theo CV Train Accuracy\n",
        "            sorted_results = sorted(all_results, key=lambda x: x['CV Train Accuracy'], reverse=True)\n",
        "            for i in range(min(num_models, len(sorted_results))):\n",
        "                model_name = sorted_results[i]['Model']\n",
        "                selected_models[model_name] = trained_models[model_name]\n",
        "\n",
        "        elif selection_mode == \"Top N (Train Precision)\":\n",
        "            # S·∫Øp x·∫øp theo CV Train Precision\n",
        "            sorted_results = sorted(all_results, key=lambda x: x['CV Train Precision'], reverse=True)\n",
        "            for i in range(min(num_models, len(sorted_results))):\n",
        "                model_name = sorted_results[i]['Model']\n",
        "                selected_models[model_name] = trained_models[model_name]\n",
        "\n",
        "        elif selection_mode == \"Top N (Test Precision)\":\n",
        "            # S·∫Øp x·∫øp theo Test Precision\n",
        "            sorted_results = sorted(all_results, key=lambda x: x['Test Precision'], reverse=True)\n",
        "            for i in range(min(num_models, len(sorted_results))):\n",
        "                model_name = sorted_results[i]['Model']\n",
        "                selected_models[model_name] = trained_models[model_name]\n",
        "\n",
        "        else:  # Manual\n",
        "            try:\n",
        "                indices = [int(x.strip()) for x in model_indices.split(',')]\n",
        "                sorted_results = sorted(all_results, key=lambda x: x['Test Accuracy'], reverse=True)\n",
        "                for idx in indices:\n",
        "                    if 1 <= idx <= len(sorted_results):\n",
        "                        model_name = sorted_results[idx-1]['Model']\n",
        "                        selected_models[model_name] = trained_models[model_name]\n",
        "            except:\n",
        "                return \"‚ùå L·ªói: Nh·∫≠p s·ªë theo format: 1,2,3\", None\n",
        "\n",
        "        if not selected_models:\n",
        "            return \"‚ùå Kh√¥ng c√≥ model n√†o ƒë∆∞·ª£c ch·ªçn!\", None\n",
        "\n",
        "        gradio_results['selected_models'] = selected_models\n",
        "\n",
        "        info = f\"‚úÖ **ƒê√£ ch·ªçn {len(selected_models)} models:**\\n\"\n",
        "        for name in selected_models.keys():\n",
        "            info += f\"- {name}\\n\"\n",
        "\n",
        "        info += \"\\n---\\n\\n### üéâ **XONG B∆Ø·ªöC 3! H√ÉY CHUY·ªÇN SANG B∆Ø·ªöC 4 ƒê·ªÇ T·∫†O TH∆Ø VI·ªÜN ARDUINO**\\n\"\n",
        "\n",
        "        return info, None\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå L·ªói: {str(e)}\", None\n",
        "\n",
        "def gradio_generate_arduino(lib_name, separate_files):\n",
        "    \"\"\"T·∫°o th∆∞ vi·ªán Arduino cho Gradio\"\"\"\n",
        "    try:\n",
        "        global ARDUINO_LIB_NAME, SEPARATE_MODEL_FILES, gradio_results\n",
        "\n",
        "        if 'selected_models' not in gradio_results:\n",
        "            return \"‚ùå Ch∆∞a ch·ªçn models!\", None\n",
        "\n",
        "        ARDUINO_LIB_NAME = lib_name\n",
        "        SEPARATE_MODEL_FILES = separate_files\n",
        "\n",
        "        selected_models = gradio_results['selected_models']\n",
        "        X_train = gradio_data['X_train']\n",
        "        X_test = gradio_data['X_test']\n",
        "        y_train = gradio_data['y_train']\n",
        "        feature_names = gradio_data['feature_names']\n",
        "        class_labels = sorted(np.unique(y_train))\n",
        "\n",
        "        # Generate Arduino library\n",
        "        lib_dir = generate_arduino_library(selected_models, feature_names, class_labels, X_train, X_test)\n",
        "\n",
        "        # ‚ö†Ô∏è Kh√¥ng t√≠nh memory t·ª´ Python - kh√¥ng ch√≠nh x√°c!\n",
        "        memory_info = {}\n",
        "        for model_name in selected_models.keys():\n",
        "            memory_info[model_name] = {\n",
        "                'rom_bytes': 0,\n",
        "                'ram_bytes': 0,\n",
        "                'rom_kb': 0,\n",
        "                'ram_kb': 0,\n",
        "                'details': 'Compile to measure'\n",
        "            }\n",
        "\n",
        "        # T·∫°o b·∫£ng MCU compatibility\n",
        "        mcu_table = \"\\nüîå **T∆∞∆°ng th√≠ch vi ƒëi·ªÅu khi·ªÉn:**\\n\\n\"\n",
        "        mcu_table += \"| Vi ƒëi·ªÅu khi·ªÉn | Flash (KB) | SRAM (KB) |\\n\"\n",
        "        mcu_table += \"|--------------|-------|------|\\n\"\n",
        "\n",
        "        mcu_specs = [\n",
        "            (\"Arduino Uno\", 32, 2),\n",
        "            (\"Arduino Nano\", 32, 2),\n",
        "            (\"Arduino Pro Mini\", 32, 2),\n",
        "            (\"Arduino Mega 2560\", 256, 8),\n",
        "            (\"ESP8266\", (1024, 16384), 80),\n",
        "            (\"ESP32\", (4096, 16384), 520),\n",
        "            (\"ESP32-S2\", (4096, 16384), 320),\n",
        "            (\"ESP32-S3\", (4096, 16384), 512),\n",
        "            (\"ESP32-C3\", (4096, 16384), 400),\n",
        "            (\"Raspberry Pi Pico\", (2048, 16384), 264),\n",
        "            (\"STM32F103\", (64, 512), (20, 64)),\n",
        "            (\"STM32F4\", (256, 2048), (64, 256)),\n",
        "            (\"STM32F7\", (512, 2048), (256, 512)),\n",
        "            (\"STM32H7\", (1024, 2048), (512, 1024)),\n",
        "            (\"STM32L\", (64, 1024), (16, 320)),\n",
        "        ]\n",
        "\n",
        "        for mcu_name, flash, sram in mcu_specs:\n",
        "            # X·ª≠ l√Ω Flash range ho·∫∑c gi√° tr·ªã ƒë∆°n\n",
        "            if isinstance(flash, tuple):\n",
        "                flash_min = flash[0]\n",
        "                flash_str = f\"{flash[0]}‚Äì{flash[1]} KB\"\n",
        "            else:\n",
        "                flash_min = flash\n",
        "                flash_str = f\"{flash} KB\"\n",
        "\n",
        "            # X·ª≠ l√Ω SRAM range ho·∫∑c gi√° tr·ªã ƒë∆°n\n",
        "            if isinstance(sram, tuple):\n",
        "                sram_min = sram[0]\n",
        "                sram_str = f\"{sram[0]}‚Äì{sram[1]} KB\"\n",
        "            else:\n",
        "                sram_min = sram\n",
        "                sram_str = f\"{sram} KB\"\n",
        "\n",
        "            # Ch·ªâ hi·ªÉn th·ªã k√≠ch th∆∞·ªõc, kh√¥ng ƒë√°nh gi√° ph√π h·ª£p\n",
        "            mcu_table += f\"| {mcu_name} | {flash_str} | {sram_str} |\\n\"\n",
        "\n",
        "        # Append MCU compatibility v√† memory info v√†o results.xlsx\n",
        "        excel_path = f\"{OUTPUT_DIR}/results.xlsx\"\n",
        "        with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a') as writer:\n",
        "            # Sheet 5: Library Info (t·ªïng quan)\n",
        "            lib_info_df = pd.DataFrame([{\n",
        "                'Library Name': lib_name,\n",
        "                'Number of Models': len(memory_info),\n",
        "                'Note': '‚ö†Ô∏è Compile example in Arduino IDE to measure actual Flash/RAM usage'\n",
        "            }])\n",
        "            lib_info_df.to_excel(writer, sheet_name='Library Info', index=False)\n",
        "\n",
        "            # Sheet 6: Model list\n",
        "            mem_df = pd.DataFrame([\n",
        "                {'Model': name, 'Note': 'Compile to see size'}\n",
        "                for name in memory_info.keys()\n",
        "            ])\n",
        "            mem_df.to_excel(writer, sheet_name='Model List', index=False)\n",
        "\n",
        "            # Sheet 7: H∆∞·ªõng d·∫´n ƒëo memory\n",
        "            instructions = pd.DataFrame([\n",
        "                {'Step': 1, 'Action': 'Open Arduino IDE'},\n",
        "                {'Step': 2, 'Action': 'Open your sketch using the library'},\n",
        "                {'Step': 3, 'Action': 'Select your board (ESP32/ESP32-S3/...)'},\n",
        "                {'Step': 4, 'Action': 'Click Verify to see Flash/RAM in compile output'},\n",
        "                {'Step': 5, 'Action': 'Upload and open Serial Monitor (115200)'},\n",
        "                {'Step': 6, 'Action': 'View detailed memory report and benchmarks'},\n",
        "            ])\n",
        "            instructions.to_excel(writer, sheet_name='How to Measure', index=False)\n",
        "\n",
        "        print(f\"‚úì ƒê√£ l∆∞u th√¥ng tin Arduino v√†o: {excel_path}\")\n",
        "\n",
        "        # Ki·ªÉm tra file debug log\n",
        "        debug_log_exists = os.path.exists(f\"{OUTPUT_DIR}/generation_debug.log\")\n",
        "        debug_note = \"\"\n",
        "        if debug_log_exists:\n",
        "            debug_note = f\"\"\"\n",
        "‚ö†Ô∏è **L∆ØU √ù**: C√≥ file debug log chi ti·∫øt t·∫°i:\n",
        "üìÑ `{OUTPUT_DIR}/generation_debug.log`\n",
        "‚Üí Xem file n√†y n·∫øu c√≥ model b·ªã skip (kh√¥ng t·∫°o ƒë∆∞·ª£c code)\n",
        "\"\"\"\n",
        "\n",
        "        info = f\"\"\"‚úÖ **ƒê√£ t·∫°o th∆∞ vi·ªán Arduino: {lib_name}**\n",
        "\n",
        "üì¶ **Th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c t·∫°o t·∫°i:** `{OUTPUT_DIR}/{lib_name}/`\n",
        "{debug_note}\n",
        "‚ö†Ô∏è **ƒê·ªÇ ƒêO K√çCH TH∆Ø·ªöC TH·ª∞C T·∫æ:**\n",
        "\n",
        "üîç **B∆∞·ªõc 1: Compile ƒë·ªÉ xem Flash/RAM**\n",
        "1. M·ªü Arduino IDE\n",
        "2. M·ªü sketch s·ª≠ d·ª•ng th∆∞ vi·ªán\n",
        "3. Ch·ªçn board (ESP32/ESP32-S3/...)\n",
        "4. Click **Verify** (Compile)\n",
        "5. Xem output cu·ªëi c√πng:\n",
        "   ```\n",
        "   Sketch uses XXXXX bytes (XX%) of program storage space\n",
        "   Global variables use XXXX bytes (XX%) of dynamic memory\n",
        "   ```\n",
        "\n",
        "üìä **B∆∞·ªõc 2: Upload ƒë·ªÉ xem chi ti·∫øt**\n",
        "6. Upload sketch l√™n board\n",
        "7. M·ªü Serial Monitor (115200 baud)\n",
        "8. Xem b√°o c√°o ƒë·∫ßy ƒë·ªß:\n",
        "   - Flash Size th·ª±c t·∫ø\n",
        "   - Heap/RAM usage\n",
        "   - Th·ªùi gian th·ª±c thi t·ª´ng model\n",
        "\n",
        "{mcu_table}\n",
        "\n",
        "üí° **C√°c sketch c√≥ s·∫µn:**\n",
        "1. **SerialPredict**: ƒê·ªçc d·ªØ li·ªáu t·ª´ Serial Monitor\n",
        "2. **SerialPredict**: Nh·∫≠p data qua Serial ƒë·ªÉ predict\n",
        "3. **SensorPredict**: Template cho ƒë·ªçc c·∫£m bi·∫øn\n",
        "\n",
        "üîß **C√°c models ƒë√£ ch·ªçn ({len(selected_models)}):**\n",
        "\"\"\"\n",
        "        for name, mem in memory_info.items():\n",
        "            info += f\"- {name}\\n\"\n",
        "\n",
        "        info += f\"\\nüìÇ **Ch·∫ø ƒë·ªô:** {'T√°ch files ri√™ng (t·ªëi ∆∞u Flash)' if separate_files else 'File ƒë∆°n'}\\n\"\n",
        "        info += f\"üìä **ƒê√£ l∆∞u Excel:** {excel_path}\\n\"\n",
        "        info += \"\\n---\\n\\n### üéâ **XONG B∆Ø·ªöC 4! H√ÉY CHUY·ªÇN SANG B∆Ø·ªöC 5 ƒê·ªÇ T·∫¢I XU·ªêNG**\\n\"\n",
        "\n",
        "        return info, None\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        return f\"‚ùå L·ªói: {str(e)}\\n\\n```\\n{traceback.format_exc()}\\n```\", None\n",
        "\n",
        "def gradio_create_download():\n",
        "    \"\"\"T·ª± ƒë·ªông n√©n v√† t·∫£i file ZIP\"\"\"\n",
        "    try:\n",
        "        # T·ª± ƒë·ªông n√©n t·∫•t c·∫£ k·∫øt qu·∫£\n",
        "        package_and_download()\n",
        "\n",
        "        zip_file = f\"{OUTPUT_DIR}_Results.zip\"\n",
        "        if os.path.exists(zip_file):\n",
        "            file_size = os.path.getsize(zip_file) / (1024 * 1024)  # MB\n",
        "            return zip_file, f\"‚úÖ ƒê√£ n√©n xong! File: {zip_file} ({file_size:.1f} MB)\"\n",
        "        else:\n",
        "            return None, \"‚ùå Kh√¥ng t√¨m th·∫•y file ZIP. Vui l√≤ng ch·∫°y Training tr∆∞·ªõc!\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"‚ùå L·ªói khi n√©n: {str(e)}\"\n",
        "\n",
        "def create_gradio_interface():\n",
        "    \"\"\"T·∫°o giao di·ªán Gradio\"\"\"\n",
        "\n",
        "    # CSS ƒë·ªÉ ·∫©n footer Gradio v√† th√™m animation cho buttons\n",
        "    custom_css = \"\"\"\n",
        "    footer {display: none !important;}\n",
        "\n",
        "    /* Card-like sections */\n",
        "    .gradio-container {\n",
        "        max-width: 1400px !important;\n",
        "        margin: auto !important;\n",
        "    }\n",
        "\n",
        "    /* Tab styling */\n",
        "    .tabs {\n",
        "        border-radius: 8px;\n",
        "        overflow: hidden;\n",
        "    }\n",
        "\n",
        "    /* Better spacing */\n",
        "    .gradio-row {\n",
        "        gap: 16px !important;\n",
        "    }\n",
        "\n",
        "    .gradio-column {\n",
        "        padding: 12px !important;\n",
        "        background: #1e293b !important;\n",
        "        border-radius: 8px;\n",
        "    }\n",
        "\n",
        "    /* Button hover effect - ph√≥ng to khi hover */\n",
        "    .pulse-btn button {\n",
        "        transition: all 0.3s ease !important;\n",
        "        font-weight: 600 !important;\n",
        "        border-radius: 8px !important;\n",
        "    }\n",
        "    .pulse-btn button:hover {\n",
        "        transform: scale(1.05) !important;\n",
        "        box-shadow: 0 0 20px rgba(99, 102, 241, 0.5) !important;\n",
        "    }\n",
        "\n",
        "    /* Loading state - m·ªù ƒëi khi click */\n",
        "    .pulse-btn button:active {\n",
        "        opacity: 0.4 !important;\n",
        "        transform: scale(0.98) !important;\n",
        "    }\n",
        "\n",
        "    /* Link-like help buttons (ti√™u ƒë·ªÅ xanh c√≥ th·ªÉ b·∫•m) */\n",
        "    .link-btn button {\n",
        "        background: linear-gradient(135deg, #1e3a8a 0%, #3b82f6 100%) !important;\n",
        "        color: white !important;\n",
        "        border: none !important;\n",
        "        padding: 8px 16px !important;\n",
        "        border-radius: 6px !important;\n",
        "        box-shadow: 0 2px 8px rgba(30, 58, 138, 0.3) !important;\n",
        "        font-weight: 600 !important;\n",
        "        font-size: 13px !important;\n",
        "        transition: all 0.3s ease !important;\n",
        "    }\n",
        "    .link-btn button:hover {\n",
        "        background: linear-gradient(135deg, #1e40af 0%, #60a5fa 100%) !important;\n",
        "        transform: translateY(-2px) !important;\n",
        "        box-shadow: 0 4px 12px rgba(30, 58, 138, 0.4) !important;\n",
        "    }\n",
        "\n",
        "    /* Training config inputs - nh·ªè g·ªçn h∆°n */\n",
        "    .training-input input,\n",
        "    .training-input textarea {\n",
        "        font-size: 14px !important;\n",
        "        padding: 8px 12px !important;\n",
        "        background: #1e293b !important;\n",
        "        border: 1px solid #475569 !important;\n",
        "        color: #e2e8f0 !important;\n",
        "        border-radius: 6px !important;\n",
        "    }\n",
        "\n",
        "    .training-input input:focus,\n",
        "    .training-input textarea:focus {\n",
        "        border-color: #3b82f6 !important;\n",
        "        box-shadow: 0 0 0 2px rgba(59, 130, 246, 0.2) !important;\n",
        "    }\n",
        "\n",
        "    /* Progress status */\n",
        "    .status-box {\n",
        "        padding: 16px;\n",
        "        border-radius: 8px;\n",
        "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
        "        color: white;\n",
        "        font-weight: 600;\n",
        "    }\n",
        "\n",
        "    /* Markdown headings */\n",
        "    .markdown-text h3 {\n",
        "        color: #1f2937;\n",
        "        border-bottom: 2px solid #e5e7eb;\n",
        "        padding-bottom: 8px;\n",
        "        margin-top: 24px;\n",
        "    }\n",
        "\n",
        "    /* File upload areas */\n",
        "    .file-upload {\n",
        "        border: 2px dashed #475569 !important;\n",
        "        border-radius: 8px !important;\n",
        "        padding: 20px !important;\n",
        "        transition: all 0.3s ease !important;\n",
        "        background: #0f172a !important;\n",
        "        color: #94a3b8 !important;\n",
        "    }\n",
        "\n",
        "    .file-upload:hover {\n",
        "        border-color: #64748b !important;\n",
        "        background: #1e293b !important;\n",
        "        box-shadow: 0 4px 12px rgba(71, 85, 105, 0.3) !important;\n",
        "    }\n",
        "\n",
        "    /* Progress bar - make it LARGE and prominent */\n",
        "    .progress-container {\n",
        "        position: fixed !important;\n",
        "        top: 50% !important;\n",
        "        left: 50% !important;\n",
        "        transform: translate(-50%, -50%) !important;\n",
        "        z-index: 9999 !important;\n",
        "        background: rgba(0, 0, 0, 0.85) !important;\n",
        "        padding: 40px 60px !important;\n",
        "        border-radius: 16px !important;\n",
        "        box-shadow: 0 20px 60px rgba(0, 0, 0, 0.5) !important;\n",
        "    }\n",
        "\n",
        "    .progress-text {\n",
        "        color: white !important;\n",
        "        font-size: 24px !important;\n",
        "        font-weight: 700 !important;\n",
        "        text-align: center !important;\n",
        "        margin-bottom: 20px !important;\n",
        "    }\n",
        "\n",
        "    .progress-bar {\n",
        "        height: 12px !important;\n",
        "        border-radius: 6px !important;\n",
        "        background: linear-gradient(90deg, #3b82f6, #8b5cf6) !important;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    with gr.Blocks(title=\"ML Pipeline - Arduino Library Generator\", theme=gr.themes.Soft(), css=custom_css) as app:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # ü§ñ Machine Learning Pipeline - Arduino Library Generator\n",
        "        ### Train ML models v√† t·∫°o th∆∞ vi·ªán cho Arduino/ESP32\n",
        "        ---\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Tabs():\n",
        "            # ===== TAB 1: UPLOAD DATA =====\n",
        "            with gr.Tab(\"üìÅ 1. T·∫£i D·ªØ Li·ªáu\"):\n",
        "                gr.Markdown(\"\"\"\n",
        "                # üìÇ B∆∞·ªõc 1: T·∫£i D·ªØ Li·ªáu\n",
        "                Ch·ªçn m·ªôt trong hai c√°ch t·∫£i d·ªØ li·ªáu b√™n d∆∞·ªõi:\n",
        "                \"\"\")\n",
        "\n",
        "                data_info = gr.Markdown(\"\"\"\n",
        "                ### üí° H∆∞·ªõng d·∫´n:\n",
        "                - **Option 1** (Khuy·∫øn ngh·ªã): Upload 1 file Excel/CSV c√≥ c·ªôt 'Class' ‚Üí T·ª± ƒë·ªông chia train/test b·∫±ng SPXY\n",
        "                - **Option 2**: Upload 2 files ƒë√£ chia s·∫µn (train.xlsx + test.xlsx)\n",
        "                - Sau khi x·ª≠ l√Ω xong ‚Üí Ki·ªÉm tra bi·ªÉu ƒë·ªì ph√¢n b·ªë d·ªØ li·ªáu\n",
        "                \"\"\", elem_classes=[\"info-box\"])\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=1, elem_classes=[\"upload-section\"]):\n",
        "                        gr.Markdown(\"### üìÑ Option 1: File Ch∆∞a Chia\")\n",
        "                        gr.Markdown(\"*T·ª± ƒë·ªông chia train/test b·∫±ng thu·∫≠t to√°n SPXY*\")\n",
        "                        single_file = gr.File(label=\"üìé Ch·ªçn file Excel ho·∫∑c CSV\", file_types=['.xlsx', '.csv'], show_label=True, elem_classes=[\"file-upload\"])\n",
        "\n",
        "                        # Help panel for upload params\n",
        "                        upload_help = gr.Markdown(\"üí° *B·∫•m ti√™u ƒë·ªÅ xanh ƒë·ªÉ xem gi·∫£i th√≠ch chi ti·∫øt*\", visible=True)\n",
        "\n",
        "                        with gr.Row():\n",
        "                            with gr.Column(scale=1):\n",
        "                                btn_help_train_ratio = gr.Button(\"üéØ T·ª∑ l·ªá Train (Train Ratio)\", elem_classes=[\"link-btn\"])\n",
        "                                train_ratio = gr.Slider(0.5, 0.9, value=0.7, step=0.05, label=\"\", show_label=False)\n",
        "                            with gr.Column(scale=1):\n",
        "                                btn_help_random_seed = gr.Button(\"üé≤ Random Seed\", elem_classes=[\"link-btn\"])\n",
        "                                random_state = gr.Number(value=42, label=\"\", precision=0, show_label=False)\n",
        "\n",
        "                        hide_upload_help = gr.Button(\"·∫®n gi·∫£i th√≠ch\", variant=\"secondary\", size=\"sm\")\n",
        "                        btn_single = gr.Button(\"‚ñ∂Ô∏è X·ª≠ l√Ω File\", variant=\"primary\", size=\"lg\", elem_classes=\"pulse-btn\")\n",
        "\n",
        "                    with gr.Column(scale=1, elem_classes=[\"upload-section\"]):\n",
        "                        gr.Markdown(\"### üìÇ Option 2: Files ƒê√£ Chia\")\n",
        "                        gr.Markdown(\"*S·ª≠ d·ª•ng train.xlsx v√† test.xlsx c√≥ s·∫µn*\")\n",
        "                        train_file = gr.File(label=\"üìä File Train\", file_types=['.xlsx', '.csv'], show_label=True, elem_classes=[\"file-upload\"])\n",
        "                        test_file = gr.File(label=\"üìã File Test\", file_types=['.xlsx', '.csv'], show_label=True, elem_classes=[\"file-upload\"])\n",
        "                        gr.Markdown(\"\")  # Spacer\n",
        "                        btn_split = gr.Button(\"‚ñ∂Ô∏è X·ª≠ l√Ω Files\", variant=\"primary\", size=\"lg\", elem_classes=\"pulse-btn\")\n",
        "\n",
        "                # Th√¥ng tin k·∫øt qu·∫£ v√† download\n",
        "                gr.Markdown(\"---\")\n",
        "                gr.Markdown(\"## üìä K·∫øt Qu·∫£ X·ª≠ L√Ω\")\n",
        "                data_result = gr.Markdown(\"*K·∫øt qu·∫£ s·∫Ω hi·ªÉn th·ªã ·ªü ƒë√¢y sau khi x·ª≠ l√Ω*\", elem_classes=[\"status-message\"])\n",
        "\n",
        "                gr.Markdown(\"### üìà Bi·ªÉu ƒê·ªì Ph√¢n B·ªë D·ªØ Li·ªáu\")\n",
        "                plot_image = gr.Image(label=\"Ph√¢n b·ªë Train/Test\", type=\"filepath\", interactive=False, show_label=True)\n",
        "\n",
        "                # Explanation functions for upload parameters\n",
        "                def explain_train_ratio():\n",
        "                    return (\n",
        "                        \"\"\"### üéØ T·ª∑ l·ªá Train (Train Ratio)\n",
        "\n",
        "                        - T·ª∑ l·ªá % d·ªØ li·ªáu d√πng ƒë·ªÉ hu·∫•n luy·ªán model (ph·∫ßn c√≤n l·∫°i ƒë·ªÉ ki·ªÉm tra).\n",
        "                        - **Khuy·∫øn ngh·ªã**: 0.7-0.8 (70-80%) cho h·∫ßu h·∫øt b√†i to√°n.\n",
        "                        - Code d√πng thu·∫≠t to√°n **SPXY** ƒë·ªÉ chia d·ªØ li·ªáu ƒë·ªìng ƒë·ªÅu.\n",
        "                        \"\"\"\n",
        "                    )\n",
        "\n",
        "                def explain_random_seed():\n",
        "                    return (\n",
        "                        \"\"\"### üé≤ Random Seed l√† g√¨?\n",
        "\n",
        "                        **ƒê·ªãnh nghƒ©a ƒë∆°n gi·∫£n:**\n",
        "                        - Random Seed l√† 1 con s·ªë B·∫§T K·ª≤ (42, 99, 5, 123, 9999... s·ªë n√†o c≈©ng ƒë∆∞·ª£c!)\n",
        "                        - Code d√πng s·ªë n√†y ƒë·ªÉ **kh·ªüi t·∫°o b·ªô tr·ªôn d·ªØ li·ªáu**\n",
        "                        - Seed KH√ÅC ‚Üí Code tr·ªôn d·ªØ li·ªáu theo c√°ch KH√ÅC ‚Üí K·∫øt qu·∫£ KH√ÅC\n",
        "\n",
        "                        ### üìä V√≠ d·ª• C·ª§ TH·ªÇ:\n",
        "\n",
        "                        **B·∫°n c√≥ file 10 m·∫´u:** [A, B, C, D, E, F, G, H, I, J]\n",
        "\n",
        "                        **Khi seed = 42:**\n",
        "                        1. Code tr·ªôn th√†nh: [D, A, H, B, I, C, J, E, F, G]\n",
        "                        2. L·∫•y 70% = 7 m·∫´u ƒë·∫ßu v√†o Train: [D, A, H, B, I, C, J]\n",
        "                        3. L·∫•y 30% = 3 m·∫´u cu·ªëi v√†o Test: [E, F, G]\n",
        "                        4. Ch·∫°y model ‚Üí Test Accuracy = **93.2%**\n",
        "\n",
        "                        **Khi seed = 99:**\n",
        "                        1. Code tr·ªôn th√†nh: [G, C, A, I, E, B, D, F, H, J]\n",
        "                        2. L·∫•y 70% = 7 m·∫´u ƒë·∫ßu v√†o Train: [G, C, A, I, E, B, D]\n",
        "                        3. L·∫•y 30% = 3 m·∫´u cu·ªëi v√†o Test: [F, H, J]\n",
        "                        4. Ch·∫°y model ‚Üí Test Accuracy = **92.8%** (KH√ÅC v√¨ test kh√°c m·∫´u!)\n",
        "\n",
        "                        ---\n",
        "\n",
        "                        \"\"\"\n",
        "                    )\n",
        "\n",
        "                # Wire click events for upload params\n",
        "                btn_help_train_ratio.click(fn=explain_train_ratio, outputs=upload_help)\n",
        "                btn_help_random_seed.click(fn=explain_random_seed, outputs=upload_help)\n",
        "                hide_upload_help.click(fn=lambda: \"üí° *B·∫•m ti√™u ƒë·ªÅ xanh ƒë·ªÉ xem gi·∫£i th√≠ch chi ti·∫øt*\", outputs=upload_help)\n",
        "\n",
        "            # ===== TAB 2: TRAINING CONFIG =====\n",
        "            with gr.Tab(\"‚öôÔ∏è 2. C·∫•u H√¨nh & Training\"):\n",
        "                training_info = gr.Markdown(\"‚ÑπÔ∏è Ch∆∞a train\")\n",
        "\n",
        "                gr.Markdown(\"### Thi·∫øt l·∫≠p tham s·ªë training:\")\n",
        "\n",
        "                # H·ªôp hi·ªán gi·∫£i th√≠ch tham s·ªë khi b·∫•m ti√™u ƒë·ªÅ xanh + n√∫t ·∫©n\n",
        "                param_help = gr.Markdown(\"‚ÑπÔ∏è B·∫•m ti√™u ƒë·ªÅ c·ªßa tham s·ªë ƒë·ªÉ xem gi·∫£i th√≠ch.\")\n",
        "                hide_help_btn = gr.Button(\"‚ùå ·∫®n gi·∫£i th√≠ch\", variant=\"secondary\", size=\"sm\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=1):\n",
        "                        btn_help_n_trials = gr.Button(\"üî¢ N_TRIALS\", elem_classes=[\"link-btn\"])\n",
        "                        n_trials = gr.Number(value=50, label=\"\", precision=0, show_label=False, elem_classes=[\"training-input\"])\n",
        "                    with gr.Column(scale=1):\n",
        "                        btn_help_cv_folds = gr.Button(\"üìä CV_FOLDS\", elem_classes=[\"link-btn\"])\n",
        "                        cv_folds = gr.Number(value=5, label=\"\", precision=0, show_label=False, elem_classes=[\"training-input\"])\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=1):\n",
        "                        btn_help_optuna_timeout = gr.Button(\"‚è±Ô∏è TIMEOUT (gi√¢y)\", elem_classes=[\"link-btn\"])\n",
        "                        optuna_timeout = gr.Number(value=180, label=\"\", precision=0, show_label=False, elem_classes=[\"training-input\"])\n",
        "                    with gr.Column(scale=1):\n",
        "                        btn_help_output_dir = gr.Button(\"üìÅ OUTPUT_DIR\", elem_classes=[\"link-btn\"])\n",
        "                        output_dir_name = gr.Textbox(value=\"ML_Output\", label=\"\", show_label=False, elem_classes=[\"training-input\"])\n",
        "\n",
        "                # H√†m gi·∫£i th√≠ch c√°c tham s·ªë\n",
        "                def explain_n_trials():\n",
        "                    return (\n",
        "                        \"\"\"### N_TRIALS l√† g√¨?\n",
        "\n",
        "                        - S·ªë l·∫ßn th·ª≠ ƒë·ªÉ Optuna t√¨m b·ªô si√™u tham s·ªë t·ªët nh·∫•t cho M·ªñI model.\n",
        "                        - M·ªói trial = 1 b·ªô tham s·ªë kh√°c nhau. Nhi·ªÅu trials ‚Üí d·ªÖ t√¨m c·∫•u h√¨nh t·ªët h∆°n nh∆∞ng ch·∫°y l√¢u h∆°n.\n",
        "                        - G·ª£i √Ω: 20‚Äì50 (nhanh, demo), 100‚Äì300 (ch·∫•t l∆∞·ª£ng t·ªët), >300 (khi d·ªØ li·ªáu l·ªõn v√† c·∫ßn t·ªëi ∆∞u k·ªπ).\n",
        "\n",
        "                        V√≠ d·ª•:\n",
        "                        - N_TRIALS = 50 ‚Üí m·ªói model s·∫Ω th·ª≠ 50 c·∫•u h√¨nh kh√°c nhau.\n",
        "                        - 6 models √ó 50 trials = t·ªëi ƒëa ~300 c·∫•u h√¨nh ƒë∆∞·ª£c th·ª≠.\n",
        "                        \"\"\"\n",
        "                    )\n",
        "\n",
        "                def explain_cv_folds():\n",
        "                    return (\n",
        "                        \"\"\"### CV_FOLDS l√† g√¨?\n",
        "\n",
        "                        - S·ªë ph·∫ßn (fold) khi chia t·∫≠p train ƒë·ªÉ ƒë√°nh gi√° ch√©o (K-Fold CV).\n",
        "                        - L·ªõn h∆°n ‚Üí k·∫øt qu·∫£ ·ªïn ƒë·ªãnh h∆°n nh∆∞ng t·ªën th·ªùi gian h∆°n. Ph·ªï bi·∫øn: 5 ho·∫∑c 10.\n",
        "\n",
        "                        V√≠ d·ª•:\n",
        "                        - CV_FOLDS = 5 ‚Üí chia train th√†nh 5 ph·∫ßn, lu√¢n phi√™n 5 l·∫ßn train/validate, l·∫•y ƒëi·ªÉm trung b√¨nh.\n",
        "                        - D·ªØ li·ªáu √≠t ‚Üí n√™n d√πng 5 ƒë·ªÉ c√¢n b·∫±ng t·ªëc ƒë·ªô v√† ƒë·ªô ·ªïn ƒë·ªãnh.\n",
        "                        \"\"\"\n",
        "                    )\n",
        "\n",
        "                def explain_optuna_timeout():\n",
        "                    return (\n",
        "                        \"\"\"### OPTUNA_TIMEOUT l√† g√¨?\n",
        "\n",
        "                        - Th·ªùi gian t·ªëi ƒëa (gi√¢y) ƒë·ªÉ t·ªëi ∆∞u M·ªñI model. D√π ch∆∞a h·∫øt N_TRIALS, Optuna s·∫Ω d·ª´ng khi h·∫øt th·ªùi gian.\n",
        "                        - D√πng ƒë·ªÉ gi·ªõi h·∫°n t·ªïng th·ªùi gian training.\n",
        "\n",
        "                        V√≠ d·ª•:\n",
        "                        - OPTUNA_TIMEOUT = 180 ‚Üí m·ªói model t·ªëi ∆∞u t·ªëi ƒëa 3 ph√∫t.\n",
        "                        - N·∫øu N_TRIALS=200 nh∆∞ng 180 gi√¢y ƒë√£ h·∫øt ‚Üí d·ª´ng s·ªõm, l·∫•y c·∫•u h√¨nh t·ªët nh·∫•t t√¨m ƒë∆∞·ª£c.\n",
        "                        \"\"\"\n",
        "                    )\n",
        "\n",
        "                def explain_output_dir():\n",
        "                    return (\n",
        "                        \"\"\"### OUTPUT_DIR l√† g√¨?\n",
        "\n",
        "                        - T√™n th∆∞ m·ª•c l∆∞u to√†n b·ªô k·∫øt qu·∫£: results.xlsx, bi·ªÉu ƒë·ªì, v√† th∆∞ vi·ªán Arduino ƒë√£ sinh.\n",
        "                        - C√≥ th·ªÉ ƒë·∫∑t theo project ƒë·ªÉ d·ªÖ qu·∫£n l√Ω.\n",
        "\n",
        "                        V√≠ d·ª•:\n",
        "                        - OUTPUT_DIR = \"ML_Output\" ‚Üí t·∫°o th∆∞ m·ª•c ML_Output/ ch·ª©a to√†n b·ªô k·∫øt qu·∫£.\n",
        "                        - M·ªói l·∫ßn ch·∫°y m·ªõi n√™n ƒë·ªïi t√™n ƒë·ªÉ kh√¥ng ghi ƒë√® (vd: ML_Output_2025_12_29).\n",
        "                        \"\"\"\n",
        "                    )\n",
        "\n",
        "                # G√°n s·ª± ki·ªán click ƒë·ªÉ hi·ªÉn th·ªã/·∫©n gi·∫£i th√≠ch\n",
        "                btn_help_n_trials.click(fn=explain_n_trials, outputs=param_help)\n",
        "                btn_help_cv_folds.click(fn=explain_cv_folds, outputs=param_help)\n",
        "                btn_help_optuna_timeout.click(fn=explain_optuna_timeout, outputs=param_help)\n",
        "                btn_help_output_dir.click(fn=explain_output_dir, outputs=param_help)\n",
        "                hide_help_btn.click(fn=lambda: \"\", outputs=param_help)\n",
        "\n",
        "                gr.Markdown(\"---\")\n",
        "                gr.Markdown(\"### Ch·ªçn nh√≥m m√¥ h√¨nh mu·ªën train:\")\n",
        "\n",
        "                model_groups = gr.CheckboxGroup(\n",
        "                    choices=[\n",
        "                        \"SVM (9 models)\",\n",
        "                        \"Tree (5 models)\",\n",
        "                        \"Neural Network (3 models)\",\n",
        "                        \"KNN (2 models)\",\n",
        "                        \"Discriminant (2 models)\",\n",
        "                        \"Naive Bayes (2 models)\"\n",
        "                    ],\n",
        "                    value=[\n",
        "                        \"SVM (9 models)\",\n",
        "                        \"Tree (5 models)\",\n",
        "                        \"Neural Network (3 models)\",\n",
        "                        \"KNN (2 models)\",\n",
        "                        \"Discriminant (2 models)\",\n",
        "                        \"Naive Bayes (2 models)\"\n",
        "                    ],\n",
        "                    label=\"Nh√≥m m√¥ h√¨nh\",\n",
        "                    interactive=True\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    btn_select_all = gr.Button(\"‚úÖ Ch·ªçn T·∫•t C·∫£\", size=\"sm\")\n",
        "                    btn_deselect_all = gr.Button(\"‚ùå B·ªè Ch·ªçn T·∫•t C·∫£\", size=\"sm\")\n",
        "\n",
        "                gr.Markdown(\"---\")\n",
        "                btn_train = gr.Button(\"üöÄ B·∫Øt ƒê·∫ßu Training\", variant=\"primary\", size=\"lg\", elem_classes=\"pulse-btn\")\n",
        "\n",
        "                results_table = gr.Dataframe(\n",
        "                    label=\"K·∫øt qu·∫£ Training\",\n",
        "                    interactive=False,\n",
        "                    wrap=True\n",
        "                )\n",
        "\n",
        "            # ===== TAB 3: MODEL SELECTION =====\n",
        "            with gr.Tab(\"üéØ 3. Ch·ªçn Models\"):\n",
        "                gr.Markdown(\"\"\"\n",
        "                # üéØ B∆∞·ªõc 3: Ch·ªçn Models T·ªët Nh·∫•t\n",
        "                Sau khi training xong, ch·ªçn c√°c models ƒë·ªÉ t·∫°o th∆∞ vi·ªán Arduino/ESP32:\n",
        "                \"\"\")\n",
        "\n",
        "                selection_info = gr.Markdown(\"*K·∫øt qu·∫£ s·∫Ω hi·ªÉn th·ªã sau khi ch·ªçn models*\", elem_classes=[\"status-message\"])\n",
        "\n",
        "                gr.Markdown(\"### üìã C√°ch ch·ªçn models:\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=1):\n",
        "                        selection_mode = gr.Radio(\n",
        "                            choices=[\n",
        "                                \"Top N (Test Accuracy)\",\n",
        "                                \"Top N (Train Accuracy)\",\n",
        "                                \"Top N (Train Precision)\",\n",
        "                                \"Top N (Test Precision)\",\n",
        "                                \"Manual (nh·∫≠p s·ªë)\"\n",
        "                            ],\n",
        "                            value=\"Top N (Test Accuracy)\",\n",
        "                            label=\"üéØ Ti√™u ch√≠ s·∫Øp x·∫øp\",\n",
        "                            info=\"Ch·ªçn ti√™u ch√≠ ƒë·ªÉ l·ªçc top N models\"\n",
        "                        )\n",
        "                    with gr.Column(scale=1):\n",
        "                        auto_count = gr.Number(\n",
        "                            value=10,\n",
        "                            label=\"üìä S·ªë l∆∞·ª£ng models\",\n",
        "                            precision=0,\n",
        "                            minimum=1,\n",
        "                            maximum=50,\n",
        "                            info=\"Ch·ªçn bao nhi√™u models (m·∫∑c ƒë·ªãnh 10)\",\n",
        "                            elem_classes=[\"training-input\"]\n",
        "                        )\n",
        "\n",
        "                model_indices = gr.Textbox(\n",
        "                    label=\"‚úèÔ∏è Nh·∫≠p s·ªë th·ª© t·ª± models (Manual mode)\",\n",
        "                    placeholder=\"V√≠ d·ª•: 1,3,5,7,10\",\n",
        "                    visible=False,\n",
        "                    info=\"Nh·∫≠p c√°c s·ªë c√°ch nhau b·∫±ng d·∫•u ph·∫©y\"\n",
        "                )\n",
        "\n",
        "                models_list = gr.Markdown(\"‚ÑπÔ∏è Danh s√°ch models s·∫Ω hi·ªán sau khi training\", visible=False)\n",
        "\n",
        "                btn_select = gr.Button(\"‚úÖ X√°c nh·∫≠n ch·ªçn models\", variant=\"primary\", elem_classes=\"pulse-btn\")\n",
        "\n",
        "            # ===== TAB 4: ARDUINO LIBRARY =====\n",
        "            with gr.Tab(\"üì¶ 4. T·∫°o Th∆∞ Vi·ªán Arduino\"):\n",
        "                gr.Markdown(\"\"\"\n",
        "                # üì¶ B∆∞·ªõc 4: T·∫°o Th∆∞ Vi·ªán Arduino/ESP32\n",
        "                Chuy·ªÉn ƒë·ªïi models ƒë√£ ch·ªçn th√†nh code C/C++ ƒë·ªÉ ch·∫°y tr√™n vi ƒëi·ªÅu khi·ªÉn:\n",
        "                \"\"\")\n",
        "\n",
        "                generation_info = gr.Markdown(\"*K·∫øt qu·∫£ s·∫Ω hi·ªÉn th·ªã sau khi t·∫°o th∆∞ vi·ªán*\", elem_classes=[\"status-message\"])\n",
        "\n",
        "                gr.Markdown(\"### ‚öôÔ∏è C·∫•u h√¨nh th∆∞ vi·ªán:\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=1):\n",
        "                        lib_name = gr.Textbox(value=\"MLPredictor\", label=\"üìö T√™n th∆∞ vi·ªán\", info=\"T√™n th∆∞ m·ª•c v√† class C++\")\n",
        "                    with gr.Column(scale=1):\n",
        "                        separate_files = gr.Checkbox(\n",
        "                            value=True,\n",
        "                            label=\"‚úÖ T√°ch m·ªói model th√†nh file ri√™ng\",\n",
        "                            info=\"Khuy·∫øn ngh·ªã: T·ªëi ∆∞u Flash ROM cho ESP32/Arduino\"\n",
        "                        )\n",
        "\n",
        "                gr.Markdown(\"---\")\n",
        "                btn_generate = gr.Button(\"üî® T·∫°o Th∆∞ Vi·ªán Arduino\", variant=\"primary\", size=\"lg\", elem_classes=\"pulse-btn\")\n",
        "\n",
        "            # ===== TAB 5: DOWNLOAD =====\n",
        "            with gr.Tab(\"üíæ 5. T·∫£i Xu·ªëng\"):\n",
        "                gr.Markdown(\"\"\"\n",
        "                # üíæ B∆∞·ªõc 5: T·∫£i Xu·ªëng K·∫øt Qu·∫£\n",
        "                T·∫•t c·∫£ k·∫øt qu·∫£ ƒë∆∞·ª£c ƒë√≥ng g√≥i v√†o 1 file ZIP duy nh·∫•t:\n",
        "                \"\"\")\n",
        "\n",
        "                download_info = gr.Markdown(\"*B·∫•m n√∫t b√™n d∆∞·ªõi ƒë·ªÉ t·ª± ƒë·ªông n√©n v√† t·∫£i xu·ªëng*\", elem_classes=[\"status-message\"])\n",
        "\n",
        "                gr.Markdown(\"\"\"\n",
        "                ### üì¶ N·ªôi dung file ZIP:\n",
        "\n",
        "                **File ZIP s·∫Ω t·ª± ƒë·ªông n√©n v√† bao g·ªìm:**\n",
        "                - ‚úÖ **train_data.xlsx & test_data.xlsx** - D·ªØ li·ªáu ƒë√£ chia\n",
        "                - ‚úÖ **data_distribution.png** - Bi·ªÉu ƒë·ªì ph√¢n b·ªë d·ªØ li·ªáu\n",
        "                - ‚úÖ **results.xlsx** - T·∫§T C·∫¢ k·∫øt qu·∫£ (7 sheets):\n",
        "                  - Training Results (ƒë·∫ßy ƒë·ªß k·∫øt qu·∫£ s·ªë)\n",
        "                  - Summary (k·∫øt qu·∫£ format %)\n",
        "                  - Library Info (t·ªïng quan: t√™n, s·ªë model, k√≠ch th∆∞·ªõc Flash/SRAM, MCU ƒë·ªÅ xu·∫•t)\n",
        "                  - Model Memory (Flash/SRAM t·ª´ng model chi ti·∫øt)\n",
        "                  - MCU Compatibility (t∆∞∆°ng th√≠ch 15 MCUs v·ªõi % s·ª≠ d·ª•ng)\n",
        "                - ‚úÖ **confusion_matrices/** - T·∫•t c·∫£ ma tr·∫≠n nh·∫ßm l·∫´n (PNG)\n",
        "                - ‚úÖ **Th∆∞ vi·ªán Arduino ho√†n ch·ªânh** (th∆∞ m·ª•c library v·ªõi .h, .cpp, examples)\n",
        "\n",
        "                **L∆∞u √Ω:** B·∫•m n√∫t s·∫Ω t·ª± ƒë·ªông n√©n v√† t·∫£i xu·ªëng ngay!\n",
        "                \"\"\")\n",
        "\n",
        "                download_file = gr.File(label=\"File ZIP k·∫øt qu·∫£\")\n",
        "                btn_download = gr.Button(\"üì• T·∫£i Xu·ªëng T·∫•t C·∫£ (ZIP)\", variant=\"primary\", size=\"lg\", elem_classes=\"pulse-btn\")\n",
        "\n",
        "        # ===== EVENT HANDLERS =====\n",
        "\n",
        "        # Ch·ªçn/B·ªè ch·ªçn t·∫•t c·∫£ nh√≥m models\n",
        "        btn_select_all.click(\n",
        "            lambda: [\n",
        "                \"SVM (9 models)\",\n",
        "                \"Tree (5 models)\",\n",
        "                \"Neural Network (3 models)\",\n",
        "                \"KNN (2 models)\",\n",
        "                \"Discriminant (2 models)\",\n",
        "                \"Naive Bayes (2 models)\"\n",
        "            ],\n",
        "            outputs=[model_groups]\n",
        "        )\n",
        "\n",
        "        btn_deselect_all.click(\n",
        "            lambda: [],\n",
        "            outputs=[model_groups]\n",
        "        )\n",
        "\n",
        "        # Upload single file\n",
        "        btn_single.click(\n",
        "            gradio_process_single_file,\n",
        "            inputs=[single_file, train_ratio, random_state],\n",
        "            outputs=[data_result, plot_image]\n",
        "        )\n",
        "\n",
        "        # Upload split files\n",
        "        btn_split.click(\n",
        "            gradio_process_split_files,\n",
        "            inputs=[train_file, test_file],\n",
        "            outputs=[data_result, plot_image]\n",
        "        )\n",
        "\n",
        "        # Train models\n",
        "        btn_train.click(\n",
        "            gradio_train_models,\n",
        "            inputs=[n_trials, cv_folds, optuna_timeout, output_dir_name, model_groups],\n",
        "            outputs=[training_info, results_table, models_list]\n",
        "        )\n",
        "\n",
        "        # Toggle visibility cho manual mode\n",
        "        def toggle_visibility(mode):\n",
        "            if mode == \"Manual (nh·∫≠p s·ªë)\":\n",
        "                return gr.update(visible=True), gr.update(visible=True)\n",
        "            else:\n",
        "                return gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "        selection_mode.change(\n",
        "            toggle_visibility,\n",
        "            inputs=[selection_mode],\n",
        "            outputs=[model_indices, models_list]\n",
        "        )\n",
        "\n",
        "        # Select models\n",
        "        btn_select.click(\n",
        "            gradio_select_models,\n",
        "            inputs=[selection_mode, model_indices, auto_count],\n",
        "            outputs=[selection_info, generation_info]\n",
        "        )\n",
        "\n",
        "        # Generate Arduino library\n",
        "        btn_generate.click(\n",
        "            gradio_generate_arduino,\n",
        "            inputs=[lib_name, separate_files],\n",
        "            outputs=[generation_info, download_info]\n",
        "        )\n",
        "\n",
        "        # Create download package\n",
        "        btn_download.click(\n",
        "            gradio_create_download,\n",
        "            outputs=[download_file, download_info]\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"H√†m ch√≠nh th·ª±c thi to√†n b·ªô pipeline\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\" \" * 20 + \"MACHINE LEARNING PIPELINE\")\n",
        "    print(\" \" * 15 + \"SVM Models for Arduino/ESP32\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    try:\n",
        "        # 1. Load d·ªØ li·ªáu\n",
        "        X_train, X_test, y_train, y_test, feature_names = load_data()\n",
        "\n",
        "        # 2. V·∫Ω bi·ªÉu ƒë·ªì ph√¢n b·ªë\n",
        "        plot_data_distribution(X_train, X_test, y_train, y_test, feature_names)\n",
        "\n",
        "        # 3. Train models\n",
        "        all_results, trained_models, confusion_matrices = train_all_models(\n",
        "            X_train, y_train, X_test, y_test\n",
        "        )\n",
        "\n",
        "        # 4. L∆∞u k·∫øt qu·∫£\n",
        "        save_results(all_results, confusion_matrices)\n",
        "\n",
        "        # 5. Ch·ªçn models ƒë·ªÉ generate Arduino library\n",
        "        selected_models = select_models_for_arduino(all_results, trained_models)\n",
        "\n",
        "        # 6. T·∫°o th∆∞ vi·ªán Arduino (ch·ªâ cho models ƒë∆∞·ª£c ch·ªçn)\n",
        "        class_labels = sorted(np.unique(y_train))\n",
        "        generate_arduino_library(selected_models, feature_names, class_labels, X_train, X_test)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\" \" * 25 + \"HO√ÄN TH√ÄNH!\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(f\"\\nK·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {OUTPUT_DIR}/\")\n",
        "        print(\"\\nC√°c file output:\")\n",
        "        print(\"  ‚úì train_data.xlsx, test_data.xlsx - D·ªØ li·ªáu ƒë√£ chia\")\n",
        "        print(\"  ‚úì data_distribution.png - Bi·ªÉu ƒë·ªì ph√¢n b·ªë\")\n",
        "        print(\"  ‚úì results.xlsx - K·∫øt qu·∫£ training (7 sheets: Results, Summary, Library Info, Model Memory, MCU Compatibility)\")\n",
        "        print(\"  ‚úì confusion_matrices/ - Ma tr·∫≠n nh·∫ßm l·∫´n\")\n",
        "        print(f\"  ‚úì {ARDUINO_LIB_NAME}/ - Th∆∞ vi·ªán Arduino/ESP32\")\n",
        "        print(\"\\nüí° Tip: Ch·∫°y package_and_download() n·∫øu c·∫ßn t·∫°o file ZIP\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå L·ªñI: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ============================================================================\n",
        "# RUN\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # N·∫øu ch·∫°y tr√™n Colab/Kaggle, t·ª± ƒë·ªông d√πng Gradio\n",
        "    if IN_CLOUD:\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(f\"üåê {'Google Colab' if IN_COLAB else 'Kaggle'} - ML Pipeline for Arduino/ESP32\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        if not GRADIO_AVAILABLE:\n",
        "            print(\"üì¶ ƒêang c√†i ƒë·∫∑t Gradio...\")\n",
        "            import subprocess\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"gradio\"])\n",
        "            import gradio as gr\n",
        "            GRADIO_AVAILABLE = True\n",
        "\n",
        "        print(\"\\nüöÄ ƒêang kh·ªüi ƒë·ªông giao di·ªán web...\\n\")\n",
        "\n",
        "        app = create_gradio_interface()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"‚úÖ NH·∫§N V√ÄO LINK B√äN D∆Ø·ªöI ƒê·ªÇ M·ªû GIAO DI·ªÜN:\")\n",
        "        print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "        app.launch(\n",
        "            share=True,\n",
        "            debug=False,\n",
        "            show_error=True,\n",
        "            max_file_size=\"100mb\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        # Local machine - cho ph√©p ch·ªçn mode\n",
        "        import argparse\n",
        "        parser = argparse.ArgumentParser(description='ML Pipeline - Train models and generate Arduino libraries')\n",
        "        parser.add_argument('--mode', type=str, default='gradio', choices=['cli', 'gradio'],\n",
        "                           help='Ch·∫ø ƒë·ªô ch·∫°y: cli (command line) ho·∫∑c gradio (web interface)')\n",
        "        parser.add_argument('--share', action='store_true', help='T·∫°o public link cho Gradio (share=True)')\n",
        "        parser.add_argument('--port', type=int, default=7860, help='Port cho Gradio server')\n",
        "\n",
        "        args = parser.parse_args()\n",
        "\n",
        "        if args.mode == 'gradio':\n",
        "            # Ch·∫ø ƒë·ªô Gradio Web Interface\n",
        "            if not GRADIO_AVAILABLE:\n",
        "                print(\"‚ùå Gradio ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t!\")\n",
        "                print(\"C√†i ƒë·∫∑t b·∫±ng: pip install gradio\")\n",
        "                print(\"Ho·∫∑c ch·∫°y ·ªü ch·∫ø ƒë·ªô CLI: python generate11.py --mode cli\")\n",
        "                sys.exit(1)\n",
        "\n",
        "            print(\"\\n\" + \"=\" * 80)\n",
        "            print(\" \" * 20 + \"üåê GRADIO WEB INTERFACE\")\n",
        "            print(\" \" * 15 + \"ML Pipeline for Arduino/ESP32\")\n",
        "            print(\"=\" * 80)\n",
        "            print(f\"\\n‚úÖ Kh·ªüi ƒë·ªông Gradio server tr√™n port {args.port}...\")\n",
        "\n",
        "            app = create_gradio_interface()\n",
        "            app.launch(\n",
        "                share=args.share,\n",
        "                server_name=\"0.0.0.0\",\n",
        "                server_port=args.port,\n",
        "                show_error=True\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            # Ch·∫ø ƒë·ªô CLI (Command Line) - code g·ªëc\n",
        "            # Mount Google Drive n·∫øu c·∫ßn\n",
        "            if IN_COLAB and (SINGLE_FILE_PATH.startswith('/content/drive') or\n",
        "                             TRAIN_FILE_PATH.startswith('/content/drive') or\n",
        "                             TEST_FILE_PATH.startswith('/content/drive')):\n",
        "                print(\"Mounting Google Drive...\")\n",
        "                drive.mount('/content/drive')\n",
        "\n",
        "            # Ch·∫°y pipeline CLI\n",
        "            main()\n",
        "\n"
      ]
    }
  ]
}